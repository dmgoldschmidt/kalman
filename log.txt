Sat 27 Aug 2022 10:08:25 AM PDT
OK, I'm back at work on kalman.tex,cc.  It took me a few days to realize that the current git version was
something in development that I had no remaining idea about, so I went to v3.2 which appears to be the
latest.  The write-up appears to be complete through the derivation of the re-estimation equations (but I have
to do a lot of checking at some point), but there are many mysteries  in the code.  It appears to be set up for
a test case involving a language model, because  the data directory has lots of sample text.  The code currently
defaults to 5-dimensional data and 5 states.  But I found the latest "print.out" from 6/8/2020 which was run with
2-dimensional data and 2 states.  I also found an Array<ColVector<double>> Data.dict whose keys (I think)
are (capital) letters and whose values are 2-dimensional.
FIRST MYSTERY:  what is this used for? It looks like the values are set to +- 1, depending on the 0th or 1st bit
of the ASCII character code.  As far as I can see, Data.dict is not currently used in v3.2.  I did find it in the "test
section" which is currently behind a #if 0 statement.  There's a somewhat involved calculation there which I
will have to try to unravel.

There is a code option AR_mode which defaults to false.    1) it initializes the T-matrix
to a companion matrix, with zero recursion coefficients if sim_mode ==1, or recursion coefficients all = 1
otherwise.  2) In simulation, there's a code section in which the parameters are perturbed, except that
in AR_mode, nothing is changed.  3) At  the start of the alpha pass, the T-matrix is inverted in closed form
in AR_mode.  4) Finally, if T is reestimated, only the recursion coefficients (last row) are updated in AR_mode.

I'm wondering if the re-estimation equations for T can be simplified in AR_mode?  One idea is that we can
just solve for them using least-squares, with coefficients being all pairs (\gamma_t,\gamma_{t+1}).

The output I found is a run in which only the M-matrix is solved for.  It looks from the code like the data was
simulated by applying T to the previous state, and then adding gaussian noise to get a state vector.


Mon 29 Aug 2022 10:49:24 AM PDT
kalman now compiles, and I ran it with the command line:  -nstates 2 -data_dim 2 -seed 9876 -M_re.  The results
agree with the previous output up to 50 iterations (I forgot to set niters = 100).  I also ran the above with -T_re
and got convergence after 82 iterations.

Tue 30 Aug 2022 08:33:23 AM PDT
I ran with /kalman -nstates 2 -data_dim 2 -seed 9876 -T_re -M_re -niters 100 -max 100000  > printAR.out
and again with AR_mode off.  AR_mode clearly does not increase the score at every iteration -- it started out
bouncing all over the place, finally settling down.  In addition, the final score was much lower.
I think it's time to try the least-squares approach.

Tue 30 Aug 2022 09:56:47 AM PDT
I thought I found and fixed bug in the sign of det T, but apparently not.  I also reduced the noise in T by setting
S_T to I (instead of 100*I).  This caused AR mode to go crazy and regular mode to quit immediately.  Go figure.

Wed 31 Aug 2022 09:04:26 AM PDT
I'm trying the ARmode run again.  det T is correct, but we're still getting pretty grubby results.

Wed 31 Aug 2022 03:36:27 PM PDT
Well, I tried 100000 data and 1000 iterations.  From about iteration 200 on, things looked very nice, and
the score climbed smoothly.  But it didn't look like it was ever going to get there!  The score is very sensitive
to small parameter changes.  Next run:  reduce S_M to I.

Sun 04 Sep 2022 08:36:22 AM PDT
After fixing the determinant bug, I just tried re-running the dimension 5 case that had previously crashed, 
and guess what?  It still crashed.  The reason is that constraining det = 1 does not fix the problem -- the state
magnitudes still grow and finally overflow.  In order for this to work, we need an orthogonal T-matrix.  So
that's the next thing to install.

Tue 06 Sep 2022 08:00:44 AM PDT
I coded up "qr_comp.cc".  It compiles and runs correctly.  I first thought there was a bug because the test
was to compute QR and it didn't equal the original A-matrix at all.  However, what we are actually computing
is QA = R, so the test needs to be A =? Q^tR, and indeed it is. Now the big problem is how to update the
T-matrix.  If we begin with T orthonormal, I don't see any reason why T' (the updated T) should continue
to be, and it has to be.  So I think we have to do the QR computation on T' and hope for the best.

Tue 13 Sep 2022 11:26:38 AM PDT
The qr_comp code is now incorporated into kalman.  Running this version, I see the printed gamma_score
monotonically decreasing with each iteration, which is of course exactly the opposite of what's supposed to happen.  
The most obvious solution is that I've got the sign wrong, because the gamma_scores are positive.  Looking
into this, I see that the gamma calculation is rather involved -- it's not just alpha*beta, apparently.  So the first
move here is to go back to the write-up and try to understand this.

Sat 17 Sep 2022 05:11:28 PM PDT
I'm trying to run "kalman --M_re --niters 10" and getting two bizarre outputs:
1) somehow the variance computed by MatrixWelford is getting tiny values which eventually produce a fault.
2) I'm getting a mysterious reprint of the covariance matrix from somewhere.
Solution to 2):  I had a superflouous debugging print in MatrixWelford::variance.

Fri 23 Sep 2022 09:15:00 AM PDT
Oh Boy, this TAKES THE CAKE:  I just found a horrible bug in matrix multiply that's been there forever, apparently.
Namely, it doesn't work unless both matrices have the same number of columns.  In particular, Matrix * ColVector
doesn't work.  No idea how many mistakes that's responsible for over the years!  What it's really telling me is that
I should spend the next several months writing a *complete* test suite for Matrix.h and matrix.cc.

Mon 26 Sep 2022 10:51:49 AM PDT
OK, after fixing the matrix multiply bug, the code "runs" on simulated data, BUT I'm very suspicious of the output
because the gamma score is far from constant:  it increases with time.  Looking into this, my attention is
initially focused on the alpha pass.

Thu 13 Oct 2022 08:36:47 AM PDT
I'm back from the east coast!  In the interim, I noticed that the simulator doesn't really work properly.  Namely,
when we transition from time t-1 to time t, I don't supply any noise.  But the alpha/beta pass assumes that noise
has been supplied, because I use completing the square to make the transition.  Furthermore, I was using RanVec
incorrectly to apply noise to the simulated output vector. So the next move is to fix these bugs.

A second issue is that I should always reestimate  S_0 and mu_0, otherwise every iteration starts with an initial
state that doesn't reflect what the data is saying and could be wildly off,  in which case  the alpha pass has to
"catch up with the data".

Thu 13 Oct 2022 03:44:07 PM PDT
I just noticed that the initialization code in the existing version is a mess, so I've decided to clean it up.
For now, I'm deleting the ARmode option.  That can be tried later after everything else is working.  And
because state transitions now have noise, I'm initializing mu_0 to a random normal vector, and setting
the intial value of T to the identity (as well as all the covariance matrices).  The M-matrix maps states to
data, so I'm initializing it to normal random.

Fri 14 Oct 2022 11:02:02 AM PDT
OK, solving for S_0 and M looks good.  But when solving for S_0 and T, I find that S1_T has a negative eigenvalue
at iteration 3.  No idea what to do here right now.  I need to review the math and figure out what S1_T is supposed
to be.

Sun 16 Oct 2022 09:04:39 AM PDT
Hmm, looks very much like inv is not correctly computing the determinant.

Mon 17 Oct 2022 10:05:40 AM PDT
Ok, the issue is that det_S1_T is zero at iteration 3, whereas my debugging code computes the svd and shows
that it's actually 3.39276e-21.  Looks like it's computed at line 527.  Next step:  gdb with b 527.

Mon 17 Oct 2022 10:31:37 AM PDT
A possible problem is that there are inconsistent error tolerances for various Matrix<double> functions (in matrix.cc).
I think a good solution would be to include an error tolerance as a class variable in Matrix.h.  Then each function
can be called with a particular error tolerance, or  use the class value (for the input matrix to the function).
This allows any particular matrix A to be declared with an optional error tolerance, which will be set to the default value
if it's not specified.   
 I'm going to commit the current versions of everything before trying out these changes.

Tue 18 Oct 2022 10:30:46 AM PDT
So a problem I've had forever is that when I edit a hardlinked file in emacs, the new file is no longer linked.  I found
a clear explanation of this on the web, together with the solution:
(setq backup-by-copying-when-linked t)
The problem is that when emacs writes a modified file to disk, the default is to first rename the original linked file to
<file>~ and then write the new version to a new file named <file>.  But with the above modification, this is reversed,
the original contents are copied to the backup file and the new version overwrites the linked file.

Tue 18 Oct 2022 08:48:54 PM PDT
OK, I've made the changes to Matrix.h and matrix.cc.  And the emacs fix above is working.  Next step:  get a clean
compile of kalman.cc with the new Matrix stuff, and re-run.  And on another note, MemoryBlock should really be
in its own .h file.  That *should* be a relatively easy fix.

Thu 20 Oct 2022 12:16:54 PM PDT
I have a clean compile and when I run (--S_0 --T) I still get numeric errors, even when the error tolerances are
1.0e-50.  In fact, in that case, I get garbage like negative determinants, etc.  I'm now going to review the re-estimation
code for T, because I would like to try a simple least-squares approach instead.  After reviewing kalman.tex and
kalman.cc, I see that this hasn't been tried.  I also find that struct QRreg given in QRreg.cc,h is just the ticket
for this.  I need an Array<QRreg> of size nstates to solve for each row of T separately, fitting the overdetermined
equation T\gamma_t = \gamma_{t+1} (1\le t\le N).  Hopefully, this is both easy and successful.  If it works, I can
similarly fit the equation M_gama_t = x_t to solve for M.  Unfortunately, the noise matrices are another story, but
might try \sum_{t=1}^N S_c[t]/N for example as a reestimation for S_T.

Sat 22 Oct 2022 10:22:50 AM PDT
I'm getting compile errors about interp and/or IndexPair, supposedly coming from Sort.h.  I don't understand
the errors.  I thought I removed include/Sort.h.  I'll now actually delete it.

Wed 26 Oct 2022 02:35:24 PM PDT
I've solved the issue with Sort.h.  Moral:  NEVER put any serious code (more than a few lines) into a .h file,
because when it get included more than once, you get multiple definitions from the linker.
Anyway, on another subject, I discovered that the version of kalman.tex that I've been using seems to be out of
date.  I checked out the latest version I could find, and moved the current one to kalman.tex.bak.

Also, I think I need to recode kalman.cc in a modular fashion.  To begin with, I will
write a class NormalDist which handles everything for a normal distribution, especially forming the product
with another NormalDist by completing the square as in the document.  This can be tested independently.
Then I will try to modularize the alpha pass.  Maybe with classes TimeUpdate and MeasurementUpdate?

Sat Oct 29 09:01:02 2022
The code now looks like it's running with T and M reestimation, but the climb is slow, and I don't think the running
time is much better than on the Intel hardware.  Hmmm.  Anyway, next I want to get rid of the random start for
M for now, to see better how the corrections look.  Also, I will print out any parameters that are being reestimated
after each iteration.

Fri Nov  4 09:04:16 2022
OK, I changed M initialization to use the identity matrix if nstates == data_dim.  More importantly, I changed T ,M
reestimation to use QRreg least squares, fitting to T*mu_c[t] \approx mu_c[t+1] and M*mu_c[t] \approx x[t] resp.
These changes seem to improve convergence drastically.  I'll try more runs with greater tweaking of initial values
for T and M.  Also, I wonder if I can actually prove that they converge to the maximum data likelihood.  Another thing
to try:  let S_C = 1/N\sum_{t=1}^N S_c[t].  Then reestimate S_T  = T^tS_T(old)T and S_M = M^tS_M(old)M.  NOTE:
I'm not sure this is a good idea -- S_M (should really be S_x) is basically observation noise, and the uncertainty in
gamma is already taken care of in the equations.  Also, we might consider making S_T time-dependent and reestimating
via S_T = S_c[t], which is the posterior uncertainty in gamma.
Finally, we might intialize S_0 to the sample covariance matrix of the data. 

Tue Nov  8 16:21:50 2022
I downloaded a dataset from the Indian stock exchange?  It's in code/kalman/data.

Wed Nov  9 08:15:27 2022
I'm not happy with what reestimatation is doing to T and M (virtually nothing, actually).  I think the first move
is to write some test code to verify that QRreg is operating properly.

Fri Nov 11 08:12:37 2022
OK, I wrote testQRreg.cc and verified that QRreg is working.  Next step:  review the simulation and tweaking
parameters for M, then make them obviously different. Run another test and  leave T = identity.

Fri Nov 11 14:40:56 2022
Same problem.  My kalman.cc code is very difficult to debug, and I can't figure out if it even agrees with the
writeup.  Solution:  I will re-write the alpha and beta passes inefficiently, referencing each computation with a
line of the writeup.  First step: define struct QF and double sum and debug them.

Sun Nov 13 11:31:25 2022
OK QF looks good.  Now I'm going to do a commit, and then modify the alpha pass.

Tue Nov 15 10:32:57 2022
I think I may have seen a bug in the old gamma calculation.  I'll check carefully before I substitute the new
one.

Mon Nov 28 12:14:24 2022
Back from LA.  I ran 100 data points (cut to 80) with no re-estimation, and printed out alpha_score, beta-score, and
gamma_score at each time step.  The alpha score was positive and increasing with each successive forward step, which
seems right, but the beta score was negative, and got smaller with each successive backward step.  This seems wrong.  

Fri Dec  2 10:01:12 2022
In addition to the 11/28 changes, I changed both the data dimention and the no. of states to 1.  Now the gamma score is constant across the 100 points.  In addition, I perturbed T to 10.0 and
re-estimated it and got back to close to 1.0 (the simulation value) after 4 iterations.  This all looks good, AND the alpha score was
also negative.  I tried just solving for M, with an initial perturbed value of 15 (and a simulation value of 1) but it didn't work.  Gotta look into this.

Mon Dec  5 10:35:43 2022
I stepped through QRreg::reduce for the first three time steps, and everything looked OK.  So if I'm feeding in the
context/goal pairs correctly, I'm pretty sure I'm getting the correct least squares solution.  But it looks like the simulated data is wrong.  I'm supposedly simulating with with both T and M = 1.0, so the simulated data should look like the
alpha state, and I don't think it does.  That's the next thing to check.

Tue Dec  6 08:22:14 2022
Well, it looks like we can't use least-squares for M-reestimation, for the simple reason that the gammas have been
computed using the purposely tweaked value of M, and thus they reflect that value.  So the thinko was to imagine
that they will still look something like the simulation state values, but they don't.  I think the T-reestimation works
because the simulation value is 1.0.  If we change that to, say, 2.0, and then tweak it to 1.0, I predict we'll still get a
value close to 1.0 by least squares, because the least squares calculation is really fitting to gamma[t+1]-gamma[t].

Thu Dec  8 16:56:14 2022
OK, I've re-installed the EM code for M-reestimation and it is working (in dimension 1).  Also, my prediction for
T-reestimation using least squares didn't pan out.  I generated simulated data at T = 2.0 (dim = 1) and tweaked
back to 1.0, and the code found the 2.0 value.  So for now I'm keeping the least squares version.  Next up:
increase the dimension.  BTW, I think it's pretty clear that T needs to be a rotation matrix.   I'll try that in dimension
2.  I also need to record the simulation state at every value of t and compare it with the final gamma[t].

Sat Dec 10 08:11:41 2022
I ran with nstates = data_dim = 2 and got non-constant gamma_score.  So I tried nstates = 1, data_dim = 2 and
the gamma_scores were constant, but with nstates = 2, data_dim = 1, they were back to non-constant again.  So
it looks like I need to stare closely at the time update equations and code.

Wed Dec 14 17:12:45 2022
After reviewing the write-up carefully, I can't figure out how the alpha and beta pass equations were derived.
In the first place, the basic equation for completing the square, eq. (3.2), does not have any inverse matrices.
These only show up in (3.7), which gives an alternative equation for R which I don't need.  Actually, the only
equation I actually need is Cor. 3.13.  So I'm going to re-write the alpha pass and beta pass sections strictly
following Cor. 3.13.

Fri Dec 16 16:46:38 2022
 I figured out why T^{-1} shows up -- it comes from applying (3.15).
But, I *think* there's a mistake in the writeup deriving \alpha_t(s).  Namely, in the step from \hat(\alpha) to \alpha
we are supposed to apply Cor. 3.13, but I get S_{a,t} = M^tS_xM + T^{-t}\hat{S_{a,t-1}}T^{-1} and the document
says S_{s,t} = M^tS_xM + \hat{S_{a,t-1}}.

Sat Dec 17 08:55:30 2022
So that was indeed a mistake, but only in my current revision.  The original tex has it right.   I fixed it in the revision.
Next:  check the rest of the alpha pass and then check the beta pass.  I thought I saw something fishy there
a few days ago when I looked.

Sat Dec 17 15:29:14 2022
I cleaned up and checked the forward pass.  I thought there might be a problem with the outside constant but
it looks like it checks.  I put some  additional steps in the proof for verification of the results.

Sun Dec 18 08:57:22 2022
I found a mistake in the writeup of the beta pass:  I had a det(T) in the denominator of the constant for \hat{\beta}_t(s),
and it doesn't belong there.

Fri Dec 23 15:28:29 2022
So after fixing the bug, the gamma score is close to constant
over time.  I next ran with 20 iterations, but got a error after
seven.  Turns out that the corrections to T made det(T) < 0 and
log(det(T)) bombed out.  That got me thinking about restricting
T to be orthonormal, and how to do that.  I poked around on the internet a bit, and got a hint about how to proceed.  It turned into an algorithm for finding an orthonormal change of basis for any given orthonormal matrix A with respect to which A has block diagonal form with 2x2 rotations along the diagonal, except for some \pm 1 eigenvalues.  I wrote this up as an appendix to kalman.tex.

Tue Dec 27 15:49:30 2022
I tried just re-orthogonalizing T after every iteration, but that doesn't work.  So I have to put T in block diagonal form
and just correct the theta_i parameters.  I'm going to use non-linear least squares at first.  If this doesn't work, I'll
switch to EM.

Sat Dec 31 14:08:06 2022
Oops -- reorthogonalizing works fine.  I used the SVD which is wrong.  To achieve Gram-Schmidt, use qr_comp(A,Q,R)
followed by A = Q.Tr().

Mon Jan  2 14:55:47 2023
Changed the initialization of beta.S back to zero, and caught the special case t = N-1.  The gamma scores look
better after this.  They're still not completely constant, but pretty close.  I added using standard form for
T and re-estimating using non-linear least-squares to kalman.tex.

Thu Jan 12 08:40:16 2023
Well, I've been putzing around for over a week, trying to figure out why solving for M doesn't really work very well.
Namely, we never head back to the simulation value (of the identity matrix).  Whereas, solving for T does this very well
indeed.  A second problem is the non-constant value of the gamma score.  For a while, it looked like it was constantly
decreasing with time, but this isn't true.  I also observe increasing scores.  But so far, they're all essentially monotonic.
I suspect the beta calculation, but I'm not sure how to go about checking it.  One issue is the start, which I don't really
know how to do properly.  

Sun Jan 15 17:06:12 2023
I'm suspicious of the beta pass, so I'm rechecking the equations.  The measurement update checks, and now I need a
careful check of the time update.

Mon Jan 16 15:22:31 2023
OK, I found two bugs in the beta score, one of which was repeated in the alpha score: 1) When I inverted \hat{S} and
saved the determinant, I set it to det(S), which is a bug.  2) I was using n*(2pi) inside the argument to log, which
h is incorrect.  I changed ntwopi to nlog2pi and removed it from the argument to log in both the alpha and beta scores.
Now I need to check the gamma score, which seems to have the second bug at least, and maybe more.

Tue Jan 17 09:09:46 2023
I fixed the ntwopi bug in all three places and ran.  HALLELUJAH!! The gamma score is now rock solid constant.  Whew!
Unfortunately, the least squares alternative still doesn't work with M reestimation.

Thu Feb  9 15:19:44 2023
I've been busy with Docent stuff the past few weeks, but I did re-code the S_T reestimation, first using
just the noise from (gamma[t].mu -T*gamma[t-1].mu), which didn't work, and then using the EM formula
from the write-up, which unfortunately also didn't work.  I'm looking into that now.  I suspect that 9.15
isn't right.  

Fri Feb 10 10:58:43 2023
I think the problem is that the joint posterior likelihood of states (s_t-1},s_t) is not defined correctly.
It should be the conditional likelihood of s_t given s_{t-1} X posterior likelihood of S_{t-1} =
\N(s_t;TS_{t-1},S_T) X \gamma_{t-1}(s).

Mon Feb 20 09:08:58 2023
So, the previous remark leads to the usual product of two gaussians.  But after going down that rabbit hole, I
discovered that it really seems to further complicate the formulas.  But if I leave it as a product, and integrate
w.r.t. $s_t$ first, the second factor can be moved outside that integral since it only depends on $s_{t-1]$.  This
leaves us with only the expectation over $S_{t-1}$, which seems like a simplification.  We'll see.

Mon Feb 20 17:18:15 2023
I'm not distinguishing the known $\bar{\theta}$ parameters from the unknown $\theta$ parameters in section 8
and probably beyond.

Fri Feb 24 15:52:08 2023
OK, I'm now distinguishing the known from the unknown parameters, and I seem to be making progress by not trying to
combine quadratic forms as described in the first 2/20 post above.  

Sun Feb 26 11:27:27 2023
I've finished the Estimation step, and the result for E_2 looks pretty reasonable.

Fri Mar  3 10:00:22 2023
I've done the maximization for mu_0 and S_0.  I created a new branch: "new_reestimation" before pushing the changes.

Sat Mar 11 08:30:37 2023
April,Vince, and Cara leaving today.  Now I need another preliminary result:  d(tr(AX^tSX)/dX) where dA/dX = 0.
Note: 8.26 is wrong, and referenced at 8.29.

Tue Mar 14 15:30:16 2023
OK, I solved for M and got a reasonable result.  I'm pushing it to github now.
