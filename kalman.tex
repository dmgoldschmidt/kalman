\documentclass[12pt,leqno]{article}
\include{article_defs}
\title{A Continuous Hidden Markov Model}
\author{David M. Goldschmidt}
%\oddsidemargin 10 pt \evensidemargin 10 pt \marginparwidth 0.75 in \textwidth
%6.0 true in \topmargin -40 pt \textheight 8.8 true in 
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lfoot{}
%\rfoot{\thepage}
\begin{document}
%\renewcommand{\footrulewidth}{0.4pt}
\newcommand{\p}{\ensuremath{u}}
\newcommand{\VV}{V}
\maketitle


\section{Introduction}
Let $\mu$ and $x$ be vectors in $\R^n$ and let $\Sigma$ be an $n\times{n}$ positive
definite real symmetric matrix.  Recall that the gaussian probability density
on $\R^n$ is given by 
$$
\N(x;\mu,\Sigma) := (2\pi)^{-\frac{n}{2}}\det{\Sigma}^{-\frac{1}{2}}\exp\left\{-\frac{1}{2}
(x - \mu)^T\Sigma^{-1}(x-\mu)\right\}.
$$
We will make use of the formal identity $\N(x,\mu,\Sigma) = \N(\mu,x,\Sigma)$ below.

We now define a Hidden Markov Model with state space $\R^n$ as follows.  Given the state $s\in\R^n$ and an observation $x_t\in\R^m$ at time $t$, the output density is
$$
X(x_t\mid s) := \N(x_t;M_ts + b_t,\Sigma_{Ob}),
$$
where $M_t$ is a time-dependent linear map from the state space to the
measurement space, and $b_t$ is a time-dependent bias.
In the simplest case, $b_t = 0$  and $M_t = I$, the identity map, for all $t$ so that $m = n$ and $s$ is itself the mean of the output distribution.  We will call this case the {\em basic} case, and concentrate on it initially. Later, we will consider some generalizations, particularly the case that $M_t$ is a non-identity time-independent model parameter, and $b_t$ is not necessarily zero, but is known. 

The state process is a discrete time continuous state Markov process with transition probability density
from state $s_0$ at time $t$ to state $s_1$ at time $t+1$ given by
$$
Pd(s_1\mid s_0) :=  \N(s_1;s_0+c_t,\Sigma_{Tr}),
$$
where $\Sigma_{Ob}$ and $\Sigma_{Tr}$ are time-independent parameters of the model
and $c_t$ is a known ``control input'' which we also assume is zero for the time being..

As usual, we are given observations $\{x_t\mid 1\le t\le T\}$ and model parameters $\theta$.  For each observation time $t$ and state $s$, we define
\begin{align*}
  \alpha_t(s) :&= Pd(x_1,x_2,\dots,x_t~ \& \text{state $s$ at time $t$}\mid \theta ),\\
  \beta_t(s) :&= Pd(x_{t+1},\dots,x_T \mid ~\text{state $s$ at time $t$},\theta)\\
\end{align*}

It is clear from the definitions that the following recursions are satisfied:
\begin{align}
\alpha_t(s) &= X(x_t \mid s)\int_{\R^n}\alpha_{t-1}(u)Pd(s \mid u)du,\quad\text{and}\label{alpha:0}\\
\beta_t(s) &= \int_{\R^{n}}Pd(u \mid s)X(x_{t+1} \mid u)\beta_{t+1}(u)du.\label{beta:0}
\end{align}

We initialize these recursions with
\begin{align*}
\alpha_0(s) :&= (2\pi)^{-\frac{n}{2}}\det{\Sigma_0}^{-\frac{1}{2}}\exp\left\{-\frac{1}{2}(s-\mu_0)^T\Sigma_0^{-1}(s-\mu_0)\right\},\\
\beta_{T+1}(s) :&= 1 \quad\text{for all $s$},
\end{align*}
where $\Sigma_{a,0}$ and $\mu_{a,0}$ are model parameters.

Because everything in sight is gaussian, the above integrals can be evaluated in closed form, by repeated application of
\begin{Lem}\label{comp_sq:1}
  Let
  $$
  Q_i(x) := (x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)\quad(i = 1,2),
  $$
  where $x$ and $\mu_i$ are $n$-dimensional vectors and $\Sigma_i$ is a symmetric positive definite $n\times{n}$
  matrix ($i = 1,2)$. Then
  $$
  Q_1(x) + Q_2(x) = (x-\mu)^T\Sigma^{-1}(x-\mu) + (\mu_2-\mu_1)^T(\Sigma_1+\Sigma_2)^{-1}(\mu_2-\mu_1),
  $$
  where
  \begin{align}
    \mu :&= \Sigma_2(\Sigma_1+\Sigma_2)^{-1}\mu_1+\Sigma_1(\Sigma_1+\Sigma_2)^{-1}\mu_2,\quad\text{and}\label{mu}\\
    \Sigma :&= \Sigma_1(\Sigma_1 + \Sigma_2)^{-1}\Sigma_2\label{sigma}.
  \end{align}
\end{Lem}
\begin{proof}

It's straightforward to see that
$$
Q_1(x) + Q_2(x) = (x-\mu)^T(\Sigma^{-1})(x-\mu) + C,
$$
where
\begin{align}
 \Sigma^{-1} &= \Sigma_1^{-1} + \Sigma_2^{-1},\label{Sigma}\\
  \Sigma^{-1}\mu &=\Sigma_1^{-1}\mu_1+\Sigma_2^{-1}\mu_2, \quad\text{and}\label{mu:1}\\
  C &= \mu_1^T\Sigma_1^{-1}\mu_1 +\mu_2^T\Sigma_2^{-1}\mu_2 - \mu^T\Sigma^{-1}\mu.
  \label{C}
\end{align}
The problem is to prove \eqref{mu} and \eqref{sigma}, and to put $C$ into the form given in the lemma.
 To begin with, we have

\begin{align}
  \Sigma_1\Sigma^{-1}\Sigma_2 &= \Sigma_1(\Sigma_1^{-1}+\Sigma_2^{-1})\Sigma_2 = \Sigma_1+\Sigma_2
  = \Sigma_2\Sigma^{-1}\Sigma_1,\quad\text{whence}\notag\\
  \Sigma_1^{-1}\Sigma\Sigma_2^{-1} &= (\Sigma_1+\Sigma_2)^{-1} = \Sigma_2^{-1}\Sigma\Sigma_1^{-1},\quad\text{and thus}\notag\\
  \Sigma &= \Sigma_1(\Sigma_1 + \Sigma_2)^{-1}\Sigma_2 = \Sigma_2(\Sigma_1+\Sigma_2)^{-1}\Sigma_1,\label{Sigma:1}
\end{align}
proving \eqref{sigma}.  Then \eqref{mu:1} becomes
\begin{equation}\label{mu:2}
  \begin{split}
    \mu &=  \Sigma(\Sigma_1^{-1}\mu_1+\Sigma_2^{-1}\mu_2)\\
    &=\Sigma_2(\Sigma_1+\Sigma_2)^{-1}\mu_1+\Sigma_1(\Sigma_1+\Sigma_2)^{-1}\mu_2,
  \end{split}
\end{equation}
proving \eqref{mu}.

Now using \eqref{Sigma:1}, we can re-write \eqref{mu:1} as  
\begin{equation}\label{mu:3}
  \begin{split}
  \mu &= \Sigma(\Sigma_1^{-1}\mu_1 + \Sigma_2^{-1}\mu_1 +\Sigma_2^{-1}(\mu_2-\mu_1)) \\
  &= \Sigma(\Sigma^{-1}\mu_1 + \Sigma_2^{-1}(\mu_2-\mu_1) \\
  &= \mu_1 + \Sigma_1(\Sigma_1+\Sigma_2)^{-1}(\mu_2-\mu_1),
  \end{split}
  \end{equation}
and by symmetry, we also have 
\begin{equation}\label{mu:4}
  \mu = \mu_2 + \Sigma_2(\Sigma_1+\Sigma_2)^{-1}(\mu_1-\mu_2).
\end{equation}
Then we premultiply \eqref{mu} by $\mu^T$, substituting \eqref{mu:3} into the first term and
  \eqref{mu:4} into the second, as well as using \eqref{Sigma:1}, to get 
\begin{align*}
    \mu^T\Sigma^{-1}\mu &= \mu^T\Sigma_1^{-1}\mu_1 + \mu^T\Sigma_2^{-1}\mu_2 \\
    &= \mu_1^T\Sigma_1^{-1}\mu_1 + (\mu_2-\mu_1)^T(\Sigma_1+\Sigma_2)^{-1}\mu_1
    +\mu_2^T\Sigma_1\mu_2 + (\mu_1-\mu_2)^T(\Sigma_1+\Sigma_2)^{-1}\mu_2\\
    &= \mu_1^T\Sigma_1^{-1}\mu_1 + \mu_2^T\Sigma_2^{-1}\mu_2 - (\mu_2-\mu_1)^T(\Sigma_1+\Sigma_2)^{-1}(\mu_2-\mu_1).
\end{align*}

Finally, substituting this result into \eqref{C} yields
\begin{equation}\label{C:1}
  C = (\mu_2-\mu_1)^T(\Sigma_1+\Sigma_2)^{-1}(\mu_2-\mu_1)
\end{equation}
as required.
\end{proof}

\begin{Cor}\label{comp_sq:2}
\begin{align*}
  \N(x;\mu_1\Sigma_1)\N(x;\mu_2,\Sigma_2) &= \N(\mu_1;\mu_2,\Sigma_1+\Sigma_2)
  \N(x;\mu,\Sigma) \quad\text{where}\\
 \mu :&= \Sigma_2(\Sigma_1+\Sigma_2)^{-1}\mu_1+\Sigma_1(\Sigma_1+\Sigma_2)^{-1}\mu_2, \quad\text{and}\\
 \Sigma :&= \Sigma_1(\Sigma_1 + \Sigma_2)^{-1}\Sigma_2.
\end{align*}
\begin{proof}
  The exponents above are equal by \eqref{comp_sq:1}, and it is
  straightforward to verify directly that the normalization factors outside the exponentials are also equal. 
\end{proof}
\end{Cor}


\section{The Alpha Pass}
  Now we can inductively evaluate \eqref{alpha:0}. To do so, we split the computation
  into two steps.  In the first step, which we call the {\em time update}, we multiply
  $\alpha_{t-1}(u)$ by the state transition function $Pd(s\mid u)$ and integrate with respect
  to $u$. We will denote the result of the time update by $\hat{\alpha}_t(s)$.  It is the joint probability
  (density) of observations $x_1,\dots,x_{t-1}$ and state $s$ at time $t$.
  Then in the second step, which we call the {\em measurement update}, we multiply
  $\hat{\alpha}_t(s)$ by the probability (density) of observing $x_t$ at time $t$ to get $\alpha_t(s)$.

  First, we define
$$
P_{m,n}: = Pr(x_m,x_{m+1},\dots,x_n \mid \theta),\quad\text{for $1\le m \le n\le T$}
$$
\begin{Thm}\label{alpha:1}
  For each state $s$ and time $t$,
$$
  \alpha_t(s) = P_{1,t}N(s;\mu_t,\Sigma_t),
$$
where
\begin{align*}
  \mu_t &= \Sigma_{Ob}(\Sigma_{Ob}+\widehat{\Sigma}_t)^{-1}\mu_{t-1}
  + \widehat{\Sigma}_t(\Sigma_{Ob}+\widehat{\Sigma}_t)^{-1}x_t, \\
  \Sigma_t &= \Sigma_{Ob}(\Sigma_{Ob}+\widehat{\Sigma}_t)^{-1}\widehat{\Sigma}_t,\\
  \widehat{\Sigma}_t &= \Sigma_{Tr}+\Sigma_{t-1},
 \quad\text{and}\\
  P_{1,t} &=P_{1,t-1}(2\pi)^{\frac{n}{2}}\det{\widetilde{\Sigma_t}}^{\frac{1}{2}}
  N(x_t;\mu_{t-1},\Sigma_{Ob} + \widehat{\Sigma}_t).
\end{align*}
\end{Thm}

\begin{proof}

Proceeding by induction on $t$, we note that the case $t = 0$ holds by definition.
For the time update, we have, using \eqref{comp_sq:2}
\begin{align}
  \hat{\alpha}_t(s) &= \int_{\R^n}Pd(s|u)\alpha_{t-1}(u)du \\\notag
  &= P_{1,t-1}\int_{\R^n}\N(u;s,\Sigma_{Tr})\N(u;\mu_{t-1},\Sigma_{t-1})du \notag\\
  &= P_{1,t-1}\N(s;\mu_{t-1},\widehat{\Sigma}_t)\int_{\R^n}\N(u;\hat{\mu}_t,
  \widetilde{\Sigma}_t) du,
\end{align}
where
 \begin{align*}
  \hat{\mu}_t &:= \Sigma_{Tr}\widehat{\Sigma}_t^{-1}\mu_{t-1} +
  \Sigma_{t-1}\widehat{\Sigma}_t^{-1}s, \quad\text{and}\\
  \widetilde{\Sigma}_t &:= \Sigma_{Tr}\widehat{\Sigma}_t^{-1}\Sigma_{t-1}.
  \end{align*}
Because neither $\hat{\mu_t}$ nor $\widetilde{\Sigma}_t$ depends on $u$, we can
make the affine change of variable
$$
v = S(u-\hat{\mu}_t),\quad u = S^{-1}v+\hat{\mu}_t,
$$
where $S$ is the cholesky factor of $\widetilde{\Sigma}_t^{-1}$ satisfying
$S^tS = \widetilde{\Sigma}_t^{-1}$. This transforms the integrand to
  $\det{S}^{-1}\N(v;0,I)dv$
with unchanged domain of integration $\R^n$, which integrates to
$$
(2\pi)^{\frac{n}{2}}\det{S}^{-1} = (2\pi)^{\frac{n}{2}}\det{\widetilde{\Sigma_t}}^{\frac{1}{2}}.
$$
The result is
$$
\hat{\alpha}_t(s) = P_{1,t-1}(2\pi)^{\frac{n}{2}}\det{\widetilde{\Sigma_t}}^{\frac{1}{2}}
\N(s;\mu_{t-1},\widehat{\Sigma}_t).
$$
Multiplying by $X(x_t\mid s)$ finally yields
\begin{align*}
  \alpha_t(s) &= X(x_t\mid s)\hat{\alpha}_t(s) \\
  &= P_{1,t-1}(2\pi)^{\frac{n}{2}}\det{\widetilde{\Sigma_t}}^{\frac{1}{2}}
  \N(s;x_t,\Sigma_{Ob})\N(s;\mu_{t-1},\widehat{\Sigma}_t) \\
  &= P_{1,t-1}(2\pi)^{\frac{n}{2}}\det{\widetilde{\Sigma_t}}^{\frac{1}{2}}
  N(x_t;\mu_{t-1},\Sigma_{Ob} + \widehat{\Sigma}_t)\N(s;\mu_t,\Sigma_t), \\
  &= P_{1,t}\N(s,\mu_t,\Sigma_t),
\end{align*}
as required.
\end{proof}
\end{document}
