\documentclass[12pt,leqno]{article}
\include{article_defs}
\title{A Continuous State Hidden Markov Model}
\author{David M. Goldschmidt}
%\oddsidemargin 10 pt \evensidemargin 10 pt \marginparwidth 0.75 in \textwidth
%6.0 true in \topmargin -40 pt \textheight 8.8 true in 
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lfoot{}
%\rfoot{\thepage}
\begin{document}
%\renewcommand{\footrulewidth}{0.4pt}
\newcommand{\p}{\ensuremath{u}}
\newcommand{\VV}{V}
\maketitle

\section{Preface}
This expository paper  was begun as my pandemic project during the initial lockdown of spring 2020, and then sporadically
continued over the next few years.  I was already familiar with the classical Hidden Markov Model from earlier
work, and I decided to see if I could  convert all the finite probability distributions to gaussian distributions.
After I got into it, I realized that I was probably re-discovering the Kalman Filter, or more acurately as I later learned,
the Kalman Smoother.  

\section{Introduction}
We define a discrete-time hidden Markov model (HMM) with a continuous state, where the  Markov state transitions
are governed by a deterministic linear map with added gaussian noise, and the outputs are similarly obtained from the
state via another deterministic linear map with added gaussian noise.  In other words, we are re-casting a discrete-time
linear dynamical system as a hidden Markov model.  The result is 1) a derivation of the Kalman filtering and smoothing
equations \cite{Aravkin} obtained as a consequence of the Baum-Welch algorithm \cite{Bilmes} adapted to the case of a
gaussian-distributed state, and 2) the derivation of re-estimation equations for all model parameters based on the EM algorithm
\cite{Dempster}.  All these equations (with different notation) can be found in \cite{Hinton}, albeit with little or no
exposition/derivation.  \footnote{There is a vast literature on Kalman filtering and smoothing.  \cite{Aravkin} has an extensive bibliography.}
We conclude the paper with some numerical results on simulated data.
Some familiarity with the discrete state HMM methodology is assumed.  A good reference is \cite{Bilmes}. 

\section{Preliminary Definitions and Notation}
Let $\mu$ and $x$ be vectors in $\R^n$ and let $S$ be an $n\times{n}$ positive
definite real symmetric matrix.  Recall that the gaussian probability density
on $\R^n$ is given by 
$$
\N(x;\mu,S) := (2\pi)^{-\frac{n}{2}}|S|^{\frac{1}{2}}\exp\left\{-\frac{1}{2}
(x - \mu)^tS(x-\mu)\right\}.
$$
So here and below, capital $S$ denotes an inverse covariance matrix. Also, note the symmetry $\N(x;\mu,S) = \N(\mu;x,S)$
which we will exploit freely.

We now define a Hidden Markov Model with state space $\R^n$ as follows.  Given a state $s_t\in\R^n$ and an observation $x_t\in\R^m$ at time $t$, the output density is
$$
X(x_t\mid s) := \N(x_t;M_ts_t + b_t,S_x),
$$
where $M_t$ is a linear map from the state space to the
measurement space, $b_t$ is a bias, and $S_x$ is an $m\times{m}$ positive definite real symmetric matrix.  Thus, $x_t$
is normally distributed with mean $M_ts_t$ and inverse covariance matrix $S_x$.  In this paper, $S_x$ and $M := M_t$ are
time-independent model parameters which we usually seek to re-estimate from the data.  The bias $b_t$ can
usually be subtracted from the data so we assume here that $b_t = 0$ for all $t$. 
In the simplest case, $M_t = I$ for all $t$ so that $m = n$ and $s$ is itself the mean of the output distribution.  
The state process is a discrete time continuous state Markov process with transition probability density
from state $s_0$ at time $t$ to state $s_1$ at time $t+1$ given by
$$
Pd(s_1\mid s_0) :=  \N(s_1;T_ts_0+c_t,S_T),
$$
where $S_T$ is a time-independent model parameter, $T_t$, the ``transition matrix'' is an invertible linear map  
defining the deterministic component of the time update, and $c_t$ is a bias.  In this paper, we assume that
$T_t = T$ is a time-independent model parameter which we may
also re-estimate from data, and that $c_t = 0$.  Note that $T$ is not a transition matrix in the
discrete-state HMM sense.

To avoid scaling issues, we constrain $T$ to be unimodular.  In addition,  we may assume without serious loss of generality
that the characteristic polynomial of $T$ has distinct roots, which is generically true.  This implies that $T$
is similar to a companion matrix, which in this paper will always mean a matrix $(t_{ij})$ such that:
$t_{ij} = 0$ for all $i < n$ and all $j \neq i+1$, and $t_{i,i+1} = 1$ for all $i < n$.  And if there is no
preferred basis for the state space, which is an application-dependent issue,
we may optionally assume that $T$ is a companion matrix.  This is a convenient assumption because it's easy to
constrain $T$ to be unimodular in this case.  Namely,
since $|T| = (-1)^{n-1}t_{n1}$ for such a matrix, we also assume that $t_{n1} = (-1)^{n-1}$.  Thus, $T$ has $n-1$
degrees of freedom given by the coefficients $t_{nj}$ for $j > 1$.  If we are re-estimating $T$, we constrain the $M$-step
of the $EM$ algorithm in order to maintain these conditions on subsequent Baum-Welch iterations.

If we assume $T$ is a companion matrix, then in effect the state vector becomes the vector of all $n$ lags of a
univariate $AR(n)$ process, except that at every time step we add more correlated noise to each lag until it leaves
the AR-window. Thus, each term becomes less and less determinate as it recedes into the past.
And it is convenient to note that companion matrices are invertible in closed form.  Namely, let $T^{-1} = (u_{ij})$, then
it is easy to check that $u_{ij} = 0$ for all $i > 1$ and all $j\neq i-1$,
$u_{i,i-1} = 1$ for all $i > 1$, $ u_{1j} = -t_{i,j+1}$ for $j < n$, and $u_{1n} = (-1)^{n-1}$.   

As usual, we are given observations $\{x_t\mid 1\le t\le N\}$ and model parameters $\theta := \{M,S_x,T,S_T,S_0,\mu_0\}$
where $S_0$ and $\mu_0$ are defined below.  For each observation time $t$ and state $s$, we define
\begin{align*}
  \alpha_t(s) :&= L(x_1,x_2,\dots,x_t~ \& \text{state $s$ at time $t$}\mid \theta ),\\
  \beta_t(s) :&= L(x_{t+1},\dots,x_T \mid ~\text{state $s$ at time $t$},\theta)\\
  \gamma_t(s) :&= \frac{\alpha_t(s)\beta_t(s)}{\int_{\R^n}\alpha_t(s)\beta_t(s)ds},
\end{align*}
where $L$ denotes likelihood, i.e. a non-negative function which can be normalized to a probability density by dividing
by its integral.
Thus, $\alpha_t(s)$ is the joint likelihood of state $s$ at time $t$ and the observations up to (and including) time $t$,
$\beta_t(s)$ is the {\em conditional} likelihood of the future observations given state $s$ at time $t$, and $\gamma_t(s)$
is the posterior probability density of state $s$ at time $t$.

It is clear from the definitions that the following recursions are satisfied:
\begin{align}
\alpha_t(s) &= X(x_t \mid s)\int_{\R^n}\alpha_{t-1}(u)Pd(s \mid u)du,\quad\text{and}\label{alpha:0}\\
\beta_t(s) &= \int_{\R^{n}}Pd(u \mid s)X(x_{t+1} \mid u)\beta_{t+1}(u)du.\label{beta:0}
\end{align}

We initialize these recursions with
\begin{align*}
\alpha_0(s) :&= \N(s;\mu_0,S_0)\quad\text{and} \\
\beta_{T}(s) :&= \N(s;0,I), \quad\text{the standard unit normal},
\end{align*}
where $S_0$ and $\mu_0$ are additional model parameters.

\section{Completing the Square} 
Because everything in sight is gaussian, the above integrals can be evaluated in closed form
using the following lemma, which is used repeatedly below.
\begin{Lem}
 Define the quadratic form
  $$
 Q(x;\mu,S) := (x-\mu)^tS(x-\mu),
  $$
  where $x,\mu\in \R^n$ and  $S$ is a symmetric positive semi-definite $n\times{n}$
  matrix. Let $A_i$ be an $m_i\times{n}$ matrix, $\mu_i\in\R^{n_i} ~ (i = 1,2)$,
  and let $S_1$,$S_2$ be symmetric positive semi-definite. Put
  $$
  S_i' := A_i^tS_iA_i\quad (i = 1,2),
  $$
  and assume that at least one of the $S_i'$ is positive definite.  Then 
\begin{equation}\label{comp_sq:1}
  Q(A_1x;\mu_1,S_1)+Q(A_2x;\mu_2,S_2) = Q(x;\mu,S) + R(\mu_1,\mu_2,S_1,S_2,A_1,A_2),
\end{equation}
where 
\begin{align}
S &= S_1' + S_2', \label{sigma}\\
\mu &= S^{-1}(A_1^tS_1\mu_1 + A_2^tS_2\mu_2),\quad\text{and}\label{mu}\\
R &= \mu_1^tS_1\mu_1 + \mu_2^tS_2\mu_2 - \mu^tS\mu. \label{R_def} 
\end{align}
Furthermore, 
\begin{equation}\label{comp_sq:3}
  S_1'S^{-1}S_2' = S_2'S^{-1}S_1'.
\end{equation}
Moreover, if $A_1$ and $A_2$ are invertible, then with $\mu_i' := A_i^{-1}\mu_i ~ (i = 1,2)$ we have
\begin{equation}\label{comp_sq:R}
R = (\mu_2'-\mu_1')^tS_1'S^{-1}S_2'(\mu_2'-\mu_1').
\end{equation}
\end{Lem}
\begin{proof}
  Note that at least one of $S_1'$, $S_2'$ is positive definite and the other is positive semi-definite,
so $S$ is invertible.  Expanding the left-hand side of \eqref{comp_sq:1} and combining
like terms, we get
$$
x^t(S_1'+ S_2')x -2x^t(A_1^tS_1\mu_1+ A_2^tS_2\mu_2) 
+ \mu_1^tS_1\mu_1 + \mu_2^tS_2\mu_2.
$$
Completing the square, we get
$$
Q(A_1x;\mu_1,S_1) + Q(A_2x;\mu_2,S_2) = (x-\mu)^tS(x-\mu) -\mu^tS\mu + \mu_1^tS_1\mu_1 + \mu_2^tS_2\mu_2,
$$
proving the first three assertions.
To prove \eqref{comp_sq:3}, assume for the moment that both $S_1'$ and $S_2'$ are invertible.
Then
\begin{align*}
S_1'^{-1}SS_2'^{-1} &= S_1'^{-1}(S_1'+S_2')S_2'^{-1} = S_1'^{-1}+S_2'^{-1} = S_2'^{-1}SS_1'^{-1},\quad\text{hence}\\
S_2'S^{-1}S_1' &= (S_1'^{-1}+S_2'^{-1})^{-1} = S_1'S^{-1}S_2'.
\end{align*}
Now for $i = 1,2$, $S_i'+\epsilon{I}$ is positive definite and thus invertible for any $\epsilon > 0$
so \eqref{comp_sq:3} holds for $S_i'+\epsilon{I}$ and all $\epsilon > 0$.  Taking the limit
we conclude that it holds for $\epsilon = 0$ as well.

Finally, suppose that both of the $A_i$ are invertible. Then it's easy to see that

\begin{equation}\label{comp_sq:4}
Q(A_ix;\mu_i,S_i) = Q(x;\mu_i',S_i')\quad(i = 1,2),
\end{equation}
and \eqref{mu} becomes
\begin{equation} \label{mu'}
\mu = S^{-1}(S_1'\mu_1' + S_2'\mu_2')
\end{equation}.
We can re-write this as
\begin{equation}\label{mu:1}
\mu = S^{-1}(S_1'\mu_1' + S_2'\mu_1' + S_2'(\mu_2'-\mu_1)) = \mu_1' + S^{-1}S_2'(\mu_2'-\mu_1'),
\end{equation}
and by symmetry we have
\begin{equation}\label{mu:2}
  \mu = \mu_2' + S^{-1}S_1'(\mu_1'-\mu_2').
\end{equation}
Then from \eqref{mu'} we have
$$
\mu^tS\mu = \mu^t(S_1'\mu_1' + S_2'\mu_2'),
$$
and substituting \eqref{mu:1} in the first term and \eqref{mu:2} in the second, we get
$$
\mu^tS\mu = \mu_1'^tS_1'\mu_1' + (\mu_2'-\mu_1')^tS_2'S^{-1}S_1'\mu_1' + \mu_2'^tS_2'\mu_2'+(\mu_1'-\mu_2')^tS_1'S^{-1}S_2'\mu_2'.
$$
Finally, \eqref{comp_sq:3} yields
$$
\mu^tS\mu = \mu_1'^tS_1'\mu_1' + \mu_2'^tS_2'\mu_2' - (\mu_2'-\mu_1')^tS_1'S^{-1}S_2'(\mu_2'-\mu_1')
$$
and combining this with \eqref{R_def} proves \eqref{comp_sq:R} and completes the proof.
\end{proof}

By virtue of \eqref{comp_sq:3}, we define
$$
S_1*S_2 := S_1(S_1+S_2)^{-1}S_2.
$$
It's easy to check that if $X$ and $Y$ are invertible, then
\begin{equation}\label{comp_sq:XY}
  (XS_1Y)*(XS_2Y) = X(S_1*S_2)Y. 
\end{equation}

We primarily apply \eqref{comp_sq:1} below in the form: 
\begin{Cor}\label{comp_sq:2}
$$
  \N(A_1x;\mu_1,S_1)\N(A_2x;\mu_2,S_2) = \N_0\N(x;\mu,S),
  $$
where $\mu$ and $S$ are as in the lemma, and 
$$
\N_0 := \left(\frac{|S_1|\cdot|S_2|}{|2\pi{S}|}\right)^{\frac{1}{2}}\exp\{-\frac{1}{2}(\mu_1^tS_1\mu_1 + \mu_2^tS_2\mu_2-\mu^tS\mu)\}.
$$

In particular, $\N_0$ does not depend on $x$, and
\begin{equation}\label{comp_sq:int}
  \int_{\R^{n}}  \N(A_1x;\mu_1,S_1)\N(A_2x;\mu_2,S_2)dx = \N_0.
\end{equation}
\qed
\end{Cor}

\begin{Cor}\label{exact_prod} 
 If $A_1$ and $A_2$ are both invertible, then 
$$
 \N(A_1x;\mu_1,S_1)\N(A_2x;\mu_2,S_2)   = \frac{1}{|A_1|\cdot|A_2|}\N(x,\mu,S)\N(\mu_1';\mu_2',S_1'*S_2'),
$$
 where the notation is as above.
\end{Cor}
\begin{proof}

  Using \eqref{comp_sq:4}, we have
$$
  Q(A_1x;\mu_1,S_1) + Q(A_2x;\mu_2,S_2)  = Q(x;\mu_1',S_1') + Q(x;\mu_2',S_2').
$$
  Then the exponential terms on both sides of \eqref{exact_prod} are equal by
  \eqref{comp_sq:1} and \eqref{comp_sq:R}, and the scale factors are equal by inspection.
\end{proof}


\section{The Forward Pass}
  Now we can inductively evaluate \eqref{alpha:0}. To do so, we split the computation
  into two steps.  In the first step, which we call the {\em time update}, we multiply
\  $\alpha_{t-1}(u)$ by the state transition function $Pd(s\mid u)$ and integrate with respect
  to $u$. We will denote the result of the time update by $\hat{\alpha}_t(s)$.  It is the joint 
  likelihood of observations $x_1,\dots,x_{t-1}$ and state $s$ at time $t$.
  Then in the second step, which we call the {\em measurement update}, we multiply
  $\hat{\alpha}_t(s)$ by $X(x_t \mid s)$, the probability density of observing $x_t$ at time $t$
  in state $s$, to get $\alpha_t(s)$.  %Henceforth, we put $S'_T  := T^tS_TT$.

  First, for $1\le t \le N$ we inductively define
  \begin{align*}
    \hat{S}_{a,t-1} :&= T^{t}S_TT * S_{a,t-1},\\
%    \hat{\mu}_{a,t} :&= \hat{S}_{a,t}^{-1}(T^tS_Ts + S_{a,t-1}\mu_{a,t-1}),\\
    S_{a,t} &:= M^tS_xM  + T^{-T}\hat{S}_{a,t-1}T^{-1},\\
    \mu_{a,t} &:= S_{a,t}^{-1}(M^tS_xx_t + T^{-T}\hat{S}_{a,t-1}\mu_{a,t-1}),\\
    R_{a,t} &:= x_t^tS_xx_t + \mu_{a,t-1}^t\hat{S}_{a,t-1}\mu_{a,t-1} - \mu_{a,t}^tS_{a,t}\mu_{a,t},\\
    P_{a,t} &:= P_{a,t-1}|T|^{-1}\left(\frac{|S_x|\cdot|\hat{S}_{a,t-1}|}{2\pi|S_{a,t}|}\right)^{\frac{1}{2}}e^{-\frac{1}{2}R_{a,t}},\\
%    P_{a,t} &:= P_{a,t-1}N_{a,t}, \quad\text{and}\\
    P_{a,0} &:= 1.
  \end{align*}

\begin{Thm}\label{alpha:1}
  For each state $s$ and time $t \ge 1$,
$$
  \alpha_t(s) = P_{a,t}N(s;\mu_{a,t},S_{a,t})
$$
\end{Thm}

\begin{proof}

Proceeding by induction on $t$, we note that the case $t = 0$ holds by definition.
For the time update, we have
\begin{align}
  \hat{\alpha}_t(s) &= \int_{\R^n}Pd(s|u)\alpha_{t-1}(u)du \notag \\
  &= P_{a,t-1}\int_{\R^n}\N(Tu;s,S_T)\N(u;\mu_{a,t-1},S_{a,t-1})du \quad\text{(by symmetry of $s$ and $Tu$)}\notag\\
  &= P_{a,t-1}|T|^{-1}\N(T^{-1}s;\mu_{a,t-1},\hat{S}_{a,t-1}),\label{time_update} %\int_{\R^n}\N(u;\hat{\mu}_{a,t-1},\hat{S}_{a,t-1})du
\end{align}
where \eqref{time_update} was obtained by using \eqref{exact_prod} together with the fact that the normal
density integrates to unity.  Then multiplying by $X(x_t\mid s)$ and using \eqref{comp_sq:2} yields
\begin{align*}
  \alpha_t(s) &= X(x_t\mid s)\hat{\alpha}_t(s) \\
  &= P_{a,t-1}|T|^{-1}\N(Ms;x_t,S_x)\N(T^{-1}s;\mu_{a,t-1},\hat{S}_{a,t-1}) \\
  &= P_{a,t-1}|T|^{-1}(2\pi)^{-n}(|S_x||\hat{S}_{a,t-1}|)^{\frac{1}{2}}\exp\{-\frac{1}{2}(Q(T^{-1}s;
  \mu_{a,t-1},\hat{S}_{a,t-1}) + Q(Ms;x_t,S_x)\} \\
  &= P_{a,t-1}|T|^{-1}(2\pi)^{-n}(|S_x||\hat{S}_{a,t-1}|)^{\frac{1}{2}}\exp\{-\frac{1}{2}(Q(s;\mu_{a,t},S_{a,t}) + R_{a,t})\}\\
  &= P_{a,t-1}|T|^{-1}\left(\frac{|S_x||\hat{S}_{a,t-1}|}{|2\pi S_{a,t}|}\right)^{\frac{1}{2}}e^{-\frac{1}{2}R_{a,t}}
  \N(s;\mu_{a,t},S_{a,t})\\
  &= P_{a,t}\N(s;\mu_{a,t},S_{a,t}).
\end{align*}
\end{proof}

\section{The Backward Pass}
This calculation is very similar to the forward pass; the main difference being that we do
the measurement update first by multiplying by $X(x_{t+1}\mid u)$ to obtain $\hat{\beta}_{t+1}(u)$,
and then we integrate with respect to $Pd(u\mid s)du$ for the time update.  

\begin{Thm}\label{beta:1}
  For $t < N$, inductively define
\begin{align*}
  \hat{S}_{b,t+1} :&= M^tS_xM + S_{b,t+1};\\
  S_{b,t} :&= T^t(\hat{S}_{b,t+1}*S_T)T,\\
  \hat{\mu}_{b,t+1} :&= \hat{S}_{b,t+1}^{-1}(M^tS_xx_{t+1} + S_{b,t+1}\mu_{b,t+1}),\\
  \mu_{b,t} :&= T^{-1}\hat{\mu}_{b,t+1},\\
  R_{b,t} :&= x_{t+1}^tS_xx_{t+1} + \mu_{b,t+1}^tS_{b,t+1}\mu_{b,t+1} - \hat{\mu}_{b,t+1}^t\hat{S}_{b,t+1}\hat{\mu}_{b,t+1},\quad\text{and}\\
  P_{b,t} :&= P_{b,t+1}\left(\frac{|S_x|\cdot|S_{b,t+1}|}{|2\pi\hat{S}_{b,t+1}|}\right)^{\frac{1}{2}}e^{-\frac{1}{2}R_{b,t}}
\end{align*}
with initial values at $t = N$ given below. Then for each state $s$ and time $t < N$,
$$
  \beta_t(s) = P_{b,t}\N(s;\mu_{b,t},S_{b,t}).
$$
\end{Thm}

\begin{proof}
We proceed by reverse induction on $t$. However, to get started we first must  deal with
the special case of $\beta_{N}(u)$, the conditional probability of the future observations given
state $u$ at time $N$, but of course there are no such observations.  In the standard discrete
state HMM, we set $\beta_N(s) = 1$ for all states $s$ and this will work in our case as well by
setting $S_{b,N} = 0$, so we somewhat arbitrarily assume the following values
%provided that $M$ is 1-1. But we really need a proper gaussian 
%density in order to get the reverse induction started in all cases,
\begin{align*}
  S_{b,N} :&= 0,\\
  \mu_{b,N} :&= 0,\quad\text{and}\\
  P_{b,N} :&= 1.
\end{align*}
  For $t <  N$, we use \eqref{comp_sq:2} to get the measurement update 
  \begin{align*}
    \hat{\beta}_{t+1}(u) &= X(x_{t+1}\mid u)\beta_{t+1}(u)\\
    &= P_{b,t+1}\N(Mu;x_{t+1},S_x)\N(u;\mu_{b,t+1},S_{b,t+1})\\
    &= P_{b,t+1}\N_0\N(u;\hat{\mu}_{b,t+1},\hat{S}_{b,t+1})\\
%    &= P_{b,t+1}(2\pi)^{-n}(|S_x||S_{b,t+1}|)^{\frac{1}{2}}\exp\{-\frac{1}{2}(Q(Mu;x_{t+1},S_x) + Q(u;\mu_{b,t+1},S_{b,t+1}) \} \\
%    &= P_{b,t+1}(2\pi)^{-n}(|S_x||S_{b,t+1}|)^{\frac{1}{2}}\exp\{-\frac{1}{2}(Q(u;\hat{\mu}_{b,t+1},\hat{S}_{b,t+1}) + R_{b,t})\}\\
  &= P_{b,t+1}\left(\frac{|S_x||S_{b,t+1}|}{|2\pi \hat{S}_{b,t+1}|}\right)^{\frac{1}{2}}e^{-\frac{1}{2}R_{b,t}}
  \N(u;\hat{\mu}_{b,t+1},\hat{S}_{b,t+1})\\
    &= P_{b,t}\N(u;\hat{\mu}_{b,t+1},\hat{S}_{b,t+1}).
  \end{align*}
  $\hat{\beta}_{t+1}(u)$ is the conditional likelihood of state $u$ at time $t+1$, all observations $x_\tau~(\tau > t+1)$,
      {\em and} $x_{t+1}$. Then an application of \eqref{exact_prod} and \eqref{comp_sq:4} yields the time update
  \begin{align*}
    \beta_t(s) &= \int_{\R^n}\hat{\beta}_{t+1}(u)Pd(u\mid s)du \\
    &= P_{b,t}\int_{\R^n}\N(u;\hat{\mu}_{b,t+1},\hat{S}_{b,t+1})\N(u;Ts,S_T)du\\
    &=P_{b,t}\N(Ts;\hat{\mu}_{b,t+1},\hat{S}_{b,t+1}*S_T)\\
%    &=P_{b,t}\N(s;T^{-1}\hat{\mu}_{b,t+1},T^t(\hat{S}_{b,t+1}*S_T)T)\\
    &=P_{b,t}\N(s;\mu_{b,t},S_{b,t}).
  \end{align*}
\end{proof}
\newpage
\section{The Posterior Distributions}

Recall that $\gamma_t(s)$ is the posterior probability density of state $s$
at time $t$. 
\begin{Thm}
  Let notation be as in \eqref{alpha:1} and \eqref{beta:1}.  Define
\begin{align*}
  S_{c,t} :&= S_{a,t} + S_{b,t}, \\
  \mu_{c,t} :&= S_{c,t}^{-1}(S_{a,t}\mu_{a,t} + S_{b,t}\mu_{b,t}), \quad\text{and}\\
  P_{c,t} :&= P_{a,t}P_{b,t}\N(\mu_{a,t};\mu_{b,t},S_{a,t}*S_{b,t}), \quad\text{then} \\
 \gamma_t(s) &= \N(s;\mu_{c,t},S_{c,t}).
  \end{align*}
\end{Thm}
\begin{proof}
  From \eqref{alpha:1} and \eqref{beta:1} we have (using \eqref{exact_prod}
  as usual)
  \begin{align*}
  \gamma_t(s) \propto \alpha_t(s)\beta_t(s) &= P_{a,t}P_{b,t}\N(s;\mu_{a,t},S_{a,t})\N(s;\mu_{b,t},S_{b,t})\\
  &= P_{c,t}\N(s;\mu_{c,t},S_{c,t}).
  \end{align*}
\end{proof}

We also need the joint posterior probability density of state $s_{t-1}$ at time $t-1$ and state $s_t$ at time $t$,
which we denote by $\hat{\gamma}_t(s_{t-1},s_t)$.
\begin{Lem}\label{gamma_hat}
The joint likelihood of states $s_{t-1},s_t$ is
$$
    \hat{\gamma}_t(s_{t-1},s_t) = \N(s_t; Ts_{t-1},S_T)\gamma_{t-1}(s_{t-1}) .
$$
%% \begin{align*}
%%   \hat{\gamma}_t(s_{t-1},s_t) &= \N(s_{t-1};\mu_{t-1,t},S_{t-1,t}) + e^{-\frac{1}{2}s_t^tS_Ts_t},\quad\text{where}\\
%%   S_{t-1,t} &:= S_{c,t-1}+S_T',\\
%%   \mu_{t-1,t} &:= S_{t-1,t}^{-1}(T^tS_Ts_t + S_{c,t-1}\mu_{c,t-1}),\quad\text{and}\\
%% %   R &= -\frac{1}{2}s_t^tS_Ts_t
%%   \end{align*}
\end{Lem}
\begin{proof}
  By definition, the joint posterior likelihood of states $s_{t-1},s_t$ is the conditional likelihood of $s_t$ given
  $s_{t-1}$ times the posterior likelihood of $s_{t-1}$, as asserted.

%%   namely using \eqref{comp_sq:2} we have
%%    \begin{align*}
%%     \hat{\gamma}_t(s_{t-1},s_t) &= \N(s_t; Ts_{t-1},S_T)\gamma_{t-1}(s_{t-1}) \\
%%     &= \N(Ts_{t-1};s_t,S_T)\N(s_{t-1};\mu_{c,t-1},S_{c,t-1}) \\
%%     &\propto  \N(s_{t-1};\mu_{t-1,t},S_{t-1,t}) + e^{ -\frac{1}{2}s_t^tS_Ts_t} 
%%   \end{align*}
%%   where the proportionality factor does not depend on either $s_{t-1}$ or $s_t$, and
%%   \begin{align*}
%%     S_{t-1,t} :&= S'_T + S_{c,t-1},\\
%%     \mu_{t-1,t} :&= S_{t-1,t}^{-1}(T^tS_Ts_t+S_{c,t-1}\mu_{c,t-1}).%,\quad\text{and}\\
%% %    R :&= s_t^tS_Ts_t ,
%%   \end{align*}
\end{proof}

\newpage
\section{Re-estimating Parameters}
In this section, we derive re-estimation equations for the parameters using the EM algorithm.
\subsection{The Estimation Step}
We begin with the key lemma.
    
  \begin{Lem}\label{exp_log}
    Recall the notation of \eqref{comp_sq:1}. Let $\gamma := \N(x;\mu_0,S_0)$ be an $n$-dimensional gaussian,  let $S$ be
    positive definite, and let $A$ be any
    full rank $m\times{n}$ matrix. Then for any vector $\mu\in\R^n$, 
    \begin{align}
      E_{\gamma}(x-\mu)(x-\mu)^t &= S_0^{-1} + (\mu-\mu_0)(\mu-\mu_0)^t, \label{E_xx^t}\\
      E_{\gamma}(x-\mu)^t(x - \mu) &=  \tr(S_0^{-1}) + (\mu-\mu_0)^t(\mu-\mu_0), \label{E_x^tx}\\
      E_{\gamma}(x^tSx) &= \tr(S_0^{-1}S) + \mu_0^tS\mu_0, \quad\text{and} \label{E_x^tSx} \\
        E_{\gamma}{Q}(Ax;\mu,S) &=  \tr(S_0^{-1}A^tSA) + (\mu-A\mu_0)^tS(\mu-A\mu_0), \label{E_Q}
    \end{align}
  \end{Lem}
  \begin{proof}
    By definition, the covariance matrix for $\gamma$ is
    \begin{align*}
    S_0^{-1} &= E_{\gamma}(x-\mu_0)(x-\mu_0)^t = E_{\gamma}(xx^t) - E_{\gamma}(x\mu_0^t) - E_{\gamma}(\mu_0{x}^t) + E_{\gamma}(\mu_0\mu_0^t)\\
    &= E_{\gamma}(xx^t) - \mu_0\mu_0^t, \quad\text{whence}\\
    E_{\gamma}(x-\mu)(x-\mu)^t &= E_{\gamma}(xx^t) - E_{\gamma}(x\mu^t) - E_{\gamma}(\mu{x}^t) + E_{\gamma}(\mu\mu^t) \\
    &= S_0^{-1} + \mu_0\mu_0^t - \mu_0\mu^t-\mu\mu_0^t + \mu\mu^t\\
    &= S_0^{-1} + (\mu-\mu_0)(\mu-\mu_0)^t,
    \end{align*}
    proving \eqref{E_xx^t}; and \eqref{E_x^tx} follows by taking the trace.

    To prove \eqref{E_x^tSx} we let $C$ be the Cholesky matrix of $S$, so that
    $S = C^tC$.  Note that since $S$ is positive definite, $C$ is invertible.
    Making the change of variable $y = Cx,~ dy = |C|dx$ and using \eqref{E_x^tx}, we then have
      \begin{align*}
        E_{\gamma}(x^tSx) &= \int_{R^n}x^tSx~\N(x;\mu_0,S_0)dx\\
        &= \sqrt{(2\pi)^{-n}|S_0|}\int_{\R^n}x^tC^tCx~\exp\left\{-\frac{1}{2}Q(x;\mu_0,S_0)\right\}dx \\
        &= |C|^{-1}\sqrt{(2\pi)^{-n}|S_0|}\int_{\R^n}y^ty~\exp\left\{-\frac{1}{2}Q(C^{-1}y;\mu_0,S_0)\right\}dy.
      \end{align*}
      We can re-write the quadratic form as follows:
      \begin{align*}
        Q(C^{-1}y,\mu_0,S_0) &= (C^{-1}y-\mu_0)^tS_0(C^{-1}y-\mu_0)\\
        &= y^tC^{-T}S_0C^{-1}y - y^tC^{-T}S_0\mu_0 - \mu_0^tS_0C^{-1}y + \mu_0^tS_0\mu_0 \\
        &= y^tC^{-T}S_0C^{-1}y - 2y^t(C^{-T}S_0C^{-1})C\mu_0 + \mu_0C^t(C^{-T}S_0C^{-1})C\mu_0\\
        &= Q(y;C\mu_0,C^{-T}S_0C^{-1}).
      \end{align*}
      Substituting above yields
      \begin{align*}
        E_{\gamma}(x^tSx) &=  |C|^{-1}\sqrt{(2\pi)^{-n}|S_0|}\int_{\R^n}y^ty~\exp\left\{-\frac{1}{2}Q(y;C\mu_0,C^{-T}S_0C^{-1})\right\}dy \\
        &= \int_{\R^n}y^ty~\N(y;C\mu_0,C^{-T}S_0C^{-1})dy.
      \end{align*}
      Then using \eqref{E_x^tx} we get
      \begin{align*}
        E_{\gamma}(x^tSx) &=  \tr(CS_0^{-1}C^t) + \mu_0^tC^tC\mu_0 \\
        &= \tr(S_0^{-1}S) + \mu_0^tS\mu_0,
      \end{align*}
      as required.

    Finally, to prove \eqref{E_Q} we  use \eqref{E_x^tSx} (with $A^tSA$ in place of $S$):
      \begin{align*}
        E_{\gamma}Q(Ax;\mu,S) &= E_{\gamma}(x^tA^tSAx) - 2E_{\gamma}(x^tA^tS\mu) + E_{\gamma}(\mu^tS\mu) \\
        &= E_{\gamma}(x^tA^tSAx) - 2\mu_0^tA^tS\mu + \mu^tS\mu\\
        &= \tr(S_0^{-1}A^tSA) + \mu_0^tA^tSA\mu_0 - 2\mu_0^tA^tS\mu + \mu^tS\mu,\\
        &= \tr(S_0^{-1}A^tSA) + (\mu-A\mu_0)^tS(\mu- A\mu_0). %\mu_0^tA^tSA\mu_0 - 2\mu_0^tA^tS\mu + \mu^tS\mu,\\
      \end{align*}

completing the proof of \eqref{E_Q}.
  \end{proof}
  
Equipped with \eqref{E_Q}, we can now turn to the problem at hand.  Namely, if we are given a particular
state sequence $S := \{s_0,s_1,s_2,\dots,s_T\}$ and the data sequence $X := \{x_1,x_2,\dots,x_T\}$,
the probability density of $S$ given observations $X$ and  parameters $\theta := \{\mu_0,S_0,S_T,T,S_x,M\}$ is
\begin{equation} \label{density}
P(S\mid X,\theta) = \N(s_0;\mu_0,S_0)\prod_{t=1}^N\N(x_t;Ms_t,S_x)\N(s_t;Ts_{t-1},S_T).
\end{equation}

Note that $P(S\mid X, \theta)$ can also be viewed as  $L(\theta\mid X,S)$, the posterior likelihood of $\theta$
given observations $X$ and state sequence $S$.  The objective here is to choose $\theta$ to maximize $L(\theta)$.
We use the EM technology to do this, which begins with an initial choice of $\theta$, then re-estimates
parameters and (typically) iterates this procedure, replacing the initial parameters with their re-estimates .

We begin by defining 
$$
E(\theta,\bar{\theta}) := \int_{\R^{Nn}}P(S\mid X,\bar{\theta})\log{L(\theta\mid X,S)}dS,
$$
where $\bar{\theta}$ is the current set of parameters, and $\theta$ is the unknown set of new parameters
we wish to determine. So instead of maximizing the log-likelihood directly, we can maximize its expected
value with respect to the current posterior distribution on states, because it is a standard result \cite{Dempster}
that
$$
L(\theta) - L(\bar{\theta}) \ge E(\theta,\bar{\theta}) - E(\bar{\theta},\bar{\theta}).
$$
Thus, choosing $\theta$ to maximize $E(\theta,\bar{\theta})$ will increase the value of $L(\theta)$.

Expanding $E(\theta,\bar{\theta})$, we have 
\begin{align}
  E(\theta,\bar{\theta}) &= -(2N+1)\frac{n\log(2\pi)}{2} + \frac{1}{2}(E_0 + E_1 + E_2),\quad\text{where}\notag\\
  E_0 :&=\int_{\R^{Nn}}P(S\mid X,\bar{\theta})\{\log|S_0| - Q(s_0;\mu_0,S_0)\}dS,\label{E0}\\
  E_1 :&= \int_{\R^{Nn}}P(S\mid X,\bar{\theta})\left\{\sum_{t=1}^N\log|S_x| - Q(x_t;Ms_t,S_x)\right\}dS,
\label{E1}\\
E_2 :&= \int_{\R^{Nn}}P(S\mid X,\bar{\theta})\left\{\sum_{t=1}^N\log|S_T| - Q(s_t;Ts_{t-1},S_T)\right\}dS,
\label{E2}
\end{align}
Since the expected value of a constant is just that constant, $-n\log(2\pi)/2$ can be moved outside each
integral sign and ignored in the optimization.  And since none of the parameters of $\theta$ appear in more
than one of the $E_i$, we can optimize $E$ by optimizing each of $E_0,E_1,E_2$ separately.

We begin with $E_0$. Since the only component of $S$ in the integrand of \eqref{E0} is $s_0$, the integrand is a constant
with respect to all $s_t$ for $t > 0$, and the expected value
collapses to the marginal 
expected value of the integrand with respect to the marginalization of $P(S\mid X,\bar{\theta})$ at $s_0$, which is just 
$\gamma_0(s_0)$.  Furthermore, after pulling the constant and the summation 
outside the integral in \eqref{E1} we have the same collapse of the posterior to the marginalization at $s_t$, which
is just $\gamma_t(s_t)$.  It follows that
\begin{align}
  \notag E_0 &= \log|S_0| - E_{\gamma_0(s_0)}Q(s_0;\mu_0,S_0)\\
  &= \log|S_0| - \tr(\overline{S}_{c,0}^{-1}S_0) - (\mu_0-\bar{\mu}_{c,0})^tS_0(\mu_0 - \bar{\mu}_{c,0}),
  \label{E0:1}\quad\text{and}\\
  \notag E_1 &= N\log|S_x| - \sum_{t=1}^NE_{\gamma_t(s_t)}Q(Ms_t;x_t,S_x)\\
  &= N\log|S_x| - \sum_{t=1}^N\tr(\overline{S}_{c,t}^{-1}M^tS_xM) + (x_t-M\bar{\mu}_{c,t})^tS_x(x_t-M\bar{\mu}_{c,t})\label{E1:1}.
\end{align}


 Turning finally to  \eqref{E2}, the integral collapses to the 
joint marginal expectation over $s_t$ and $s_{t-1}$ at time $t$ given by \eqref{gamma_hat}, and we get
\begin{equation}\label{E2:1}
  \begin{split}
    E_2 &= N\log|S_T| - \sum_{t=1}^N\int_{R^n}\left(\int_{\R^{n}}\Q(Ts_{t-1};s_t,S_T)\gamma_{t-1}(s_{t-1})\N(s_t;\overline{T}s_{t-1},
    \overline{S}_T)\right)ds_{t-1}ds_t\\
    &= N\log|S_T| - \sum_{t=1}^N\int_{R^n}\gamma_{t-1}(s_{t-1})\left(\int_{\R^{n}}\Q(s_t;Ts_{t-1},S_T)\N(s_t;\overline{T}s_{t-1},
    \overline{S}_T)ds_t\right)ds_{t-1}\\
  \end{split}
  \end{equation}
In order to evaluate the inner integral in \eqref{E2:1}, we use \eqref{E_Q} 
(with $\gamma := N(s_t;\overline{T}s_{t-1},\overline{S}_T))$:
$$
E_{\gamma}Q(s_t;Ts_{t-1},S_T) = \tr(\overline{S}_T^{-1}S_T) + s_{t-1}^t(T - \overline{T})^tS_T(T - \overline{T})s_{t-1}.
$$
Thus, substituting this into \eqref{E2:1} and using \eqref{E_x^tSx}, we obtain
\begin{equation}\label{E2:2}
  \begin{split}
    E_2 &= N\log|S_T| - N\tr(\overline{S}_T^{-1}S_T) - \sum_{t=1}^N E_{\gamma_{t-1}(s_{t-1})}s_{t-1}^t(T - \overline{T})^t
    S_T(T - \overline{T})s_{t-1} \\
    &= N\log|S_T| - N\tr(\overline{S}_T^{-1}S_T) -\sum_{t=1}^N \tr(S_{c,t-1}^{-1}S_T) + \mu_{c,t-1}^t(\Delta{T})^tS_T
    (\Delta{T})\mu_{c,t-1},
  \end{split}
\end{equation}
where $\Delta{T} := T - \overline{T}$.
\subsection{Some Preliminaries}
If $f$ is any scalar-valued function of an $m\times{n}$ matrix $A = a_{ij}$, we denote by $\partial{f}/\partial{A}$ the 
$m\times{n}$ matrix whose $(i,j)$-entry is $\partial{f}/\partial{a_{ij}}$.  This also applies to column vectors
(when $n=1$). We will also denote by $C = A\circ{B}$ the element-wise product of $A$ and $B$ whose $i,j$-entry is
$C_{ij} = A_{ij}B_{ij}$.

In particular, if $m=n$, $A$ is symmetric, and $f(A) = |A|$,  then the usual expansion 
in minors along each row shows that 
$$
\frac{\partial{|A|}}{\partial{A}} = B\circ{A}^* = |A|B\circ{A}^{-1},
$$
where $A^*$ is the transposed matrix of cofactors and $B_{ij} := 2-\delta_{ij}$ for all $i,j$.
The off-diagonal elements are doubled because $A$ is symmetric.
It follows that
\begin{equation}\label{partial_logdet}
  \frac{\partial{\log|A|}}{\partial{A}} = B\circ{A}^{-1}. 
\end{equation}

In addition, since $Q(x;\mu,S)$ is linear in the coefficients of $S$, we see that
\begin{equation}\label{dQ_dS}
\frac{\partial{Q(x;\mu,S)}}{\partial{S}} = B\circ(x-\mu)(x-\mu)^t,
\end{equation}
and it is also easy to verify that
\begin{equation}\label{dQ_dmu}
\frac{\partial{Q(x;\mu,S)}}{\partial{\mu}} = 2S(\mu-x).
\end{equation}

Finally, to reestimate $M$ and $T$, we need

\begin{Lem}\label{dQdA}
  Let $S$ be a symmetric $m\times{m}$ matrix, let $A$ be any $m\times{n}$ matrix,
  and let $\mu,\nu\in \R^n$.  Define
  \begin{align*}
  Q :&= \mu^tA^tSA\nu,\quad\text{then}\\
  \frac{\partial{Q}}{\partial{A}} &= SA(\mu\nu^t+\nu\mu^t).
  \end{align*}
\end{Lem}
\begin{proof}
Expanding, we have
$$
Q = \mu^tA^tSA\mu = \sum_{i,j,k,l}\mu_iA_{ji}S_{jk}A_{kl}\nu_l.
$$
Collecting terms which involve $A_{qr}$, if $j=q$ and $i=r$, we get
\begin{equation}\label{SA}
A_{qr}\mu_r\sum_{k,l}S_{qk}A_{kl}\nu_l,
\end{equation}
while if $k=q$ and $l=r$, we get
\begin{equation}\label{A^tS}
A_{qr}\nu_r\sum_{i,j}\mu_iA_{ji}S_{jq}.
\end{equation}
However after replacing $i$ by $l$, $j$ by $k$ and using the fact that $S$ is symmetric,
\eqref{SA} and \eqref{A^tS} are identical except that $\mu$ and $\nu$ are interchanged.
But both equations have the common term
$\mu_r\nu_rA_{qr}^2S_{qq}$ when $j = k = q$ and $i = l = r$.
Then
\begin{align}
  \notag\frac{\partial{Q}}{\partial{A_{qr}}} &= \mu_r\sum_{(k,l)\neq(q,r)}S_{qk}A_{kl}\nu_l
  +\nu_r\sum_{(k,l)\neq(q,r)}S_{qk}A_{kl}\mu_l+ 2\mu_r\nu_rA_{qr}S_{qq}\\
  &= \mu_r\sum_{k,l}S_{qk}A_{kl}\nu_l + \nu_r\sum_{k,l}S_{qk}A_{kl}\mu_l\notag\\
  &= \sum_{k,l}S_{qk}A_{kl}(\mu_l\nu_r + \nu_l\mu_r).\label{dQ_0}
\end{align}

We conclude that the matrix whose $(q,r)$ term is given by \eqref{dQ_0} is
$$
\frac{\partial{Q}}{\partial{A}} = SA(\mu\nu^t+\nu\mu^t)
$$
as asserted.
\end{proof}

\begin{Cor}\label{dM}
  $$
  \frac{\partial{Q(x;A\mu,S)}}{\partial{A}} = -2Sx\mu^t + 2SA\mu\mu^t.
  $$
\end{Cor}
\begin{proof}
Expanding, we have
$$
Q := (x-A\mu)^tS(x-A\mu) = x^tSx - 2x^tSA\mu + \mu^tA^tSA\mu.
$$
Differentiating the linear term is just a special case of
$$
\frac{\partial}{\partial{A}}y^tAx = yx^t.
$$
The quadratic term follows from \eqref{dQdA} with $\nu = \mu$.
\end{proof}
Finally, we have
\begin{Lem} \label{d_tr_prod}Let $X$ be an $m\times{n}$ matrix, and let $Y$ be an $n\times{m}$ matrix.
  Then
  $$
  \frac{\partial\tr(XY)}{\partial{X}} = B\circ{Y^t}.
  $$
\end{Lem}
\begin{proof}
  We have $ \tr(AB) = \sum_{i,j}A_{ij}B_{ji}$, whence
  $$
  \frac{\partial\tr(AB)}{\partial{A_{ij}}} = (2-\delta_{ij})B_{ji}.
  $$
  \end{proof}

\subsection{Solving for  $\mu_0$ and $S_0$}
We first optimize \eqref{E0:1} with respect to $\mu_0$ using \eqref{dQ_dmu}:
\begin{equation} \label{mu_0}
  \begin{split}
  0 = \frac{\partial{E_0}}{\partial{\mu_0}} &= 2S(\mu_0-\bar{\mu}_{c,_0}),\quad\text{hence}\\
    \mu_0 &= \bar{\mu}_{c,0}.\\
  \end{split}
\end{equation}

Then we optimize \eqref{E0:1} with respect to $S_0$ using \eqref{partial_logdet}, \eqref{d_tr_prod}, and \eqref{dQ_dS}. Moreover,
we can set $\mu_0 = \bar{\mu}_{c,0}$ by \eqref{mu_0}:

\begin{equation}\label{S_0}
  \begin{split}
  0 = \frac{\partial{E_0}}{\partial{S_0}} &= B\circ{S_0^{-1}}-\overline{S}_0^{-1}, \quad\text{hence}\\
  S_0 = \overline{S}_0.\\
  \end{split}
\end{equation}


\subsection{Solving for M and $S_M$}
Next, we optimize \eqref{E1:1} with respect to $M$ :


  \subsection{Solving for $\mu_0$ and $S_0$ (old)}

Armed with the above preliminaries, we first optimize $E_0$ with respect to $\mu_0$: 
 
\begin{align}
0 = \frac{\partial{E_0}}{\partial{\mu_0}} &= \int_{\R^n}\gamma_0(s_0)\frac{\partial{Q(s_0;\mu_0,S_0)}}{\partial{\mu_0}}ds_0 \notag\\
&= \int_{\R^n}\gamma_0(s_0)(2S_0(\mu_0-s_0))ds_0\notag\\
&= 2S_0\int_{\R^n}\gamma_0(s_0)(\mu_0-s_0)ds_0 \notag\\
&= 2S_0(\mu_0-\mu_{c,0}), \quad\text{hence}\notag\\
\mu_0 &= \mu_{c,0}
\end{align}

Next, we optimize $E_0$ with respect to $S_0$, and we can set $\mu_0 = \mu_{c,0}$ by \eqref{mu_0}:
\begin{align*}
  0 = \frac{\partial{E_0}}{\partial{S_0}} &= \frac{\partial\log|S_0|}{\partial{S_0}}
  -\int_{\R^n}\gamma_0(s_0)\frac{\partial{Q(s_0;\mu_{c,0},S_0)}}{\partial{S_0}}ds_0 \\
&= (2-\delta_{ij})S_0^{-1} - (2-\delta_{ij})\int_{\R^n}\gamma_0(s_0)(s_0-\mu_{c,0})(s_0-\mu_{c,0})^tds_0.
\end{align*}
Not surprisingly, it follows that
\begin{equation}\label{Sigma_0}
  \begin{split}
    S_0^{-1} &= \int_{\R^n}\gamma_0(s_0)(s_0-\mu_{c,0})(s_0-\mu_{c,0})^tds_0 = S_{c,0}^{-1}, \quad\text{i.e.}\\
    S_0 &= S_{c,0}.
  \end{split}
\end{equation}

We note that above and in what follows, we have vector and matrix integrands.  Of course, this is just a convenient
notation for sets of integrals, each one having a particular vector or matrix entry as the integrand.

\subsection{Solving for M and $S_M$} (old)
To optimize $E_1$, we first differentiate with respect to $\overline{M}$ using \eqref{dM}.
Setting the derivative to zero we need to solve for $\overline{M}$:
\begin{align*}
  0 &= \frac{\partial{E_1}}{\partial\overline{M}}
  = -\sum_{t=1}^N\int_{\R^n}\gamma_t(s_t)\frac{\partial}{\partial\overline{M}}Q(x_t,\overline{M}s_t,S_M)ds_t\\
  &= 2\sum_{t=1}^N\int_{\R^n}\gamma_t(s_t)(S_Mx_ts_t^t - S_M\overline{M}s_ts_t^t)ds_t\\
  &= 2\sum_{t=1}^N(S_Mx_t\mu_{c,t}^t - S_M\overline{M}(S_{c,t}^{-1} + \mu_{c,t}\mu_{c,t}^t).
\end{align*}
Multiplying through by $\frac{1}{2}S_M^{-1}$, we find that
\begin{equation}\label{M_bar}
  \begin{split}
    \overline{M} &= \Gamma_1\Gamma_2^{-1}, \quad\text{where}\\
    \Gamma_1 :&= \sum_{t=1}^Nx_t\mu_{c,t}^t, \quad\text{and}\\
    \Gamma_2 :&= \sum_{i=1}^NS_{c,t}^{-1} + \mu_{c,t}\mu_{c,t}^t.
  \end{split}
\end{equation}

Note that $\overline{M}$ does not depend on $S_M$, so we can optimize $E_1$ with respect to $S_M$
using the new value of $\overline{M}$:
\begin{align*}
  0 = \frac{\partial{E_1}}{\partial{S_M}} &=
   N\frac{\partial{\log|S_M|}}{\partial{S_M}}
  - \sum_{t=1}^N\int_{\R^n}\gamma_t(s_t)\frac{\partial{Q(x_t;\overline{M}s_t,S_M)}}{\partial{S_M}}ds_t \\
  &= N(B\circ{S}_M^{-1}) - B\circ\sum_{t=1}^N
  \int_{\R^n}\gamma_t(s_t)(x_t-\overline{M}s_t)(x_t-\overline{M}s_t)^tds,
\end{align*}
where $B_{ij} := 2-\delta_{ij}$ as above.  Dividing by $B_{ij}$ in each component, we see that
\begin{equation}\label{S_x}
  \begin{split}
    S_M^{-1} &= \frac{1}{N}\sum_{t=1}^N\int_{\R^n}\gamma_t(s_t)(x_tx_t^t - x_ts_t^t\overline{M}^t
    - \overline{M}s_tx_t^t + \overline{M}s_ts_t^t\overline{M}^t)ds_t \\
    &= \frac{1}{N}\sum_{t=1}^N(x_tx_t^t - x_t\mu_{c,t}^t\overline{M}^t - \overline{M}\mu_{c,t}x_t^t +
    \overline{M}(S_{c,t}^{-1}+\mu_{c,t}\mu_{c,t}^t)\overline{M}^t)\\
    &= \frac{1}{N}\sum_{t=1}^N(x_t-\overline{M}\mu_{c,t})(x_t-\overline{M}\mu_{c,t})^t +
    \overline{M}S_{c,t}^{-1}\overline{M}^t.
  \end{split}
\end{equation}

\subsection{Solving for T and $S_T$} (old)
Finally, we optimize $E_2$ first by solving
$$
\frac{\partial{E_2}}{\partial{\overline{T}}} = 0
$$
for $\overline{T}$ optionally subject to the condition that $\overline{T}$ is a companion matrix.  We do this
by computing the full E-M update for $T$, but for a companion matrix we only use the result to update the variable entries,
namely $T_{nj}$ for $j > 1$, while leaving all the constant entries unchanged.

Thus, using \eqref{gamma_hat} and \eqref{dM}, we have
\begin{equation}\label{T:1}
  \begin{split}
0 &= \sum_{t=1}^N\int_{\R^{2n}}\hat{\gamma}_t(s_{t-1},s_t)
\frac{\partial{Q(s_t;\overline{T}s_{t-1},S_T)}}{\partial\overline{T}}ds_{t-1}ds_t \\
&= -2\sum_{t=1}^N\int_{\R^{2n}}\N(s_{t-1};\mu_{t-1,t},S_{t-1,t})\gamma_t(s_t)(S_Ts_ts_{t-1}^t -
S_T\overline{T}s_{t-1}s_{t-1}^t) ds_{t-1}ds_t \\
&= -2S_T\sum_{t=1}^N\int_{\R^{n}}\gamma_t(s_t) (s_t\mu_{t-1,t}^t -
\overline{T}(S_{t-1,t}^{-1} + \mu_{t-1,t}\mu_{t-1,t}^t))ds_t \\
&= 2S_T\sum_{t=1}^N\left(\overline{T}S_{t-1,t}^{-1} - \int_{\R^n}\gamma_t(s_t)s_t\mu_{t-1,t}^tds_t +
\overline{T}\int_{\R^n}\gamma_t(s_t)\mu_{t-1,t}\mu_{t-1,t}^tds_t\right).\\
  \end{split}
\end{equation}
So we have two integrals to evaluate:
\begin{equation}\label{I_1}
  \begin{split}
  I_{1,t} :&= \int_{\R^n}\gamma_t(s_t)s_t\mu_{t-1,t}^tds_t \\
  &= \int_{\R^n}\gamma_t(s_t)s_t(s_t^tS_TT + \mu_{a,t-1}^tS_{a,t-1})S_{t-1,t}^{-1})ds_t \\
  &= (S_{c,t}^{-1} + \mu_{c,t}\mu_{c,t}^t)S_TTS_{t-1,t}^{-1} + \mu_{c,t}\mu_{a,t-1}^tS_{a,t-1}S_{t-1,t}^{-1},
  \end{split}
  \end{equation}
and
\begin{equation}\label{I_2}
  \begin{split}
  I_{2,t} :&= \int_{\R^n}\gamma_t(s_t)\mu_{t-1,t}\mu_{t-1,t}^tds_t \\
  &= \int_{\R^n}\gamma_t(s_t)S_{t-1,t}^{-1}(T^tS_Ts_ts_t^tS_TT + T^tS_Ts_t\mu_{a,t-1}^tS_{a,t-1} \\
  &+ S_{a,t-1}\mu_{a,t-1}s_t^tS_TT + S_{a,t-1}\mu_{a,t-1}\mu_{a,t-1}^tS_{a,t-1}) S_{t,t-1}^{-1}ds_t \\
  &= S_{t-1,t}^{-1}[T^tS_T(S_{c,t}^{-1`}+\mu_{c,t}\mu_{c,t}^t)S_TT + T^tS_T\mu_{c,t}\mu_{a,t-1}^tS_{a,t-1} \\
  &+ S_{a,t-1}\mu_{a,t-1}\mu_{c,t}^tS_TT + S_{a,t-1}\mu_{a,t-1}\mu_{a,t-1}^tS_{a,t-1}]S_{t-1,t}^{-1}. \\
\end{split}
\end{equation}

Note that the matrices $T$ and $S_T$ appearing in \eqref{I_1} and \eqref{I_2} are the {\em previous}
values, as opposed to the unkowns $\overline{T}$ and $S_T$ for which we are solving.

To simplify notation, we put
\begin{align*}
S_* :&= S_{t-1,t}^{-1}S_{a,t-1},\\
S_+ :&= S_{t-1,t}^{-1}T^tS_T,\\
\nu_1 :&= S_+\mu_{c,t},\quad\text{and}\\
\nu_2 :&= S_*\mu_{a,t-1}.
\end{align*}
Then
\begin{align*}
  I_{1,t} &= S_{c,t}^{-1}S_+^t +\mu_{c,t}(\nu_1^t + \nu_2^t), \quad\text{and}\\
  I_{2,t} &= S_+S_{c,t}^{-1}S_+^t+\nu_1\nu_1^t + \nu_1\nu_2^t + \nu_2\nu_1^t + \nu_2\nu_2^t \\
      &= S_+S_{c,t}^{-1}S_+^t + (\nu_1+\nu_2)(\nu_1+\nu_2)^t.
\end{align*}

After multiplying by $S_T^{-1}$, \eqref{T:1} becomes
\begin{align*}
 0 &= \sum_{t=1}^N\left(\overline{T}S_{t-1,t}^{-1} - I_{1,t} + \overline{T}I_{2,t}\right)\\
  &= \overline{T}\Gamma_2 - \Gamma_1, \quad\text{where} \\
  \Gamma_2 :&= \sum_{i=1}^N(S_{t-1,t}^{-1} + I_{2,t}),\quad\text{and}\\
  \Gamma_1 :&= \sum_{i=1}^NI_{1,t}.
\end{align*}
Thus,  we have
\begin{equation}\label{T:2}
  \overline{T} = \Gamma_1\Gamma_2^{-1}.
\end{equation}
Note that if $T$ is a companion matrix, we only update $T_{nj}$ for $j > 1$ from \eqref{T:2} and
leave all other entries unchanged.

Finally, we complete our optimization of $E_2$ by differentiating with respect to $S_T$ and using \eqref{gamma_hat} and \eqref{partial_logdet}:
\begin{equation}\label{Sigma_Tr:0}
  \begin{split}
  0 &= \frac{\partial{E_2}}{\partial{S_T}} = \frac{\partial{\log|S_T|}}{\partial{S}_T}
  - \sum_{t=1}^N\int_{\R^{2n}}\hat{\gamma}_t(s_{t-1},s_t)\frac{\partial{Q(s_t;\overline{T}s_{t-1},S_T)}}
  {\partial{S}_T}ds_{t-1}ds_t \\
  &= N(B\circ{S}_T^{-1}) -B\circ\sum_{t=1}^N\int_{\R^{2n}}\N(s_{t-1};\mu_{t-1,t},S_{t-1,t})
  \gamma_t(s_t)(\overline{T}s_{t-1}-s_t)(\overline{T}s_{t-1}-s_t)^tds_{t-1}ds_t.
  \end{split}
\end{equation}

As noted earlier, $\gamma_t(s_t)$ does not depend on $s_{t-1}$, so we can integrate first with respect to $s_{t-1}$, with
a result analagous to \eqref{S_x}:
\begin{align*}
  \int_{-\infty}^{\infty}&\N(s_{t-1};\mu_{t-1,t},S_{t-1,t})(\overline{T}s_{t-1}-s_t)
  (\overline{T}s_{t-1}-s_t)^tds_{t-1}\\ &= S_{t-1,t}^{-1} +
(\overline{T}(\mu_{t-1,t}-s'_t)(\mu_{t-1,t}-s'_t)^t\overline{T}^t,
\end{align*}
where $s'_t = \overline{T}^{-1}s_t$.
Substituting this result into \eqref{Sigma_Tr:0}, we get
\begin{equation}\label{Sigma_Tr:1}
  S_T^{-1} =  \frac{1}{N}\sum_{t=1}^N\left(S_{t-1,t}^{-1} +\int_{-\infty}^{\infty}\gamma_t(s_t)
  \overline{T}(\mu_{t-1,t}-s'_t)(\mu_{t-1,t}-s'_t)^t\overline{T}^tds_t\right).
\end{equation}

However, there is a problem evaluating this integral because after checking \eqref{gamma_hat}, we see that $\mu_{t-1,t}$
depends on $s'_t = \overline{T}^{-1}s_t$, namely 
\begin{equation}\label{s-mu_1}
  \begin{split}
    \left(\mu_{t-1,t}-s'_t\right) &= (S_{t-1,t}^{-1}T^{t}S_TT - I)s'_t + S_{t-1,t}^{-1}S_{a,t-1}\mu_{a,t-1}, \\
    &= (S_{t-1,t}^{-1}T^{t}S_TT)s'_t - S_{t-1,t}^{-1}S_{t-1,t}) + S_{t-1,t}^{-1}S_{a,t-1}\mu_{a,t-1}, \\
    &= S_{t-1,t}^{-1}(T^{t}S_TT - S_{t-1,t} )s'_t + S_{t-1,t}^{-1}S_{a,t-1}\mu_{a,t-1}, \\
    &= S_{t-1,t}^{-1}S_{a,t-1}(\mu_{a,t-1}-s'_t),\\
    &= S_*(\mu_{a,t-1}-s_t')
  \end{split}
\end{equation}

Fortunately, neither $\mu_{a,t-1}$ nor $S_*$ depends on $s_t$,
so using \eqref{gamma_hat}, \eqref{Sigma_Tr:1} becomes
\begin{equation}\label{S_Tr:2}
  \begin{split}
  S_T^{-1} &=\frac{1}{N}\sum_{t=1}^N\left(S_{t-1,t}^{-1} + \overline{T}S_*
  \left[\int_{-\infty}^{\infty}\gamma_t(s_t)(\mu_{a,t-1}-s'_t)(\mu_{a,t-1}-s'_t)^tds_t\right]S_*^t\overline{T}^t\right)\\
   &=\frac{1}{N}\sum_{t=1}^NS_{t-1,t}^{-1} + \overline{T}S_*
           [S_{c,t}^{-1}+(\mu_{a,t-1}-\overline{T}^{-1}\mu_{c,t})(\mu_{a,t-1}-\overline{T}^{-1}\mu_{c,t})^t]
           S_*^t\overline{T}^t.
  \end{split}
\end{equation}
\newpage


\appendix
\section{A Standard Form for Orthonormal Matrices}\label{ortho_std_form}
Let $A$ be a real orthonormal $n\times n$ matrix, i.e. such that $A^tA = AA^t = I$.  Since $A+A^t$ is real
symmetric, it has an  orthonormal basis of eigenvectors    $\{v_0,\dots,v_n\}$ with
real eigenvalues $\{\lambda_i \mid 0\le i < n\}$.
Let $\hat{v}_0 = Av_0$, then $A^{-1} \hat{v}_0 = A^t\hat{v}_0 = v_0$, and 
\begin{align*}
  (A + A^t)v_0 &= \lambda_0 v_0,\\
  \hat{v}_0 & = \lambda_0 v_0 - A^tv_0, \\
  A\hat{v}_0 &= \lambda_0 Av_0 - AA^tv_0 = \lambda_0 \hat{v}_0 - v_0.
\end{align*}
Thus, $(A+A^t)\hat{v}_0 = \lambda_0 \hat{v}_0$, so either $\hat{v}_0 = \mu v_0$ for some eigenvalue $\mu$ of $A$ (which, because $A$ is orthonormal, must be $\pm 1$) or the subspace $V_0:= <v_0,\hat{v}_0>$ is a 2-dimensional eigenspace of $A+A^t$ with eigenvalue $\lambda_0$.  In either case, $V_0$  is invariant under both $A$ and $A^t$.
%% and 
%% with respect to the basis $\{v_0,\hat{v}_0\}$ we have

%% $$
%% A|_{V_0} \sim \left[ \begin{matrix}
%%     0 & -1 \\
 %%     1 & \lambda_0\\
%%     \end{matrix}
%%   \right].
%% $$
Since $A$ is orthonormal, it is immediate that $V_0^{\perp}$,
the orthogonal complement of $V_0$, is also $A$ and $A^t$ invariant, so an obvious induction argument shows that
$$
V = W_1\oplus W_{-1} \bigoplus_{i=0}^{k-1} V_i
$$
where $A|_{W_j} = jI$ for $j = -1,1$, and $V_i$ is a 2-dimensional eigenspace of $A+A^t$ with eigenvalue $\lambda_i$
%% A|_{V_i} \sim \left[ \begin{matrix}
%%     0 & -1 \\
%%     1 & \lambda_i\\
%% \end{matrix}
%%   \right].
%% $$
for $0\le i < k$, where $\{\lambda_0,\dots,\lambda_{k-1}\}$ are the eigenvalues of $A+A^t$ different
from $\pm 1$.  It follows that if $W$ is the matrix of eigenvectors of $A+A^t$ with respect to a suitable
re-ordering of the eigenvalues, i.e. sorted in descending order, then $W^tAW$ is a block diagonal matrix
whose entries are either $\pm 1$ or $2\times 2$ rotation matrices.  Then for each corresponding $2\times 2$
block $B$ of $W^tAW$,



\section{Solving for T by least-squares}
We first consider the general problem of solving for T by least squares, and then specialize to a constrained
version which uses Newton's method (non-linear least squares).  We don't have a proof that this approach actually
increases the score, but in practice it seems to work well.

\subsection{The general method} We observe that $T$ maps the true mean state at time $t$ to the true mean state at
time $t+1$.  Since $\gamma_t$ is the posterior mean state at time $t$, we can use $\gamma_t$ and
$\gamma_{t+1}$ to build overdetermined linear equations $T\gamma_t = \gamma_{t+1}$ on the coefficients of $T$
and solve them by least squares.
This approach seems to work fairly well, but in practice we often see that $\det(T)$ becomes negative,
which means that we can't compute the log score, which we need in order to verify that each iteration in fact
improves the log-likelihood.  Another problem is that if the data set is long, repeated application of $T$ to the
mean posterior state may produce an unstable result.  We can solve these problems by orthogonalizing $T$
after every iteration using Gram-Schmidt (via QR-reduction), but the following algorithm is an alternate.

\subsection{Non-linear least squares}
We can solve both problems by constraining $T$ to be
orthonormal, and by the previous appendix, we can choose coordinates for the state space with respect to which
$T$ has block diagonal form, with either $\pm 1$ or $2\times 2$ rotation matrices on the diagonal.  This restricts the
parameterization of $T$ to a set of angles $\{\theta_1,\dots,\theta_k\}$, one for each $2\times 2$ block.  In addition,
the block form of $T$ allows us to independently set up and solve equations for each $\theta_i$ separately.  Consider
one such block
$$
B_i = \left[\begin{matrix} \cos\theta_i & \sin\theta_i \\ -\sin\theta_i & \cos\theta_i\end{matrix}\right].
$$
If we let $u_t$ and $v_t$ be the components of $\gamma_t$ corresponding to $B_i$ for all $t$, then we have two
approximate equations
\begin{align*}
  u_{t+1} &\approx \hat{u}_{t+1}(\theta_i) := u_t\cos(\theta_i)  + v_t\sin(\theta_i),\\
  v_{t+1} &\approx \hat{v}_{t+1}(\theta_i) := -u_t\sin(\theta_i ) + v_t\cos(\theta_i) 
\end{align*}
which are linear in $\cos\theta_i$ and $\sin\theta_i$, but we need linear equations in $\theta_i$.  We obtain these
iteratively by using the first-order expansions of $\hat{u}_{t+1}(\theta_i)$ and $\hat{v}_{t+1}(\theta_i)$ around the value
$\bar{\theta_i}$ from the previous iteration and solving for delta values:
\begin{equation}\label{nls}
  \begin{split}
    u_{t+1}&\approx  \hat{u}_{t+1}(\bar{\theta_i}) + \hat{u}'_{t+1}(\bar{\theta_i})(\theta_i-\bar{\theta_i}) =\hat{u}_{t+1}(\bar{\theta_i})+  [-u_t\sin(\bar{\theta_i})+ v_t\cos(\bar{\theta_i})] (\theta_i-\bar{\theta_i}) ,\\
     v_{t+1} &\approx  \hat{v}_{t+1}(\bar{\theta_i})  + \hat{v}'_{t+1}(\bar{\theta_i})(\theta_i-\bar{\theta_i})
       = \hat{v}_{t+1}(\bar{\theta_i}) + [-u_t\cos(\bar{\theta_i}) - v_t\sin(\bar{\theta_i})] (\theta_i-\bar{\theta_i}).\\
  \end{split}
\end{equation}
We then use least squares to get  $\theta_i$ from the overdetermined system of equations \eqref{nls} for all $t$,
namely
$$
\theta_i-\bar{\theta_i} = \frac{\sum_t (u_{t+1}-\hat{u}_{t+1}(\bar{\theta_i}))\hat{u}'_{t+1}(\bar{\theta_i})+
  (v_{t+1}-\hat{v}_{t+1}(\bar{\theta_i}))\hat{v}'_{t+1}(\bar{\theta_i})}{\sum_t\hat{u}'_{t+1}(\bar{\theta_i})^2 +
\hat{v}'_{t+1}(\bar{\theta_i})^2}.
$$

Due to the various approximations involved, it may be advisable to bound $|\theta_i-\bar{\theta_i}|$.  Also, an
inner iteration is possible using the same values for the $\gamma_t$ but updating $\bar{\theta_i} \leftarrow \theta_i$.

%% \section{An Elementary Inequality}

%% \begin{Lem}\label{Q_func} Suppose that $f_1(\theta),\dots,f_n(\theta)$ are positive real-valued functions.
%% Let $f(\theta):= \sum_{i=1}^nf_i(\theta)$, and define 
%% $$
%% E(\bar{\theta},\theta):= \sum_{i=1}^nf_i(\bar{\theta})\log(f_i(\theta)).
%% $$
%% Then $ f(\theta) - f(\bar{\theta}) \ge E(\bar{\theta},\theta) - E(\bar{\theta},\bar{\theta})$ with equality only if
%% $E(\bar{\theta},\theta) = E(\bar{\theta},\bar{\theta})$.
%% \end{Lem}
%% \begin{proof}
%% The equation of the tangent line to the graph of $y = \log(\theta)$ at $\theta = 1$ is $y = \theta - 1$.
%% Since $\log(\theta)$ is concave down, it follows that 
%% $\log(\theta) \le \theta - 1$, with equality if and only if $\theta = 1$. Using this, we have
%% \begin{align*}
%% E(\bar{\theta},\theta) - E(\bar{\theta},\bar{\theta}) &= \sum_{i=1}^nf_i(\bar{\theta})[\log(f_i(\theta) - \log(f_i(\bar{\theta}))] \\
%% &= \sum_{i=1}^nf_i(\bar{\theta})\log(f_i(\theta)/f_i(\bar{\theta})) \\
%% &\le \sum_{i=1}^nf_i(\bar{\theta})[f_i(\theta)/f_i(\bar{\theta}) - 1] \\
%% &= \sum_{i=1}^n[f_i(\theta) - f_i(\bar{\theta})] \\
%% &= f(\theta) - f(\bar{\theta}),
%% \end{align*}
%% with equality if and only if $f_i(\theta)/f_i(\bar{\theta}) = 1$ for all $i$.

%% \end{proof}

%% $A$ is
%% similar to a block-diagonal matrix with $2\times 2$ blocks


%% \begin{align*}
%%   \hat{v}_0 &= \lambda_0 A^{-1}\hat{v}_0 - A^{-1}v_0,\\
%%   A^tv_0 &= A^{-1}v_0 = \lambda_0 v_0 - \hat{v}_0.
%% \end{align*}
%% In particular, the 2-dimensional subspace $V_0:= <v_0,\hat{v}_0>$ is $A$-invariant and $A^t$-invariant, and 
%% \quad\text{and}\quad A^t  \sim \left[\begin{matrix}
%%     \lambda_0 & 1 \\
%%     -1 & 0\\
%%   \end{matrix}
%%   \right].\\
%% $$

%% Thus, with respect to the basis $\{v_0,\hat{v}_0\}$ we have the diagonal matrix
%% $$
%% A+A^t \sim \left[\begin{matrix}
%%     \lambda_0 & 0 \\
%%     0 & \lambda_0 \\
%%   \end{matrix}
%%   \right]
%% $$
%% In other words, $v_0$ and $\hat{v}_0$ are both eigenvectors of $A+A^t$ with eigenvalue $\lambda_0$.

\begin{thebibliography}{9}
\bibitem{Hinton}
  Ghahramani, Z. and Hinton, G.,
  \emph{Parameter Estimation for Linear Dynamical Systems},
  Department of Computer Science, University of Toronto,
  Technical Report CRG-TR-96-2,
  Feb. 22, 1996
\bibitem{Aravkin}
  Aravkin, A. Burke, J. Ljung, L. Lozano, A. and Pillonetto, G.,
  \emph{Generalized Kalman Smoothing: Modeling and Algorithms},
  arXiv:1609.06369,
  Sep. 25, 2016
\bibitem{Bilmes}
  Bilmes, J.,
   \emph{A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation
    for Gaussian Maixture and Hidden Markov Models},
  Department of Electrical Engineering and Computer Science, University of California, Berkeley,
  Technical Report 97-021
  April, 1998
\bibitem{Dempster}
  Dempster, A Laird, N and Rubin, D,
   \emph{Maximum-likelihood from incomplete data via the EM algorithm},
  J.Royal Statist. Soc. Ser. B, 39,
  1977
\bibitem{Shumway}
  Shumway, R and Stoffer, D,
  \emph{Time Series Analysis and Its Applications, 3rd Ed. (Blue printing)}
  Springer, New York 2016
    
\end{thebibliography}
 
\end{document}


    %% To evaluate $E_{\gamma}(x^tA^tSAx)$, we let $C$ be the Cholesky matrix of $A^tSA$, so that
    %% $A^tSA = C^tC$.  Note that since $A$
    %% has full rank, $A^tSA$ is invertible, and so is $C$. Making the change
    %% of variable $y = Cx,~ dy = |C|dx$ and using \eqref{E_x^tx}, we then have
    %%   \begin{align*}
    %%     E_{\gamma}(x^tA^tSAx) &= \int_{R^n}x^tA^tSAx~\N(x;\mu_0,S_0)dx\\
    %%     &= \sqrt{(2\pi)^{-n}|S_0|}\int_{\R^n}x^tA^tSAx~\exp\left\{-\frac{1}{2}Q(x;\mu_0,S_0)\right\}dx \\
    %%     &= |C|^{-1}\sqrt{(2\pi)^{-n}|S_0|}\int_{\R^n}y^ty~\exp\left\{-\frac{1}{2}Q(C^{-1}y;\mu_0,S_0)\right\}dy.
    %%   \end{align*}
    %%   We can re-write the quadratic form as follows:
    %%   \begin{align*}
    %%     Q(C^{-1}y,\mu_0,S_0) &= (C^{-1}y-\mu_0)^tS_0(C^{-1}y-\mu_0)\\
    %%     &= y^tC^{-T}S_0C^{-1}y - y^tC^{-T}S_0\mu_0 - \mu_0^tS_0C^{-1}y + \mu_0^tS_0\mu_0 \\
    %%     &= y^tC^{-T}S_0C^{-1}y - 2y^t(C^{-T}S_0C^{-1})C\mu_0 + \mu_0C^t(C^{-T}S_0C^{-1})C\mu_0\\
    %%     &= Q(y;C\mu_0,C^{-T}S_0C^{-1}).
    %%   \end{align*}
    %%   Substituting above yields
    %%   \begin{align*}
    %%     E_{\gamma}(x^tA^tSAx) &=  |C|^{-1}\sqrt{(2\pi)^{-n}|S_0|}\int_{\R^n}y^ty~\exp\left\{-\frac{1}{2}Q(y;C\mu_0,C^{-T}S_0C^{-1})\right\}dy \\
    %%     &= \int_{\R^n}y^ty~\N(y;C\mu_0,C^{-T}S_0C^{-1})dy.
    %%   \end{align*}
    %%   Then using \eqref{E_x^tx} we get
    %%   \begin{align*}
    %%     E_{\gamma}(x^tA^tSAx) &=  \tr(CS_0^{-1}C^t) + \mu_0^tC^tC\mu_0 \\
    %%     &= \tr(S_0^{-1}A^tSA) + \mu_0^tA^tSA\mu_0.
    %%   \end{align*}



%%     &= N\log|S_T|-\sum_{t=1}^N\int_{R^n}\left(tr(S_{t-1,t}^{-1}T^tS_TT) +
%%     (s_t - T\mu_{t-1,t})^tS_T(s_t-T\mu_{t-1,t})\right)\gamma(s_t)ds_t\\
%%     &= N\log|S_T|-\sum_{t=1}^N\left(tr(S_{t-1,t}^{-1}T^tS_TT) + E_{\gamma_t(s_t)}Q(s_t;T\mu_{t-1,t},S_T)\right).\\
%%     \end{split}
%% \end{equation}
%% At this point we would like to apply \eqref{E_Q} to this last expectation, but unfortunately after checking \eqref{gamma_hat},
%% we find that $\mu_{t-1,t}$ depends on $s_t$.  Namely, with $s_t' := \overline{T}^{-1}s_t$ and
%% $\overline{S}_T' := \overline{T}^t\overline{S}_T\overline{T}$ we have
%% \begin{align*}
%% \mu_{t-1,t}-s'_t &= (\overline{S}_{t-1,t}^{-1}\overline{S}'_T - I)s'_t +
%%   \overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1}\mu_{a,t-1}, \\
%%   &= (\overline{S}_{t-1,t}^{-1}\overline{S}'_T) - \overline{S}_{t-1,t}^{-1}\overline{S}_{t-1,t})s'_t +
%%   \overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1}\mu_{a,t-1}, \\
%%     &= \overline{S}_{t-1,t}^{-1}(\overline{S}'_T - \overline{S}_{t-1,t} )s'_t + \overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1}\mu_{a,t-1}, \\
%%     &= \overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1}(\mu_{a,t-1}-s'_t), \quad\text{hence}\\
%%     \mu_{t-1,t} &= \overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1}\mu_{a,t-1} - (\overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1}-I)\overline{T}^{-1}s_t,\quad\text{and therefore}\\
%%     T\mu_{t-1,t} - s_t &= T\bar{v}_t - (T\overline{S}_* + I)s_t, 
%%   \end{align*}
%% where
%% \begin{align*}
%%   \overline{S}_* :&= (\overline{W}_t - I)\overline{T}^{-1} ,\\
%%   \overline{W}_t :&= \overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1},\quad\text{and}\\
%%   \bar{v}_t :&= W_t\mu_{a,t-1}.
%% \end{align*}

%% Note that both $\overline{S}_*$ and $\bar{v}_t$  depend only on the known posterior parameters as indicated
%% by the bar notation, and neither one depends on $s_t$.

%% We can now re-write \eqref{E2:1} in a form amenable to \eqref{E_Q}:

%% \begin{equation}\label{mu_t-1,t}
%%   \begin{split}
%%     Q(s_t,T\mu_{t-1,t},S_T) &= ((T\overline{S}_*+I)s_t - T\bar{v}_t)^tS_T((T\overline{S}_* + I)s_t - T\bar{v}_t),\quad\text{and}\\
%%     E_{\gamma_t(s_t)}Q(s_t,T\mu_{t-1,t},S_T) &= E_{\gamma_t(s_t)}Q((T\overline{S}_*+ I)s_t;T\bar{v}_t,S_T)\\
%%     &= tr(\overline{S}_{c,t}^{-1}(T\overline{S}_*+I)^tS_T(T\overline{S}_* + I)) + (T\bar{v}_t - \mu_{c,t})^tS_T(T\bar{v}_t-\mu_{c,t}).
%%   \end{split}
%% \end{equation}
