\documentclass[12pt,leqno]{article}
\include{article_defs}
\title{A Continuous State Hidden Markov Model}
\author{David M. Goldschmidt}
%\oddsidemargin 10 pt \evensidemargin 10 pt \marginparwidth 0.75 in \textwidth
%6.0 true in \topmargin -40 pt \textheight 8.8 true in 
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lfoot{}
%\rfoot{\thepage}
\begin{document}
%\renewcommand{\footrulewidth}{0.4pt}
\newcommand{\p}{\ensuremath{u}}
\newcommand{\VV}{V}
\maketitle


\section{Introduction}
In this expository paper, we define a discrete-time hidden Markov model (HMM) with a continuous state.
The Markov state transitions
are governed by a deterministic linear map with added gaussian noise, and the outputs are similarly obtained from the
state via another deterministic linear map with added gaussian noise.  In other words, we are re-casting a discrete-time
linear dynamical system as a hidden Markov model.  The result is 1) a derivation of the Kalman filtering and smoothing
equations \cite{Aravkin} obtained as a consequence of the Baum-Welch algorithm \cite{Bilmes} adapted to the case of a
gaussian-distributed state, and 2) the derivation of re-estimation equations for all model parameters based on the EM algorithm
\cite{Dempster}.  All these equations (with different notation) can be found in \cite{Hinton}, albeit with little or no
exposition/derivation.  \footnote{There is a vast literature on Kalman filtering and smoothing.  \cite{Aravkin} has an extensive bibliography.}
We conclude the paper with some numerical results on simulated data.
Some familiarity with the discrete state HMM methodology is assumed.  A good reference is \cite{Bilmes}. 

\section{Preliminary Definitions and Notation}
Let $\mu$ and $x$ be vectors in $\R^n$ and let $S$ be an $n\times{n}$ positive
definite real symmetric matrix.  Recall that the gaussian probability density
on $\R^n$ is given by 
$$
\N(x;\mu,S) := (2\pi)^{-\frac{n}{2}}|S|^{\frac{1}{2}}\exp\left\{-\frac{1}{2}
(x - \mu)^TS(x-\mu)\right\}.
$$
So here and below, capital $S$ denotes an inverse covariance matrix. Also, note the symmetry $\N(x;\mu,S) = \N(\mu;x,S)$
which we will exploit freely.

We now define a Hidden Markov Model with state space $\R^n$ as follows.  Given a state $s_t\in\R^n$ and an observation $x_t\in\R^m$ at time $t$, the output density is
$$
X(x_t\mid s) := \N(x_t;M_ts_t + b_t,S_x),
$$
where $M_t$ is a linear map from the state space to the
measurement space, $b_t$ is a bias, and $S_x$ is an $m\times{m}$ positive definite real symmetric matrix.  Thus, $x_t$
is normally distributed with mean $M_ts_t$ and inverse covariance matrix $S_x$.  In this paper, $S_x$ and $M := M_t$ are
time-independent model parameters which we usually seek to re-estimate from the data.  The bias $b_t$ can
usually be subtracted from the data so we assume here that $b_t = 0$ for all $t$. 
In the simplest case, $M_t = I$ for all $t$ so that $m = n$ and $s$ is itself the mean of the output distribution.  
The state process is a discrete time continuous state Markov process with transition probability density
from state $s_0$ at time $t$ to state $s_1$ at time $t+1$ given by
$$
Pd(s_1\mid s_0) :=  \N(s_1;T_ts_0+c_t,S_T),
$$
where $S_T$ is a time-independent model parameter, $T_t$, the ``transition matrix'' is an invertible linear map  
defining the deterministic component of the time update, and $c_t$ is a bias.  In this paper, we assume that
$T_t = T$ is a time-independent model parameter which we may
also re-estimate from data, and that $c_t = 0$.  Note that $T$ is not a transition matrix in the
discrete-state HMM sense.

To avoid scaling issues, we constrain $T$ to be unimodular.  In addition,  we may assume without serious loss of generality
that the characteristic polynomial of $T$ has distinct roots, which is generically true.  This implies that $T$
is similar to a companion matrix, which in this paper will always mean a matrix $(t_{ij})$ such that:
$t_{ij} = 0$ for all $i < n$ and all $j \neq i+1$, and $t_{i,i+1} = 1$ for all $i < n$.  And if there is no
preferred basis for the state space, which is an application-dependent issue,
we may optionally assume that $T$ is a companion matrix.  This is a convenient assumption because it's easy to
constrain $T$ to be unimodular in this case.  Namely,
since $|T| = (-1)^{n-1}t_{n1}$ for such a matrix, we also assume that $t_{n1} = (-1)^{n-1}$.  Thus, $T$ has $n-1$
degrees of freedom given by the coefficients $t_{nj}$ for $j > 1$.  If we are re-estimating $T$, we constrain the $M$-step
of the $EM$ algorithm in order to maintain these conditions on subsequent Baum-Welch iterations.

If we assume $T$ is a companion matrix, then in effect the state vector becomes the vector of all $n$ lags of a
univariate $AR(n)$ process, except that at every time step we add more correlated noise to each lag until it leaves
the AR-window. Thus, each term becomes less and less determinate as it recedes into the past.
And it is convenient to note that companion matrices are invertible in closed form.  Namely, let $T^{-1} = (u_{ij})$, then
it is easy to check that $u_{ij} = 0$ for all $i > 1$ and all $j\neq i-1$,
$u_{i,i-1} = 1$ for all $i > 1$, $ u_{1j} = -t_{i,j+1}$ for $j < n$, and $u_{1n} = (-1)^{n-1}$.   

As usual, we are given observations $\{x_t\mid 1\le t\le N\}$ and model parameters $\theta := \{M,S_x,T,S_T,S_0,\mu_0\}$
where $S_0$ and $\mu_0$ are defined below.  For each observation time $t$ and state $s$, we define
\begin{align*}
  \alpha_t(s) :&= L(x_1,x_2,\dots,x_t~ \& \text{state $s$ at time $t$}\mid \theta ),\\
  \beta_t(s) :&= L(x_{t+1},\dots,x_T \mid ~\text{state $s$ at time $t$},\theta)\\
  \gamma_t(s) :&= \frac{\alpha_t(s)\beta_t(s)}{\int_{\R^n}\alpha_t(s)\beta_t(s)ds},
\end{align*}
where $L$ denotes likelihood, i.e. a non-negative function which can be normalized to a probability density by dividing
by its integral.
Thus, $\alpha_t(s)$ is the joint likelihood of state $s$ at time $t$ and the observations up to (and including) time $t$,
$\beta_t(s)$ is the {\em conditional} likelihood of the future observations given state $s$ at time $t$, and $\gamma_t(s)$
is the posterior probability density of state $s$ at time $t$.

It is clear from the definitions that the following recursions are satisfied:
\begin{align}
\alpha_t(s) &= X(x_t \mid s)\int_{\R^n}\alpha_{t-1}(u)Pd(s \mid u)du,\quad\text{and}\label{alpha:0}\\
\beta_t(s) &= \int_{\R^{n}}Pd(u \mid s)X(x_{t+1} \mid u)\beta_{t+1}(u)du.\label{beta:0}
\end{align}

We initialize these recursions with
\begin{align*}
\alpha_0(s) :&= \N(s;\mu_0,S_0)\quad\text{and} \\
\beta_{T}(s) :&= \N(s;0,I), \quad\text{the standard unit normal},
\end{align*}
where $S_0$ and $\mu_0$ are additional model parameters.

\section{Completing the Square} 
Because everything in sight is gaussian, the above integrals can be evaluated in closed form
using the following lemma, which is used repeatedly below.
\begin{Lem}
 Define the quadratic form
  $$
 Q(x;\mu,S) := (x-\mu)^TS(x-\mu),
  $$
  where $x,\mu\in \R^n$ and  $S$ is a symmetric positive semi-definite $n\times{n}$
  matrix. Let $A_i$ be an $m_i\times{n}$ matrix, $\mu_i\in\R^{n_i} ~ (i = 1,2)$,
  and let $S_1$,$S_2$ be symmetric positive semi-definite. Put
  $$
  S_i' := A_i^TS_iA_i\quad (i = 1,2),
  $$
  and assume that at least one of the $S_i'$ is positive definite.  Then 
\begin{equation}\label{comp_sq:1}
  Q(A_1x;\mu_1,S_1)+Q(A_2x;\mu_2,S_2) = Q(x;\mu,S) + R(\mu_1,\mu_2,S_1,S_2,A_1,A_2),
\end{equation}
where 
\begin{align}
S &= S_1' + S_2', \label{sigma}\\
\mu &= S^{-1}(A_1^TS_1\mu_1 + A_2^TS_2\mu_2),\quad\text{and}\label{mu}\\
R &= \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2 - \mu^TS\mu. \label{R_def} 
\end{align}
Furthermore, 
\begin{equation}\label{comp_sq:3}
  S_1'S^{-1}S_2' = S_2'S^{-1}S_1'.
\end{equation}
Moreover, if $A_1$ and $A_2$ are invertible, then with $\mu_i' := A_i^{-1}\mu_i ~ (i = 1,2)$ we have
\begin{equation}\label{comp_sq:R}
R = (\mu_2'-\mu_1')^TS_1'S^{-1}S_2'(\mu_2'-\mu_1').
\end{equation}
\end{Lem}
\begin{proof}
  Note that at least one of $S_1'$, $S_2'$ is positive definite and the other is positive semi-definite,
so $S$ is invertible.  Expanding the left-hand side of \eqref{comp_sq:1} and combining
like terms, we get
$$
x^T(S_1'+ S_2')x -2x^T(A_1^TS_1\mu_1+ A_2^TS_2\mu_2) 
+ \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2.
$$
Completing the square, we get
$$
Q(A_1x;\mu_1,S_1) + Q(A_2x;\mu_2,S_2) = (x-\mu)^TS(x-\mu) -\mu^TS\mu + \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2,
$$
proving the first three assertions.
To prove \eqref{comp_sq:3}, assume for the moment that both $S_1'$ and $S_2'$ are invertible.
Then
\begin{align*}
S_1'^{-1}SS_2'^{-1} &= S_1'^{-1}(S_1'+S_2')S_2'^{-1} = S_1'^{-1}+S_2'^{-1} = S_2'^{-1}SS_1'^{-1},\quad\text{hence}\\
S_2'S^{-1}S_1' &= (S_1'^{-1}+S_2'^{-1})^{-1} = S_1'S^{-1}S_2'.
\end{align*}
Now for $i = 1,2$, $S_i'+\epsilon{I}$ is positive definite and thus invertible for any $\epsilon > 0$
so \eqref{comp_sq:3} holds for $S_i'+\epsilon{I}$ and all $\epsilon > 0$.  Taking the limit
we conclude that it holds for $\epsilon = 0$ as well.

Finally, suppose that both of the $A_i$ are invertible. Then it's easy to see that

\begin{equation}\label{comp_sq:4}
Q(A_ix;\mu_i,S_i) = Q(x;\mu_i',S_i')\quad(i = 1,2),
\end{equation}
and \eqref{mu} becomes
\begin{equation} \label{mu'}
\mu = S^{-1}(S_1'\mu_1' + S_2'\mu_2')
\end{equation}.
We can re-write this as
\begin{equation}\label{mu:1}
\mu = S^{-1}(S_1'\mu_1' + S_2'\mu_1' + S_2'(\mu_2'-\mu_1)) = \mu_1' + S^{-1}S_2'(\mu_2'-\mu_1'),
\end{equation}
and by symmetry we have
\begin{equation}\label{mu:2}
  \mu = \mu_2' + S^{-1}S_1'(\mu_1'-\mu_2').
\end{equation}
Then from \eqref{mu'} we have
$$
\mu^TS\mu = \mu^T(S_1'\mu_1' + S_2'\mu_2'),
$$
and substituting \eqref{mu:1} in the first term and \eqref{mu:2} in the second, we get
$$
\mu^TS\mu = \mu_1'^TS_1'\mu_1' + (\mu_2'-\mu_1')^TS_2'S^{-1}S_1'\mu_1' + \mu_2'^TS_2'\mu_2'+(\mu_1'-\mu_2')^TS_1'S^{-1}S_2'\mu_2'.
$$
Finally, \eqref{comp_sq:3} yields
$$
\mu^TS\mu = \mu_1'^TS_1'\mu_1' + \mu_2'^TS_2'\mu_2' - (\mu_2'-\mu_1')^TS_1'S^{-1}S_2'(\mu_2'-\mu_1')
$$
and combining this with \eqref{R_def} proves \eqref{comp_sq:R} and completes the proof.
\end{proof}

By virtue of \eqref{comp_sq:3}, we define
$$
S_1*S_2 := S_1(S_1+S_2)^{-1}S_2.
$$
It's easy to check that if $X$ and $Y$ are invertible, then
\begin{equation}\label{comp_sq:XY}
  (XS_1Y)*(XS_2Y) = X(S_1*S_2)Y. 
\end{equation}

We primarily apply \eqref{comp_sq:1} below in the form: 
\begin{Cor}\label{comp_sq:2}
$$
  \N(A_1x;\mu_1,S_1)\N(A_2x;\mu_2,S_2) = \N_0\N(x;\mu,S),
  $$
where $\mu$ and $S$ are as in the lemma, and 
$$
\N_0 := \left(\frac{|S_1|\cdot|S_2|}{|2\pi{S}|}\right)^{\frac{1}{2}}\exp\{-\frac{1}{2}(\mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2-\mu^TS\mu)\}.
$$

In particular, $\N_0$ does not depend on $x$, and
\begin{equation}\label{comp_sq:int}
  \int_{\R^{n}}  \N(A_1x;\mu_1,S_1)\N(A_2x;\mu_2,S_2)dx = \N_0.
\end{equation}
\qed
\end{Cor}

\begin{Cor}\label{exact_prod} 
 If $A_1$ and $A_2$ are both invertible, then 
$$
 \N(A_1x;\mu_1,S_1)\N(A_2x;\mu_2,S_2)   = \frac{1}{|A_1|\cdot|A_2|}\N(x,\mu,S)\N(\mu_1';\mu_2',S_1'*S_2'),
$$
 where the notation is as above.
\end{Cor}
\begin{proof}

  Using \eqref{comp_sq:4}, we have
$$
  Q(A_1x;\mu_1,S_1) + Q(A_2x;\mu_2,S_2)  = Q(x;\mu_1',S_1') + Q(x;\mu_2',S_2').
$$
  Then the exponential terms on both sides of \eqref{exact_prod} are equal by
  \eqref{comp_sq:1} and \eqref{comp_sq:R}, and the scale factors are equal by inspection.
\end{proof}


\section{The Forward Pass}
  Now we can inductively evaluate \eqref{alpha:0}. To do so, we split the computation
  into two steps.  In the first step, which we call the {\em time update}, we multiply
  $\alpha_{t-1}(u)$ by the state transition function $Pd(s\mid u)$ and integrate with respect
  to $u$. We will denote the result of the time update by $\hat{\alpha}_t(s)$.  It is the joint 
  likelihood of observations $x_1,\dots,x_{t-1}$ and state $s$ at time $t$.
  Then in the second step, which we call the {\em measurement update}, we multiply
  $\hat{\alpha}_t(s)$ by $X(x_t \mid s)$, the probability density of observing $x_t$ at time $t$
  in state $s$, to get $\alpha_t(s)$.  %Henceforth, we put $S'_T  := T^TS_TT$.

  First, for $1\le t \le N$ we inductively define
  \begin{align*}
    \hat{S}_{a,t-1} :&= T^{t}S_TT * S_{a,t-1},\\
%    \hat{\mu}_{a,t} :&= \hat{S}_{a,t}^{-1}(T^TS_Ts + S_{a,t-1}\mu_{a,t-1}),\\
    S_{a,t} &:= M^TS_xM  + T^{-T}\hat{S}_{a,t-1}T^{-1},\\
    \mu_{a,t} &:= S_{a,t}^{-1}(M^TS_xx_t + T^{-T}\hat{S}_{a,t-1}\mu_{a,t-1}),\\
    R_{a,t} &:= x_t^TS_xx_t + \mu_{a,t-1}^T\hat{S}_{a,t-1}\mu_{a,t-1} - \mu_{a,t}^TS_{a,t}\mu_{a,t},\\
    P_{a,t} &:= P_{a,t-1}|T|^{-1}\left(\frac{|S_x|\cdot|\hat{S}_{a,t-1}|}{2\pi|S_{a,t}|}\right)^{\frac{1}{2}}e^{-\frac{1}{2}R_{a,t}},\\
%    P_{a,t} &:= P_{a,t-1}N_{a,t}, \quad\text{and}\\
    P_{a,0} &:= 1.
  \end{align*}

\begin{Thm}\label{alpha:1}
  For each state $s$ and time $t \ge 1$,
$$
  \alpha_t(s) = P_{a,t}N(s;\mu_{a,t},S_{a,t})
$$
\end{Thm}

\begin{proof}

Proceeding by induction on $t$, we note that the case $t = 0$ holds by definition.
For the time update, we have
\begin{align}
  \hat{\alpha}_t(s) &= \int_{\R^n}Pd(s|u)\alpha_{t-1}(u)du \notag \\
  &= P_{a,t-1}\int_{\R^n}\N(Tu;s,S_T)\N(u;\mu_{a,t-1},S_{a,t-1})du \quad\text{(by symmetry of $s$ and $Tu$)}\notag\\
  &= P_{a,t-1}|T|^{-1}\N(T^{-1}s;\mu_{a,t-1},\hat{S}_{a,t-1}),\label{time_update} %\int_{\R^n}\N(u;\hat{\mu}_{a,t-1},\hat{S}_{a,t-1})du
\end{align}
where \eqref{time_update} was obtained by using \eqref{exact_prod} together with the fact that the normal
density integrates to unity.  Then multiplying by $X(x_t\mid s)$ and using \eqref{comp_sq:2} yields
\begin{align*}
  \alpha_t(s) &= X(x_t\mid s)\hat{\alpha}_t(s) \\
  &= P_{a,t-1}|T|^{-1}\N(Ms;x_t,S_x)\N(T^{-1}s;\mu_{a,t-1},\hat{S}_{a,t-1}) \\
  &= P_{a,t-1}|T|^{-1}(2\pi)^{-n}(|S_x||\hat{S}_{a,t-1}|)^{\frac{1}{2}}\exp\{-\frac{1}{2}(Q(T^{-1}s;
  \mu_{a,t-1},\hat{S}_{a,t-1}) + Q(Ms;x_t,S_x)\} \\
  &= P_{a,t-1}|T|^{-1}(2\pi)^{-n}(|S_x||\hat{S}_{a,t-1}|)^{\frac{1}{2}}\exp\{-\frac{1}{2}(Q(s;\mu_{a,t},S_{a,t}) + R_{a,t})\}\\
  &= P_{a,t-1}|T|^{-1}\left(\frac{|S_x||\hat{S}_{a,t-1}|}{|2\pi S_{a,t}|}\right)^{\frac{1}{2}}e^{-\frac{1}{2}R_{a,t}}
  \N(s;\mu_{a,t},S_{a,t})\\
  &= P_{a,t}\N(s;\mu_{a,t},S_{a,t}).
\end{align*}
\end{proof}

\section{The Backward Pass}
This calculation is very similar to the forward pass; the main difference being that we do
the measurement update first by multiplying by $X(x_{t+1}\mid u)$ to obtain $\hat{\beta}_{t+1}(u)$,
and then we integrate with respect to $Pd(u\mid s)du$ for the time update.  

\begin{Thm}\label{beta:1}
  For $t < N$, inductively define
\begin{align*}
  \hat{S}_{b,t+1} :&= M^TS_xM + S_{b,t+1};\\
  S_{b,t} :&= T^T(\hat{S}_{b,t+1}*S_T)T,\\
  \hat{\mu}_{b,t+1} :&= \hat{S}_{b,t+1}^{-1}(M^TS_xx_{t+1} + S_{b,t+1}\mu_{b,t+1}),\\
  \mu_{b,t} :&= T^{-1}\hat{\mu}_{b,t+1},\\
  R_{b,t} :&= x_{t+1}^TS_xx_{t+1} + \mu_{b,t+1}^TS_{b,t+1}\mu_{b,t+1} - \hat{\mu}_{b,t+1}^T\hat{S}_{b,t+1}\hat{\mu}_{b,t+1},\quad\text{and}\\
  P_{b,t} :&= P_{b,t+1}\left(\frac{|S_x|\cdot|S_{b,t+1}|}{|2\pi\hat{S}_{b,t+1}|}\right)^{\frac{1}{2}}e^{-\frac{1}{2}R_{b,t}}
\end{align*}
with initial values at $t = N$ given below. Then for each state $s$ and time $t < N$,
$$
  \beta_t(s) = P_{b,t}\N(s;\mu_{b,t},S_{b,t}).
$$
\end{Thm}

\begin{proof}
We proceed by reverse induction on $t$. However, to get started we first must  deal with
the special case of $\beta_{N}(u)$, the conditional probability of the future observations given
state $u$ at time $N$, but of course there are no such observations.  In the standard discrete
state HMM, we set $\beta_N(s) = 1$ for all states $s$ and this will work in our case as well by
setting $S_{b,N} = 0$, so we somewhat arbitrarily assume the following values
%provided that $M$ is 1-1. But we really need a proper gaussian 
%density in order to get the reverse induction started in all cases,
\begin{align*}
  S_{b,N} :&= 0,\\
  \mu_{b,N} :&= 0,\quad\text{and}\\
  P_{b,N} :&= 1.
\end{align*}
  For $t <  N$, we use \eqref{comp_sq:2} to get the measurement update 
  \begin{align*}
    \hat{\beta}_{t+1}(u) &= X(x_{t+1}\mid u)\beta_{t+1}(u)\\
    &= P_{b,t+1}\N(Mu;x_{t+1},S_x)\N(u;\mu_{b,t+1},S_{b,t+1})\\
    &= P_{b,t+1}(2\pi)^{-n}(|S_x||S_{b,t+1}|)^{\frac{1}{2}}\exp\{-\frac{1}{2}(Q(Mu;x_{t+1},S_x) + Q(u;\mu_{b,t+1},S_{b,t+1}) \} \\
    &= P_{b,t+1}(2\pi)^{-n}(|S_x||S_{b,t+1}|)^{\frac{1}{2}}\exp\{-\frac{1}{2}(Q(u;\hat{\mu}_{b,t+1},\hat{S}_{b,t+1}) + R_{b,t})\}\\
  &= P_{b,t+1}\left(\frac{|S_x||S_{b,t+1}|}{|2\pi \hat{S}_{b,t+1}|}\right)^{\frac{1}{2}}e^{-\frac{1}{2}R_{b,t}}
  \N(u;\hat{\mu}_{b,t+1},\hat{S}_{b,t+1})\\
    &= P_{b,t}\N(u;\hat{\mu}_{b,t+1},\hat{S}_{b,t+1}).
  \end{align*}
  $\hat{\beta}_{t+1}(u)$ is the conditional likelihood of state $u$ at time $t+1$, all observations $x_\tau~(\tau > t+1)$,
      {\em and} $x_{t+1}$. Then an application of \eqref{exact_prod} and \eqref{comp_sq:4} yields the time update
  \begin{align*}
    \beta_t(s) &= \int_{\R^n}\hat{\beta}_{t+1}(u)Pd(u\mid s)du \\
    &= P_{b,t}\int_{\R^n}\N(u;\hat{\mu}_{b,t+1},\hat{S}_{b,t+1})\N(u;Ts,S_T)du\\
    &=P_{b,t}\N(Ts;\hat{\mu}_{b,t+1},\hat{S}_{b,t+1}*S_T)\\
%    &=P_{b,t}\N(s;T^{-1}\hat{\mu}_{b,t+1},T^T(\hat{S}_{b,t+1}*S_T)T)\\
    &=P_{b,t}\N(s;\mu_{b,t},S_{b,t}).
  \end{align*}
\end{proof}
\newpage
\section{The Posterior Distributions}

Recall that $\gamma_t(s)$ is the posterior probability density of state $s$
at time $t$. 
\begin{Thm}
  Let notation be as in \eqref{alpha:1} and \eqref{beta:1}.  Define
\begin{align*}
  S_{c,t} :&= S_{a,t} + S_{b,t}, \\
  \mu_{c,t} :&= S_{c,t}^{-1}(S_{a,t}\mu_{a,t} + S_{b,t}\mu_{b,t}), \quad\text{and}\\
  P_{c,t} :&= P_{a,t}P_{b,t}\N(\mu_{a,t};\mu_{b,t},S_{a,t}*S_{b,t}), \quad\text{then} \\
 \gamma_t(s) &= \N(s;\mu_{c,t},S_{c,t}).
  \end{align*}
\end{Thm}
\begin{proof}
  From \eqref{alpha:1} and \eqref{beta:1} we have (using \eqref{exact_prod}
  as usual)
  \begin{align*}
  \gamma_t(s) \propto \alpha_t(s)\beta_t(s) &= P_{a,t}P_{b,t}\N(s;\mu_{a,t},S_{a,t})\N(s;\mu_{b,t},S_{b,t})\\
  &= P_{c,t}\N(s;\mu_{c,t},S_{c,t}).
  \end{align*}
\end{proof}

We also need the joint posterior probability density of state $s_{t-1}$ at time $t-1$ and state $s_t$ at time $t$,
which we denote by $\hat{\gamma}_t(s_{t-1},s_t)$.
\begin{Lem}\label{gamma_hat}
The joint likelihood of all the data and states $s_{t-1},s_t$ is
\begin{align*}
  \hat{\gamma}_t(s_{t-1},s_t) &= \N(s_{t-1};\mu_{t-1,t},S_{t-1,t})\gamma_t(s_t), \quad\text{where} \\
  S_{t-1,t} &:= S_{a,t-1}+S_T',\quad\text{and}\\
  \mu_{t-1,t} &:= S_{t-1,t}^{-1}(T^TS_Ts_t + S_{a,t-1}\mu_{a,t-1})\\
  &= S_{t-1,t}^{-1}(T^{t}S_TTs'_t + S_{a,t-1}\mu_{a,t-1}).\\
\end{align*}
\end{Lem}
Recall that $s'_t := T^{-1}s_t$.
\begin{proof}
  By definition, the joint likelihood of all the data and states $s_{t-1},s_t$ is 
  \begin{align*}
    \hat{\gamma}_t(s_{t-1},s_t) &= \alpha_{t-1}(s_{t-1})Pd(s_t\mid s_{t-1})X(x_t\mid s_t)\beta_t(s_t) \\
    &\propto \N(s_{t-1};\mu_{a,t-1},S_{a,t-1})\N(s_t;Ts_{t-1},S_T)\N(x_t;Ms_t,S_x)\N(s_t;\mu_{b,t},S_{b,t}).
  \end{align*}
  Combining the first two factors using \eqref{comp_sq:2} and \eqref{exact_prod} yields
  \begin{equation}\label{gamma_hat:1}
    \hat{\gamma}_t(s_{t-1},s_t) = \N(s_{t-1};\mu_{t-1,t},S_{t-1,t})\N(T^{-1}s_t;\mu_{a,t-1},\hat{S}_{a,t-1})\N(Ms_t,x_t,S_x)\N(s_t;\mu_{b,t},S_{b,t}).
  \end{equation}
  Then using \eqref{comp_sq:2} again to combine the second and third factors of \eqref{gamma_hat:1}, and discarding
  the term that does not involve either $s_{t-1}$ or $s_t$, we have
    \begin{align*}
      \hat{\gamma}_t(s_{t-1},s_t) &= \N(s_{t-1};\mu_{t-1,t},S_{t-1,t})\N(s_t;\mu_{a,t},S_{a,t})\N(s_t;\mu_{b,t},S_{b,t})\\
      &\propto \N(s_{t-1};\mu_{t-1,t},S_{t-1,t})\N(s_t;\mu_{c,t},S_{c,t})\\ 
      &= \N(s_{t-1};\mu_{t-1,t},S_{t-1,t})\gamma_t(s_t).
    \end{align*}
\end{proof}
  Note that
  $$
  \int_{\R^{2n}}\N(s_{t-1};\mu_{t-1,t},S_{t-1,t})\gamma_t(s_t)ds_{t-1}ds_t = \int_{\R^n}\gamma_t(s_t)ds_t = 1 
  $$
  because $\gamma_t(s_t)$ does not depend on $s_{t-1}$, so \eqref{gamma_hat} is a bona-fide probability density and no
  normalization factor is required.  We remark that $\mu_{t-1,t}$ does depend on $s_t$, showing that $\gamma_{t-1}(s)$
  and $\gamma_t(s)$ are not independent.  This fact will play a role in the reestimation of $S_T$ and $T$ below.
  
  \section{The Expected Log-Likelihood}
    
    It turns out that the expected log-likelihood is computable in closed form.
    Although this isn't strictly necessary in order to derive the re-estimation formulas, it serves as a useful check on the
    derivation and on the numerical computation.  We begin with the key lemma.
    
  \begin{Lem}\label{exp_log}
    Recall the notation of \eqref{comp_sq:1}. Let $\gamma := \N(x;\mu_0,S_0)$ be an $n$-dimensional gaussian, and let $A$ be any
    full rank $n\times{m}$ matrix. Then for any vector $\mu\in\R^n$, 
    \begin{align}
      E_{\gamma}(x-\mu)(x-\mu)^T &= S_0^{-1} + (\mu-\mu_0)(\mu-\mu_0)^T, \label{E_xx^T}\\
      E_{\gamma}(x-\mu)^T(x - \mu) &=  tr(S_0^{-1}) + (\mu-\mu_0)^T(\mu-\mu_0),\quad\text{and}\label{E_x^Tx}\\
        E_{\gamma}{Q}(Ax;\mu,S) &=  tr(S_0^{-1}A^TSA) + (\mu-A\mu_0)^TS(\mu-A\mu_0), \label{E_Q}
    \end{align}
  \end{Lem}
  \begin{proof}
    By definition, the covariance matrix for $\gamma$ is
    \begin{align*}
    S_0^{-1} &= E_{\gamma}(x-\mu_0)(x-\mu_0)^T = E_{\gamma}(xx^T) - E_{\gamma}(x\mu_0^T) - E_{\gamma}(\mu_0{x}^T) + E_{\gamma}(\mu_0\mu_0^T)\\
    &= E_{\gamma}(xx^T) - \mu_0\mu_0^T, \quad{whence}\\
    E_{\gamma}(x-\mu)(x-\mu)^T &= E_{\gamma}(xx^T) - E_{\gamma}(x\mu^T) - E_{\gamma}(\mu{x}^T) + E_{\gamma}(\mu\mu^T) \\
    &= S_0^{-1} + \mu_0\mu_0^T - \mu_0\mu^T-\mu\mu_0^T + \mu\mu^T\\
    &= S_0^{-1} + (\mu-\mu_0)(\mu-\mu_0)^T,
    \end{align*}
    proving \eqref{E_xx^T}; and \eqref{E_x^Tx} follows by taking the trace.

    To prove \eqref{E_Q} we first of all have
    \begin{equation}\label{E_Q:1}
      \begin{split}
      E_{\gamma}Q(Ax;\mu,S) &= E_{\gamma}(x^TA^TSAx) - 2E_{\gamma}(x^TA^TS\mu) + E_{\gamma}(\mu^TS\mu) \\
      &= E_{\gamma}(x^TA^TSAx) - 2\mu_0^TA^TS\mu + \mu^TS\mu.
      \end{split}
    \end{equation}
    
    To evaluate $E_{\gamma}(x^TA^TSAx)$, we let $C$ be the Cholesky matrix of $A^TSA$, so that
    $A^TSA = C^TC$.  Note that since $A$
    has full rank, $A^TSA$ is invertible, and so is $C$. Making the change
    of variable $y = Cx,~ dy = |C|dx$ and using \eqref{E_x^Tx}, we then have
      \begin{align*}
        E_{\gamma}(x^TA^TSAx) &= \int_{R^n}x^TA^TSAx~\N(x;\mu_0,S_0)dx\\
        &= \sqrt{(2\pi)^{-n}|S_0|}\int_{\R^n}x^TA^TSAx~\exp\left\{-\frac{1}{2}Q(x;\mu_0,S_0)\right\}dx \\
        &= |C|^{-1}\sqrt{(2\pi)^{-n}|S_0|}\int_{\R^n}y^Ty~\exp\left\{-\frac{1}{2}Q(C^{-1}y;\mu_0,S_0)\right\}dy \\
        &= \int_{\R^n}y^Ty~\N(y;C\mu_0,C^{-T}S_0C^{-1})dy\\
        &= tr(CS_0^{-1}C^T) + \mu_0^TC^TC\mu_0 \\
        &= tr(S_0^{-1}A^TSA) + \mu_0^TA^TSA\mu_0.
      \end{align*}
    Combining this with \eqref{E_Q:1} completes the proof of \eqref{E_Q}.  
  \end{proof}

Equipped with \eqref{E_Q}, we can now turn to the problem at hand.  Namely, if we are given a particular
state sequence $S := \{s_0,s_1,s_2,\dots,s_T\}$ and the data sequence $X := \{x_1,x_2,\dots,x_T\}$,
the joint probability density of $S$ and $X$ given parameters $\theta := \{\mu_0,S_0,S_T,T,S_x,M\}$ is
$$
P(X,S\mid\theta) = \N(s_0;\mu_0,S_0)\prod_{t=1}^N\N(x_t;Ms_t,S_x)\N(s_t;Ts_{t-1},S_T).
$$

Note that $P(X,S\mid\theta)$ can also be viewed as  $L(\theta\mid X,S)$, the posterior likelihood of $\theta$.
To re-estimate $\theta$, we try to maximize $L(\theta)$ using the EM algorithm. Namely, we define 
$$
E(\theta,\bar{\theta}) := \int_{\R^{Nn}}P(X,S\mid\bar{\theta})\log{L(\theta)\mid X,S)}dS,
$$
where $\bar{\theta}$ is the current set of parameters, and $\theta$ is the unknown set of new parameters
we wish to determine. So instead of maximizing the log-likelihood directly, we can maximize its expected
value with respect to the current posterior distribution, because it is a standard result \cite{Dempster}
(and easy to prove) that
$$
L(\theta) - L(\bar{\theta}) \ge E(\theta,\bar{\theta}) - E(\theta,\theta),
$$
so choosing $\theta$ to maximize $E(\theta,\bar{\theta})$ will increase the value of $L(\theta)$.


  
Expanding $E(\theta,\bar{\theta})$, we have 
\begin{align}
  E(\theta,\bar{\theta}) &= -(2N+1)\frac{n\log(2\pi)}{2} + \frac{1}{2}(E_0 + E_1 + E_2),\quad\text{where}\notag\\
  E_0 :&=\int_{\R^{Nn}}P(X,S\mid\bar{\theta})\{\log|S_0| - Q(s_0;\mu_0,S_0)\}dS,\label{E0}\\
  E_1 :&= \int_{\R^{Nn}}P(X,S\mid\bar{\theta})\left\{\sum_{t=1}^N\log|S_x| - Q(x_t;Ms_t,S_x)\right\}dS,
\label{E1}\\
E_2 :&= \int_{\R^{Nn}}P(X,S\mid\bar{\theta})\left\{\sum_{t=1}^N\log|S_T| - Q(s_t;Ts_{t-1},S_T)\right\}dS,
\label{E2}
\end{align}
Since the expected value of a constant is just that constant, $-n\log(2\pi)/2$ can be moved outside each
integral sign and ignored in the optimization.  And since none of the parameters of $\theta$ appear in more
than one of the $E_i$, we can optimize $E$ by optimizing each of $E_0,E_1,E_2$ separately.

We begin with $E_0$. Since the only component of $S$ in the integrand of \eqref{E0} is $s_0$, the integrand is a constant
with respect to all $s_t$ for $t > 0$, and the expected value
collapses to the marginal 
expected value of the integrand with respect to the marginalization of $P(X,S\mid\bar{\theta})$ at $s_0$, which is just 
$\gamma_0(s_0)$.  Furthermore, after pulling the constant and the summation 
outside the integral in \eqref{E1} we have the same collapse of the posterior to the marginalization at $s_t$, which
is just $\gamma_t(s_t)$.  It follows that
\begin{align}
  E_0 &= \log|S_0| - E_{\gamma_0(s_0)}Q(s_0;\mu_0,S_0)ds_0 \\
  &= \log|S_0| - tr(\overline{S}_{c,0}^{-1}S_0) - (\mu_0-\bar{\mu}_{c,0})^TS_0(\mu_0 - \bar{\mu}_{c,0}),
  \label{E0:1}\quad\text{and}\\
  E_1 &= N\log|S_x| - \sum_{t=1}^NE_{\gamma_t(s_t)}Q(Ms_t;x_t,S_x)ds_t \\
  &= N\log|S_x| - \sum_{t=1}^Ntr(\overline{S}_{c,t}^{-1}M^TS_xM) + (x_t-M\bar{\mu}_{c,t})^TS_x(x_t-M\bar{\mu}_{c,t})\label{E1:1}.
\end{align}


The situation for \eqref{E2} is more complicated, because after again moving the constant and the summation outside
the integral, the integrand depends on both $s_t$ and $s_{t-1}$.  So in this case, the integral collapses to the 
joint marginal expectation over $s_t$ and $s_{t-1}$ at time $t$ given by \eqref{gamma_hat}, and we get
\begin{equation}\label{E2:1}
  \begin{split}
    E_2 &= N\log|S_T| - \sum_{t=1}^N\int_{R^n}\left(\int_{\R^{n}}\N(s_{t-1};\bar{\mu}_{t-1,t}\overline{S}_{t-1,t})Q(Ts_{t-1};s_t,S_T)ds_{t-1}\right)\gamma(s_t)ds_t\\
    &= N\log|S_T|-\sum_{t=1}^N\int_{R^n}\left(tr(\overline{S}_{t-1,t}^{-1}T^TS_TT) +
    (s_t - T\bar{\mu}_{t-1,t})^TS_T(s_t-T\bar{\mu}_{t-1,t})\right)\gamma(s_t)ds_t\\
    &= N\log|S_T|-\sum_{t=1}^N\left(tr(\overline{S}_{t-1,t}^{-1}T^TS_TT) + E_{\gamma_t(s_t)}Q(s_t;T\bar{\mu}_{t-1,t},S_T)\right).\\
    \end{split}
\end{equation}
At this point we would like to apply \eqref{E_Q} to this last expectation, but unfortunately after checking \eqref{gamma_hat},
we find that $\bar{\mu}_{t-1,t}$ depends on $s_t$.  Namely, with $s_t' := \overline{T}^{-1}s_t$ and
$\overline{S}_T' := \overline{T}^T\overline{S}_T\overline{T}$ we have
\begin{align*}
\bar{\mu}_{t-1,t}-s'_t &= (\overline{S}_{t-1,t}^{-1}\overline{S}'_T - I)s'_t +
  \overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1}\bar{\mu}_{a,t-1}, \\
  &= (\overline{S}_{t-1,t}^{-1}\overline{S}'_T) - \overline{S}_{t-1,t}^{-1}\overline{S}_{t-1,t})s'_t +
  \overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1}\bar{\mu}_{a,t-1}, \\
    &= \overline{S}_{t-1,t}^{-1}(\overline{S}'_T - \overline{S}_{t-1,t} )s'_t + \overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1}\bar{\mu}_{a,t-1}, \\
    &= \overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1}(\bar{\mu}_{a,t-1}-s'_t), \quad\text{hence}\\
    \bar{\mu}_{t-1,t} &= \overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1}\bar{\mu}_{a,t-1} - (\overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1}-I)\overline{T}^{-1}s_t,\quad\text{and therefore}\\
    T\bar{\mu}_{t-1,t} - s_t &= T\bar{v}_t - (T\overline{S}_* + I)s_t, 
  \end{align*}
where
\begin{align*}
  \overline{S}_* :&= (\overline{W}_t - I)\overline{T}^{-1} ,\\
  \overline{W}_t :&= \overline{S}_{t-1,t}^{-1}\overline{S}_{a,t-1},\quad\text{and}\\
  \bar{v}_t :&= W_t\bar{\mu}_{a,t-1}.
\end{align*}

Note that both $\overline{S}_*$ and $\bar{v}_t$  depend only on the known posterior parameters as indicated
by the bar notation, and neither one depends on $s_t$.

We can now re-write \eqref{E2:1} in a form amenable to \eqref{E_Q}:

\begin{equation}\label{mu_t-1,t}
  \begin{split}
    Q(s_t,T\bar{\mu}_{t-1,t},S_T) &= ((T\overline{S}_*+I)s_t - T\bar{v}_t)^TS_T((T\overline{S}_* + I)s_t - T\bar{v}_t),\quad\text{and}\\
    E_{\gamma_t(s_t)}Q(s_t,T\bar{\mu}_{t-1,t},S_T) &= E_{\gamma_t(s_t)}Q((T\overline{S}_*+ I)s_t;T\bar{v}_t,S_T)\\
    &= tr(\overline{S}_{c,t}^{-1}(T\overline{S}_*+I)^TS_T(T\overline{S}_* + I)) + (T\bar{v}_t - \bar{\mu}_{c,t})^TS_T(T\bar{v}_t-\bar{\mu}_{c,t}).
  \end{split}
\end{equation}


\section{Re-estimation}
If $f$ is any scalar-valued function of an $m\times{n}$ matrix $A = a_{ij}$, we denote by $\partial{f}/\partial{A}$ the 
$m\times{n}$ matrix whose $(i,j)$-entry is $\partial{f}/\partial{a_{ij}}$.  This also applies to column vectors
(when $n=1$). We will also denote by $C = A\circ{B}$ the element-wise product of $A$ and $B$ whose $i,j$-entry is
$C_{ij} = A_{ij}B_{ij}$.

In particular, if $m=n$, $A$ is symmetric, and $f(A) = |A|$,  then the usual expansion 
in minors along each row shows that 
$$
\frac{\partial{|A|}}{\partial{A}} = B\circ{A}^* = |A|B\circ{A}^{-1},
$$
where $A^*$ is the transposed matrix of cofactors and $B_{ij} := 2-\delta_{ij}$ for all $i,j$.
The off-diagonal elements are doubled because $A$ is symmetric.
It follows that
$$
\frac{\partial{\log|A|}}{\partial{A}} = B\circ{A}^{-1}. 
$$

In addition, since $Q(x;\mu,S)$ is linear in the coefficients of $S$, we see that
$$
\frac{\partial{Q(x;\mu,S)}}{\partial{S}} = B\circ(x-\mu)(x-\mu)^T,
$$
and it is also easy to verify that
$$
\frac{\partial{Q(x;\mu,S)}}{\partial{\mu}} = 2S(\mu-x).
$$

Finally, to reestimate $M$ and $T$, we need

\begin{Lem}\label{dQdA}
  Let $S$ be a symmetric $m\times{m}$ matrix, let $A$ be any $m\times{n}$ matrix,
  and let $\mu,\nu\in \R^n$.  Define
  \begin{align*}
  Q :&= \mu^TA^TSA\nu,\quad\text{then}\\
  \frac{\partial{Q}}{\partial{A}} &= SA(\mu\nu^T+\nu\mu^T).
  \end{align*}
\end{Lem}
\begin{proof}
Expanding, we have
$$
Q = \mu^TA^TSA\mu = \sum_{i,j,k,l}\mu_iA_{ji}S_{jk}A_{kl}\nu_l.
$$
Collecting terms which involve $A_{qr}$, if $j=q$ and $i=r$, we get
\begin{equation}\label{SA}
A_{qr}\mu_r\sum_{k,l}S_{qk}A_{kl}\nu_l,
\end{equation}
while if $k=q$ and $l=r$, we get
\begin{equation}\label{A^TS}
A_{qr}\nu_r\sum_{i,j}\mu_iA_{ji}S_{jq}.
\end{equation}
However after replacing $i$ by $l$, $j$ by $k$ and using the fact that $S$ is symmetric,
\eqref{SA} and \eqref{A^TS} are identical except that $\mu$ and $\nu$ are interchanged.
But both equations have the common term
$\mu_r\nu_rA_{qr}^2S_{qq}$ when $j = k = q$ and $i = l = r$.
Then
\begin{align}
  \notag\frac{\partial{Q}}{\partial{A_{qr}}} &= \mu_r\sum_{(k,l)\neq(q,r)}S_{qk}A_{kl}\nu_l
  +\nu_r\sum_{(k,l)\neq(q,r)}S_{qk}A_{kl}\mu_l+ 2\mu_r\nu_rA_{qr}S_{qq}\\
  &= \mu_r\sum_{k,l}S_{qk}A_{kl}\nu_l + \nu_r\sum_{k,l}S_{qk}A_{kl}\mu_l\notag\\
  &= \sum_{k,l}S_{qk}A_{kl}(\mu_l\nu_r + \nu_l\mu_r).\label{dQ_0}
\end{align}

We conclude that the matrix whose $(q,r)$ term is given by \eqref{dQ_0} is
$$
\frac{\partial{Q}}{\partial{A}} = SA(\mu\nu^T+\nu\mu^T)
$$
as asserted.
\end{proof}

\begin{Cor}\label{dM}
  $$
  \frac{\partial{Q(x;A\mu,S)}}{\partial{A}} = -2Sx\mu^T + 2SA\mu\mu^T.
  $$
\end{Cor}
\begin{proof}
Expanding, we have
$$
Q := (x-A\mu)^TS(x-A\mu) = x^TSx - 2x^TSA\mu + \mu^TA^TSA\mu.
$$
Differentiating the linear term is just a special case of
$$
\frac{\partial}{\partial{A}}y^TAx = yx^T.
$$
The quadratic term follows from \eqref{dQdA} with $\nu = \mu$.
\end{proof}


Armed with these formulas, we first optimize $E_0$ with respect to $\bar{\mu}_0$: 
 
\begin{align}
0 = \frac{\partial{E_0}}{\partial{\bar{\mu}_0}} &= \int_{\R^n}\gamma_0(s_0)\frac{\partial{Q(s_0;\bar{\mu}_0,\overline{S}_0)}}{\partial{\bar{\mu}_0}}ds_0 \notag\\
&= \int_{\R^n}\gamma_0(s_0)(2\overline{S}_0(\bar{\mu}_0-s_0))ds_0\notag\\
&= 2\overline{S}_0\int_{\R^n}\gamma_0(s_0)(\bar{\mu}_0-s_0)ds_0 \notag\\
&= 2\overline{S}_0(\bar{\mu}_0-\mu_{c,0}), \quad\text{hence}\notag\\
\bar{\mu}_0 &= \mu_{c,0}\label{mu_0}
\end{align}

Next, we optimize $E_0$ with respect to $\overline{S}_0$, and we can set $\bar{\mu}_0 = \mu_{c,0}$ by \eqref{mu_0}:
\begin{align*}
  0 = \frac{\partial{E_0}}{\partial{\overline{S}_0}} &= \frac{\partial\log|\overline{S}_0|}{\partial\overline{S}_0}
  -\int_{\R^n}\gamma_0(s_0)\frac{\partial{Q(s_0;\mu_{c,0},\overline{S}_0)}}{\partial\overline{S}_0}ds_0 \\
&= (2-\delta_{ij})\overline{S}_0^{-1} - (2-\delta_{ij})\int_{\R^n}\gamma_0(s_0)(s_0-\mu_{c,0})(s_0-\mu_{c,0})^Tds_0.
\end{align*}
Not surprisingly, it follows that
\begin{equation}\label{Sigma_0}
  \begin{split}
    \overline{S}_0^{-1} &= \int_{\R^n}\gamma_0(s_0)(s_0-\mu_{c,0})(s_0-\mu_{c,0})^Tds_0 = S_{c,0}^{-1}, \quad\text{i.e.}\\
    \overline{S}_0 &= S_{c,0}.
  \end{split}
\end{equation}

We note that above and in what follows, we have vector and matrix integrands.  Of course, this is just a convenient
notation for sets of integrals, each one having a particular vector or matrix entry as the integrand.

To optimize $E_1$, we first differentiate with respect to $\overline{M}$ using \eqref{dM}.
Setting the derivative to zero we need to solve for $\overline{M}$:
\begin{align*}
  0 &= \frac{\partial{E_1}}{\partial\overline{M}}
  = -\sum_{t=1}^N\int_{\R^n}\gamma_t(s_t)\frac{\partial}{\partial\overline{M}}Q(x_t,\overline{M}s_t,\overline{S}_M)ds_t\\
  &= 2\sum_{t=1}^N\int_{\R^n}\gamma_t(s_t)(\overline{S}_Mx_ts_t^T - \overline{S}_M\overline{M}s_ts_t^T)ds_t\\
  &= 2\sum_{t=1}^N(\overline{S}_Mx_t\mu_{c,t}^T - \overline{S}_M\overline{M}(S_{c,t}^{-1} + \mu_{c,t}\mu_{c,t}^T).
\end{align*}
Multiplying through by $\frac{1}{2}\overline{S}_M^{-1}$, we find that
\begin{equation}\label{M_bar}
  \begin{split}
    \overline{M} &= \Gamma_1\Gamma_2^{-1}, \quad\text{where}\\
    \Gamma_1 :&= \sum_{t=1}^Nx_t\mu_{c,t}^T, \quad\text{and}\\
    \Gamma_2 :&= \sum_{i=1}^NS_{c,t}^{-1} + \mu_{c,t}\mu_{c,t}^T.
  \end{split}
\end{equation}

Note that $\overline{M}$ does not depend on $\overline{S}_M$, so we can optimize $E_1$ with respect to $\overline{S}_M$
using the new value of $\overline{M}$:
\begin{align*}
  0 = \frac{\partial{E_1}}{\partial{\overline{S}_M}} &=
   N\frac{\partial{\log|\overline{S}_M|}}{\partial\overline{S}_M}
  - \sum_{t=1}^N\int_{\R^n}\gamma_t(s_t)\frac{\partial{Q(x_t;\overline{M}s_t,\overline{S}_M)}}{\partial{\overline{S}_M}}ds_t \\
  &= N(B\circ\overline{S}_M^{-1}) - B\circ\sum_{t=1}^N
  \int_{\R^n}\gamma_t(s_t)(x_t-\overline{M}s_t)(x_t-\overline{M}s_t)^Tds,
\end{align*}
where $B_{ij} := 2-\delta_{ij}$ as above.  Dividing by $B_{ij}$ in each component, we see that
\begin{equation}\label{S_x}
  \begin{split}
    \overline{S}_M^{-1} &= \frac{1}{N}\sum_{t=1}^N\int_{\R^n}\gamma_t(s_t)(x_tx_t^T - x_ts_t^T\overline{M}^T
    - \overline{M}s_tx_t^T + \overline{M}s_ts_t^T\overline{M}^T)ds_t \\
    &= \frac{1}{N}\sum_{t=1}^N(x_tx_t^T - x_t\mu_{c,t}^T\overline{M}^T - \overline{M}\mu_{c,t}x_t^T +
    \overline{M}(S_{c,t}^{-1}+\mu_{c,t}\mu_{c,t}^T)\overline{M}^T)\\
    &= \frac{1}{N}\sum_{t=1}^N(x_t-\overline{M}\mu_{c,t})(x_t-\overline{M}\mu_{c,t})^T +
    \overline{M}S_{c,t}^{-1}\overline{M}^T.
  \end{split}
\end{equation}

Next we optimize $E_2$ first by solving
$$
\frac{\partial{E_2}}{\partial{\overline{T}}} = 0
$$
for $\overline{T}$ optionally subject to the condition that $\overline{T}$ is a companion matrix.  We do this
by computing the full E-M update for $T$, but for a companion matrix we only use the result to update the variable entries,
namely $T_{nj}$ for $j > 1$, while leaving all the constant entries unchanged.

Thus, using \eqref{gamma_hat} and \eqref{dM}, we have
\begin{equation}\label{T:1}
  \begin{split}
0 &= \sum_{t=1}^N\int_{\R^{2n}}\hat{\gamma}_t(s_{t-1},s_t)
\frac{\partial{Q(s_t;\overline{T}s_{t-1},\overline{S}_T)}}{\partial\overline{T}}ds_{t-1}ds_t \\
&= -2\sum_{t=1}^N\int_{\R^{2n}}\N(s_{t-1};\mu_{t-1,t},S_{t-1,t})\gamma_t(s_t)(\overline{S}_Ts_ts_{t-1}^T -
\overline{S}_T\overline{T}s_{t-1}s_{t-1}^T) ds_{t-1}ds_t \\
&= -2\overline{S}_T\sum_{t=1}^N\int_{\R^{n}}\gamma_t(s_t) (s_t\mu_{t-1,t}^T -
\overline{T}(S_{t-1,t}^{-1} + \mu_{t-1,t}\mu_{t-1,t}^T))ds_t \\
&= 2\overline{S}_T\sum_{t=1}^N\left(\overline{T}S_{t-1,t}^{-1} - \int_{\R^n}\gamma_t(s_t)s_t\mu_{t-1,t}^Tds_t +
\overline{T}\int_{\R^n}\gamma_t(s_t)\mu_{t-1,t}\mu_{t-1,t}^Tds_t\right).\\
  \end{split}
\end{equation}
So we have two integrals to evaluate:
\begin{equation}\label{I_1}
  \begin{split}
  I_{1,t} :&= \int_{\R^n}\gamma_t(s_t)s_t\mu_{t-1,t}^Tds_t \\
  &= \int_{\R^n}\gamma_t(s_t)s_t(s_t^TS_TT + \mu_{a,t-1}^TS_{a,t-1})S_{t-1,t}^{-1})ds_t \\
  &= (S_{c,t}^{-1} + \mu_{c,t}\mu_{c,t}^T)S_TTS_{t-1,t}^{-1} + \mu_{c,t}\mu_{a,t-1}^TS_{a,t-1}S_{t-1,t}^{-1},
  \end{split}
  \end{equation}
and
\begin{equation}\label{I_2}
  \begin{split}
  I_{2,t} :&= \int_{\R^n}\gamma_t(s_t)\mu_{t-1,t}\mu_{t-1,t}^Tds_t \\
  &= \int_{\R^n}\gamma_t(s_t)S_{t-1,t}^{-1}(T^TS_Ts_ts_t^TS_TT + T^TS_Ts_t\mu_{a,t-1}^TS_{a,t-1} \\
  &+ S_{a,t-1}\mu_{a,t-1}s_t^TS_TT + S_{a,t-1}\mu_{a,t-1}\mu_{a,t-1}^TS_{a,t-1}) S_{t,t-1}^{-1}ds_t \\
  &= S_{t-1,t}^{-1}[T^TS_T(S_{c,t}^{-1`}+\mu_{c,t}\mu_{c,t}^T)S_TT + T^TS_T\mu_{c,t}\mu_{a,t-1}^TS_{a,t-1} \\
  &+ S_{a,t-1}\mu_{a,t-1}\mu_{c,t}^TS_TT + S_{a,t-1}\mu_{a,t-1}\mu_{a,t-1}^TS_{a,t-1}]S_{t-1,t}^{-1}. \\
\end{split}
\end{equation}

Note that the matrices $T$ and $S_T$ appearing in \eqref{I_1} and \eqref{I_2} are the {\em previous}
values, as opposed to the unkowns $\overline{T}$ and $\overline{S}_T$ for which we are solving.

To simplify notation, we put
\begin{align*}
S_* :&= S_{t-1,t}^{-1}S_{a,t-1},\\
S_+ :&= S_{t-1,t}^{-1}T^TS_T,\\
\nu_1 :&= S_+\mu_{c,t},\quad\text{and}\\
\nu_2 :&= S_*\mu_{a,t-1}.
\end{align*}
Then
\begin{align*}
  I_{1,t} &= S_{c,t}^{-1}S_+^T +\mu_{c,t}(\nu_1^T + \nu_2^T), \quad\text{and}\\
  I_{2,t} &= S_+S_{c,t}^{-1}S_+^T+\nu_1\nu_1^T + \nu_1\nu_2^T + \nu_2\nu_1^T + \nu_2\nu_2^T \\
      &= S_+S_{c,t}^{-1}S_+^T + (\nu_1+\nu_2)(\nu_1+\nu_2)^T.
\end{align*}

After multiplying by $\overline{S}_T^{-1}$, \eqref{T:1} becomes
\begin{align*}
 0 &= \sum_{t=1}^N\left(\overline{T}S_{t-1,t}^{-1} - I_{1,t} + \overline{T}I_{2,t}\right)\\
  &= \overline{T}\Gamma_2 - \Gamma_1, \quad\text{where} \\
  \Gamma_2 :&= \sum_{i=1}^N(S_{t-1,t}^{-1} + I_{2,t}),\quad\text{and}\\
  \Gamma_1 :&= \sum_{i=1}^NI_{1,t}.
\end{align*}
Thus,  we have
\begin{equation}\label{T:2}
  \overline{T} = \Gamma_1\Gamma_2^{-1}.
\end{equation}
Note that if $T$ is a companion matrix, we only update $T_{nj}$ for $j > 1$ from \eqref{T:2} and
leave all other entries unchanged.

Finally, we complete our optimization of $E_2$ by differentiating with respect to $\overline{S}_T$ and using \eqref{gamma_hat}:
\begin{equation}\label{Sigma_Tr:0}
  \begin{split}
  0 &= \frac{\partial{E_2}}{\partial{\overline{S}_T}} = \frac{\partial{\log|\overline{S}_T|}}{\partial\overline{S}_T}
  - \sum_{t=1}^N\int_{\R^{2n}}\hat{\gamma}_t(s_{t-1},s_t)\frac{\partial{Q(s_t;\overline{T}s_{t-1},\overline{S}_T)}}
  {\partial\overline{S}_T}ds_{t-1}ds_t \\
  &= N(B\circ\overline{S}_T^{-1}) -B\circ\sum_{t=1}^N\int_{\R^{2n}}\N(s_{t-1};\mu_{t-1,t},S_{t-1,t})
  \gamma_t(s_t)(\overline{T}s_{t-1}-s_t)(\overline{T}s_{t-1}-s_t)^Tds_{t-1}ds_t.
  \end{split}
\end{equation}

As noted earlier, $\gamma_t(s_t)$ does not depend on $s_{t-1}$, so we can integrate first with respect to $s_{t-1}$, with
a result analagous to \eqref{S_x}:
\begin{align*}
  \int_{-\infty}^{\infty}&\N(s_{t-1};\mu_{t-1,t},S_{t-1,t})(\overline{T}s_{t-1}-s_t)
  (\overline{T}s_{t-1}-s_t)^Tds_{t-1}\\ &= S_{t-1,t}^{-1} +
(\overline{T}(\mu_{t-1,t}-s'_t)(\mu_{t-1,t}-s'_t)^T\overline{T}^T,
\end{align*}
where $s'_t = \overline{T}^{-1}s_t$.
Substituting this result into \eqref{Sigma_Tr:0}, we get
\begin{equation}\label{Sigma_Tr:1}
  \overline{S}_T^{-1} =  \frac{1}{N}\sum_{t=1}^N\left(S_{t-1,t}^{-1} +\int_{-\infty}^{\infty}\gamma_t(s_t)
  \overline{T}(\mu_{t-1,t}-s'_t)(\mu_{t-1,t}-s'_t)^T\overline{T}^Tds_t\right).
\end{equation}

However, there is a problem evaluating this integral because after checking \eqref{gamma_hat}, we see that $\mu_{t-1,t}$
depends on $s'_t = \overline{T}^{-1}s_t$, namely 
\begin{equation}\label{s-mu_1}
  \begin{split}
    \left(\mu_{t-1,t}-s'_t\right) &= (S_{t-1,t}^{-1}T^{t}S_TT - I)s'_t + S_{t-1,t}^{-1}S_{a,t-1}\mu_{a,t-1}, \\
    &= (S_{t-1,t}^{-1}T^{t}S_TT)s'_t - S_{t-1,t}^{-1}S_{t-1,t}) + S_{t-1,t}^{-1}S_{a,t-1}\mu_{a,t-1}, \\
    &= S_{t-1,t}^{-1}(T^{t}S_TT - S_{t-1,t} )s'_t + S_{t-1,t}^{-1}S_{a,t-1}\mu_{a,t-1}, \\
    &= S_{t-1,t}^{-1}S_{a,t-1}(\mu_{a,t-1}-s'_t),\\
    &= S_*(\mu_{a,t-1}-s_t')
  \end{split}
\end{equation}

Fortunately, neither $\mu_{a,t-1}$ nor $S_*$ depends on $s_t$,
so using \eqref{gamma_hat}, \eqref{Sigma_Tr:1} becomes
\begin{equation}\label{S_Tr:2}
  \begin{split}
  \overline{S}_T^{-1} &=\frac{1}{N}\sum_{t=1}^N\left(S_{t-1,t}^{-1} + \overline{T}S_*
  \left[\int_{-\infty}^{\infty}\gamma_t(s_t)(\mu_{a,t-1}-s'_t)(\mu_{a,t-1}-s'_t)^Tds_t\right]S_*^T\overline{T}^T\right)\\
   &=\frac{1}{N}\sum_{t=1}^NS_{t-1,t}^{-1} + \overline{T}S_*
           [S_{c,t}^{-1}+(\mu_{a,t-1}-\overline{T}^{-1}\mu_{c,t})(\mu_{a,t-1}-\overline{T}^{-1}\mu_{c,t})^T]
           S_*^T\overline{T}^T.
  \end{split}
\end{equation}
\newpage
\begin{thebibliography}{9}
\bibitem{Hinton}
  Ghahramani, Z. and Hinton, G.,
  \emph{Parameter Estimation for Linear Dynamical Systems},
  Department of Computer Science, University of Toronto,
  Technical Report CRG-TR-96-2,
  Feb. 22, 1996
\bibitem{Aravkin}
  Aravkin, A. Burke, J. Ljung, L. Lozano, A. and Pillonetto, G.,
  \emph{Generalized Kalman Smoothing: Modeling and Algorithms},
  arXiv:1609.06369,
  Sep. 25, 2016
\bibitem{Bilmes}
  Bilmes, J.,
  \emph{A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation
    for Gaussian Maixture and Hidden Markov Models},
  Department of Electrical Engineering and Computer Science, University of California, Berkeley,
  Technical Report 97-021
  April, 1998
\bibitem{Dempster}
  Dempster, A Laird, N and Rubin, D,
  \emph{Maximum-likelihood from incomplete data via the EM algorithm},
  J.Royal Statist. Soc. Ser. B, 39,
  1977
\bibitem{Shumway}
  Shumway, R and Stoffer, D,
  \emph{Time Series Analysis and Its Applications, 3rd Ed. (Blue printing)}
  Springer, New York 2016
    
\end{thebibliography}

\end{document}


