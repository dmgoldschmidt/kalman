\documentclass[12pt,leqno]{article}
\include{article_defs}
\title{A Continuous Hidden Markov Model}
\author{David M. Goldschmidt}
%\oddsidemargin 10 pt \evensidemargin 10 pt \marginparwidth 0.75 in \textwidth
%6.0 true in \topmargin -40 pt \textheight 8.8 true in 
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lfoot{}
%\rfoot{\thepage}
\begin{document}
%\renewcommand{\footrulewidth}{0.4pt}
\newcommand{\p}{\ensuremath{u}}
\newcommand{\VV}{V}
\maketitle


\section{Introduction}
Let $\mu$ and $x$ be vectors in $\R^n$ and let $\Sigma$ be an $n\times{n}$ positive
definite real symmetric matrix.  Recall that the gaussian probability density
on $\R^n$ is given by 
$$
\N(x;\mu,\Sigma) := \det(2\pi\Sigma)^{-\frac{1}{2}}\exp\left\{-\frac{1}{2}
(x - \mu)^T\Sigma^{-1}(x-\mu)\right\}.
$$
We will make use of the formal identity $\N(x,\mu,\Sigma) = \N(\mu,x,\Sigma)$ below.

We now define a Hidden Markov Model with state space $\R^n$ as follows.  Given the state $s\in\R^n$ and an observation $x_t\in\R^m$ at time $t$, the output density is
$$
X(x_t\mid s) := \N(x_t;M_ts + b_t,\Sigma_{Ob}),
$$
where $M_t$ is a linear map from the state space to the
measurement space, $b_t$ is a bias, and $\Sigma_{Ob}$ is an $m\times{m}$ positive definite real symmetric matrix.  Typically, $\Sigma_{Ob}$ is a time-independent model parameter.
In the simplest case, $b_t = 0$  and $M_t = I$, the identity map, for all $t$ so that $m = n$ and $s$ is itself the mean of the output distribution.  We will call this case the {\em basic} case, and concentrate on it initially. Later, we will consider some generalizations, particularly the case that $M_t = M$ and $b_t = b$ are time-independent model parameters.

The state process is a discrete time continuous state Markov process with transition probability density
from state $s_0$ at time $t$ to state $s_1$ at time $t+1$ given by
$$
Pd(s_1\mid s_0) :=  \N(s_1;s_0+c_t,\Sigma_{Tr}),
$$
where $\Sigma_{Tr}$ is a time-independent parameter of the model
and $c_t$ is a known ``control input'' which we also assume is zero for the time being.

As usual, we are given observations $\{x_t\mid 1\le t\le T\}$ and model parameters $\theta$.  For each observation time $t$ and state $s$, we define
\begin{align*}
  \alpha_t(s) :&= Pd(x_1,x_2,\dots,x_t~ \& \text{state $s$ at time $t$}\mid \theta ),\\
  \beta_t(s) :&= Pd(x_{t+1},\dots,x_T \mid ~\text{state $s$ at time $t$},\theta)\\
  \gamma_t(s) :&= \frac{\alpha_t(s)\beta_t(s)}{\int_{\R^n}\alpha_t(s)\beta_t(s)ds}.
\end{align*}

Thus, $\alpha_t(s)$ is the joint density of state $s$ at time $t$ and the observations up to (and including) time $t$,
$\beta_t(s)$ is the {\em conditional} density of the future observations given state $s$ at time $t$, and $\gamma_t(s)$
is the posterior probability density of state $s$ at time $t$.

It is clear from the definitions that the following recursions are satisfied:
\begin{align}
\alpha_t(s) &= X(x_t \mid s)\int_{\R^n}\alpha_{t-1}(u)Pd(s \mid u)du,\quad\text{and}\label{alpha:0}\\
\beta_t(s) &= \int_{\R^{n}}Pd(u \mid s)X(x_{t+1} \mid u)\beta_{t+1}(u)du.\label{beta:0}
\end{align}

We initialize these recursions with
\begin{align*}
\alpha_0(s) :&= \N(\mu_0,\Sigma_0) \\
\beta_{T+1}(s) :&= 1 \quad\text{for all $s$},
\end{align*}
where $\Sigma_0$ and $\mu_0$ are model parameters.

Because everything in sight is gaussian, the above integrals can be evaluated in closed form, by repeated application of
\begin{Lem}\label{comp_sq:1}
  Let
  $$
  Q_i(x) := (x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)\quad(i = 1,2),
  $$
  where $x$ and $\mu_i$ are $n$-dimensional vectors and $\Sigma_i$ is a symmetric positive definite $n\times{n}$
  matrix ($i = 1,2)$. Then
  $$
  Q_1(x) + Q_2(x) = (x-\mu)^T\Sigma^{-1}(x-\mu) + (\mu_2-\mu_1)^T(\Sigma_1+\Sigma_2)^{-1}(\mu_2-\mu_1),
  $$
  where
  \begin{align}
    \mu :&= \Sigma_2(\Sigma_1+\Sigma_2)^{-1}\mu_1+\Sigma_1(\Sigma_1+\Sigma_2)^{-1}\mu_2,\quad\text{and}\label{mu}\\
    \Sigma :&= \Sigma_1(\Sigma_1 + \Sigma_2)^{-1}\Sigma_2\label{sigma}.
  \end{align}
\end{Lem}
\begin{proof}

It's straightforward to see that
$$
Q_1(x) + Q_2(x) = (x-\mu)^T(\Sigma^{-1})(x-\mu) + C,
$$
where
\begin{align}
 \Sigma^{-1} &= \Sigma_1^{-1} + \Sigma_2^{-1},\label{Sigma}\\
  \Sigma^{-1}\mu &=\Sigma_1^{-1}\mu_1+\Sigma_2^{-1}\mu_2, \quad\text{and}\label{mu:1}\\
  C &= \mu_1^T\Sigma_1^{-1}\mu_1 +\mu_2^T\Sigma_2^{-1}\mu_2 - \mu^T\Sigma^{-1}\mu.
  \label{C}
\end{align}
The problem is to prove \eqref{mu} and \eqref{sigma}, and to put $C$ into the form given in the lemma.
 To begin with, we have

\begin{align}
  \Sigma_1\Sigma^{-1}\Sigma_2 &= \Sigma_1(\Sigma_1^{-1}+\Sigma_2^{-1})\Sigma_2 = \Sigma_1+\Sigma_2
  = \Sigma_2\Sigma^{-1}\Sigma_1,\quad\text{whence}\notag\\
  \Sigma_1^{-1}\Sigma\Sigma_2^{-1} &= (\Sigma_1+\Sigma_2)^{-1} = \Sigma_2^{-1}\Sigma\Sigma_1^{-1},\quad\text{and thus}\notag\\
  \Sigma &= \Sigma_1(\Sigma_1 + \Sigma_2)^{-1}\Sigma_2 = \Sigma_2(\Sigma_1+\Sigma_2)^{-1}\Sigma_1,\label{Sigma:1}
\end{align}
proving \eqref{sigma}.  Then \eqref{mu:1} becomes
\begin{equation}\label{mu:2}
  \begin{split}
    \mu &=  \Sigma(\Sigma_1^{-1}\mu_1+\Sigma_2^{-1}\mu_2)\\
    &=\Sigma_2(\Sigma_1+\Sigma_2)^{-1}\mu_1+\Sigma_1(\Sigma_1+\Sigma_2)^{-1}\mu_2,
  \end{split}
\end{equation}
proving \eqref{mu}.

Now using \eqref{Sigma:1}, we can re-write \eqref{mu:1} as  
\begin{equation}\label{mu:3}
  \begin{split}
  \mu &= \Sigma(\Sigma_1^{-1}\mu_1 + \Sigma_2^{-1}\mu_1 +\Sigma_2^{-1}(\mu_2-\mu_1)) \\
  &= \Sigma(\Sigma^{-1}\mu_1 + \Sigma_2^{-1}(\mu_2-\mu_1) \\
  &= \mu_1 + \Sigma_1(\Sigma_1+\Sigma_2)^{-1}(\mu_2-\mu_1),
  \end{split}
  \end{equation}
and by symmetry, we also have 
\begin{equation}\label{mu:4}
  \mu = \mu_2 + \Sigma_2(\Sigma_1+\Sigma_2)^{-1}(\mu_1-\mu_2).
\end{equation}
Then we premultiply \eqref{mu} by $\mu^T$, substituting \eqref{mu:3} into the first term and
  \eqref{mu:4} into the second, as well as using \eqref{Sigma:1}, to get 
\begin{align*}
    \mu^T\Sigma^{-1}\mu &= \mu^T\Sigma_1^{-1}\mu_1 + \mu^T\Sigma_2^{-1}\mu_2 \\
    &= \mu_1^T\Sigma_1^{-1}\mu_1 + (\mu_2-\mu_1)^T(\Sigma_1+\Sigma_2)^{-1}\mu_1
    +\mu_2^T\Sigma_1\mu_2 + (\mu_1-\mu_2)^T(\Sigma_1+\Sigma_2)^{-1}\mu_2\\
    &= \mu_1^T\Sigma_1^{-1}\mu_1 + \mu_2^T\Sigma_2^{-1}\mu_2 - (\mu_2-\mu_1)^T(\Sigma_1+\Sigma_2)^{-1}(\mu_2-\mu_1).
\end{align*}

Finally, substituting this result into \eqref{C} yields
\begin{equation}\label{C:1}
  C = (\mu_2-\mu_1)^T(\Sigma_1+\Sigma_2)^{-1}(\mu_2-\mu_1)
\end{equation}
as required.
\end{proof}

\begin{Cor}\label{comp_sq:2}
\begin{align*}
  \N(x;\mu_1\Sigma_1)\N(x;\mu_2,\Sigma_2) &= \N(\mu_1;\mu_2,\Sigma_1+\Sigma_2)
  \N(x;\mu,\Sigma) \quad\text{where}\\
 \mu :&= \Sigma_2(\Sigma_1+\Sigma_2)^{-1}\mu_1+\Sigma_1(\Sigma_1+\Sigma_2)^{-1}\mu_2, \quad\text{and}\\
 \Sigma :&= \Sigma_1(\Sigma_1 + \Sigma_2)^{-1}\Sigma_2.
\end{align*}
\begin{proof}
  The right-hand and left-hand exponents above are equal by \eqref{comp_sq:1}, and it is
  straightforward to verify directly that the normalization factors outside the exponentials are also equal. 
\end{proof}
\end{Cor}


\section{The Alpha Pass}
  Now we can inductively evaluate \eqref{alpha:0}. To do so, we split the computation
  into two steps.  In the first step, which we call the {\em time update}, we multiply
  $\alpha_{t-1}(u)$ by the state transition function $Pd(s\mid u)$ and integrate with respect
  to $u$. We will denote the result of the time update by $\hat{\alpha}_t(s)$.  It is the joint probability
  (density) of observations $x_1,\dots,x_{t-1}$ and state $s$ at time $t$.
  Then in the second step, which we call the {\em measurement update}, we multiply
  $\hat{\alpha}_t(s)$ by $X(x_t \mid s)$, the probability (density) of observing $x_t$ at time $t$
  in state $s$, to get $\alpha_t(s)$.

  First, we define
\begin{align*}
  P_{m,n} &:= Pd(x_m,x_{m+1},\dots,x_n \mid \theta),\quad\text{for $1\le m \le n\le T$},\quad\text{and}\\
  P_{1,0} &:= 1.
\end{align*}
\begin{Thm}\label{alpha:1}
  For each state $s$ and time $t \ge 1$,
$$
  \alpha_t(s) = P_{1,t}N(s;\mu_{a,t},\Sigma_{a,t}),
$$
where
\begin{align*}
  \mu_{a,t} &= \Sigma_{Ob}(\Sigma_{Ob}+\widehat{\Sigma}_t)^{-1}\mu_{a,t-1}
  + \widehat{\Sigma}_t(\Sigma_{Ob}+\widehat{\Sigma}_t)^{-1}x_t, \\
  \Sigma_{a,t} &= \Sigma_{Ob}(\Sigma_{Ob}+\widehat{\Sigma}_t)^{-1}\widehat{\Sigma}_t,\\
  \widehat{\Sigma}_t &= \Sigma_{Tr}+\Sigma_{a,t-1},
 \quad\text{and}\\
  P_{1,t} &=P_{1,t-1}(2\pi)^{\frac{n}{2}}\det(\Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}\Sigma_{t-1})^{\frac{1}{2}}
  N(x_t;\mu_{t-1},\Sigma_{Ob} + \widehat{\Sigma}_t).
\end{align*}
\end{Thm}

\begin{proof}

Proceeding by induction on $t$, we note that the case $t = 0$ holds by definition.
For the time update, we have, using \eqref{comp_sq:2}
\begin{align}
  \hat{\alpha}_t(s) &= \int_{\R^n}Pd(s|u)\alpha_{t-1}(u)du \\\notag
  &= P_{1,t-1}\int_{\R^n}\N(u;s,\Sigma_{Tr})\N(u;\mu_{t-1},\Sigma_{t-1})du \notag\\
  &= P_{1,t-1}\N(s;\mu_{t-1},\widehat{\Sigma}_t)\int_{\R^n}\N(u;\hat{\mu}_t,
  \widetilde{\Sigma}_t) du,
\end{align}
where
 \begin{align*}
  \hat{\mu}_t &:= \Sigma_{Tr}\widehat{\Sigma}_t^{-1}\mu_{t-1} +
  \Sigma_{t-1}\widehat{\Sigma}_t^{-1}s, \quad\text{and}\\
  \widetilde{\Sigma}_t &:= \Sigma_{Tr}\widehat{\Sigma}_t^{-1}\Sigma_{t-1}.
  \end{align*}
Because neither $\hat{\mu_t}$ nor $\widetilde{\Sigma}_t$ depends on $u$, we can
make the affine change of variable
$$
v = S(u-\hat{\mu}_t),\quad u = S^{-1}v+\hat{\mu}_t,
$$
where $S$ is the cholesky factor of $\widetilde{\Sigma}_t^{-1}$ satisfying
$S^tS = \widetilde{\Sigma}_t^{-1}$. This transforms the integrand to
  $\det{S}^{-1}\N(v;0,I)dv$
with unchanged domain of integration $\R^n$, which integrates to
$$
(2\pi)^{\frac{n}{2}}\det{S}^{-1} = (2\pi)^{\frac{n}{2}}\det{\widetilde{\Sigma}_t}^{\frac{1}{2}}.
$$
The result is
$$
\hat{\alpha}_t(s) = P_{1,t-1}(2\pi)^{\frac{n}{2}}\det{\widetilde{\Sigma}_t}^{\frac{1}{2}}
\N(s;\mu_{t-1},\widehat{\Sigma}_t).
$$
Multiplying by $X(x_t\mid s)$ finally yields
\begin{align*}
  \alpha_t(s) &= X(x_t\mid s)\hat{\alpha}_t(s) \\
  &= P_{1,t-1}(2\pi)^{\frac{n}{2}}\det{\widetilde{\Sigma}_t}^{\frac{1}{2}}
  \N(s;x_t,\Sigma_{Ob})\N(s;\mu_{t-1},\widehat{\Sigma}_t) \\
  &= P_{1,t-1}(2\pi)^{\frac{n}{2}}\det{\widetilde{\Sigma}_t}^{\frac{1}{2}}
  \N(x_t;\mu_{t-1},\Sigma_{Ob} + \widehat{\Sigma}_t)\N(s;\mu_{a,t},\Sigma_{a,t}), \\
  &= P_{1,t}\N(s;\mu_{a,t},\Sigma_{a,t}),
\end{align*}
as required.
\end{proof}

\section{The Beta Pass}
This calculation is very similar to the alpha pass; the main difference being that we do
the measurement update first by multiplying by $X(x_{t+1}\mid s)$ to obtain $\hat{\beta}_t(s)$,
and then we integrate with respect to $Pd(u\mid s)du$ for the time update.  To conserve notation,
we redefine the intermediate quantities $\hat{\mu}_t$ and $\widehat{\Sigma}_t$ below.

\begin{Thm}\label{beta:1}
  For each state $s$ and time $t \le T$,
$$
  \beta_t(s) = P_{t+1,T}N(s;\mu_{b,t},\Sigma_{b,t}),
$$
where
\begin{align*}
  \mu_{b,t} &= \Sigma_{Tr}(\Sigma_{Tr}+\widehat{\Sigma}_t)^{-1}\hat{\mu}_{t+1}
  + \widehat{\Sigma}_t(\Sigma_{Tr}+\widehat{\Sigma}_t)^{-1}, \\
  \Sigma_{b,t} &= \Sigma_{Tr}(\Sigma_{Tr}+\widehat{\Sigma}_t)^{-1}\widehat{\Sigma}_t,\\
  \widehat{\Sigma}_t &= \Sigma_{Ob}+\Sigma_{b,t+1},
 \quad\text{and}\\
  P_{1,t} &=P_{1,t-1}(2\pi)^{\frac{n}{2}}\det(\Sigma_{Tr}\widehat{\Sigma}_{b,t}^{-1}\Sigma_{t-1})^{\frac{1}{2}}
  N(x_t;\mu_{t-1},\Sigma_{Ob} + \widehat{\Sigma}_t).
\end{align*}
\end{Thm}

\begin{proof}
  We proceed by reverse induction on $t$.  To evaluate \eqref{beta:0} we apply \eqref{comp_sq:2}:
  \begin{align*}
    \hat{\beta}_t(u) &= X(x_{t+1}\mid u)\beta_{t+1}(u)\\
    &= \N(u;x_{t+1},\Sigma_{Ob})\N(u;\mu_{b,t+1},\Sigma_{b,t+1})\\
    &= \N(x_t;\mu_{b,t+1},\widehat{\Sigma}_t)\N(u;\hat{\mu}_t,\widetilde{\Sigma}_t),\quad\text{where}\\
    \widehat{\Sigma_t} :&= \Sigma_{b,t+1}+\Sigma_{Ob},\\
    \hat{\mu}_t :&= \Sigma_{b,t+1}\widehat{\Sigma}_t^{-1}x_{t+1} + \Sigma_{Ob}\widehat{\Sigma}_t^{-1}\mu_{b,t+1},\quad\text{and}\\    \widetilde{\Sigma}_t :&= \Sigma_{b,t+1}\widehat{\Sigma}_t^{-1}\Sigma_{Ob}.
  \end{align*}

  Then a second application of \eqref{comp_sq:2} yields
  \begin{align*}
    \beta_t(s) &= \int_{\R^n}\hat{\beta}_t(u)Pd(u\mid s)du \\
    &= \N(x_t;\mu_{b,t+1},\widehat{\Sigma}_t)\int_{\R^n}\N(u;\hat{\mu}_t,\widetilde{\Sigma}_t)\N(u;s,\Sigma_{Tr})du\\
    &=\N(x_t;\mu_{b,t+1},\widehat{\Sigma}_t)\N(s;\hat{\mu_t},\widetilde{\Sigma}_t+\Sigma_{Tr})\int_{\R^n}\N(u;\mu_*,\Sigma_*)du \\
    &= \N(x_t;\mu_{b,t+1},\widehat{\Sigma}_t)\det(2\pi\Sigma_*)^{\frac{1}{2}}
    \N(s;\hat{\mu_t},\widetilde{\Sigma}_t+\Sigma_{Tr}),
  \end{align*}
  where
  $$
  \Sigma_* := \widetilde{\Sigma}(\widetilde{\Sigma}+\Sigma_{Tr})^{-1}\Sigma_{Tr}.
  $$
\end{proof}
\end{document}

Problems with re-estimation:
1. How to reestimate the state transition function covariance matrix? In the
discrete world, we compute \gamma_{ij}, the posterior probability of state i
and time t-1 and state j at time t.  Maybe just the product \alpha_t(s_0)Pd(s|s_0)\hat{\beta}_{t=1}(s) ?
2. How to reestimate $M$ and $b$? Use least-squares!
