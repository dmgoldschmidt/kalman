\documentclass[12pt,leqno]{article}
\include{article_defs}
\title{A Continuous State Hidden Markov Model}
\author{David M. Goldschmidt}
%\oddsidemargin 10 pt \evensidemargin 10 pt \marginparwidth 0.75 in \textwidth
%6.0 true in \topmargin -40 pt \textheight 8.8 true in 
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lfoot{}
%\rfoot{\thepage}
\begin{document}
%\renewcommand{\footrulewidth}{0.4pt}
\newcommand{\p}{\ensuremath{u}}
\newcommand{\VV}{V}
\maketitle


\section{Introduction}
Let $\mu$ and $x$ be vectors in $\R^n$ and let $\S$ be an $n\times{n}$ positive
definite real symmetric matrix.  Recall that the gaussian probability density
on $\R^n$ is given by 
$$
\N(x;\mu,S) := (2\pi)^{-\frac{n}{2}}|S|^{\frac{1}{2}}\exp\left\{-\frac{1}{2}
(x - \mu)^TS(x-\mu)\right\}.
$$
So here and below, capital $S$ denotes an inverse covariance matrix.

We now define a Hidden Markov Model with state space $\R^n$ as follows.  Given the state $s\in\R^n$ and an observation $x_t\in\R^m$ at time $t$, the output density is
$$
X(x_t\mid s) := \N(x_t;M_ts + b_t,S_{Ob}),
$$
where $M_t$ is a linear map from the state space to the
measurement space, $b_t$ is a bias, and $S_{Ob}$ is an $m\times{m}$ positive definite real symmetric matrix.  In this paper, $S_{Ob}$ and 
$M$ are time-independent model parameters which we usually seek to re-estimate from the data.  The bias $b_t$ can usually be 
subtracted from the data so we assume here that $b_t = 0$ for all $t$. 
In the simplest case, $M_t = I$ for all $t$ so that $m = n$ and $s$ is itself the mean of the output distribution.  
The state process is a discrete time continuous state Markov process with transition probability density
from state $s_0$ at time $t$ to state $s_1$ at time $t+1$ given by
$$
Pd(s_1\mid s_0) :=  \N(s_1;s_0+c_t,S_{Tr}),
$$
where $S_{Tr}$ is a time-independent model parameter 
and $c_t$ is a known ``control input'' which we also assume is zero for the time being.

As usual, we are given observations $\{x_t\mid 1\le t\le T\}$ and model parameters $\theta$.  For each observation time $t$ and state $s$, we define
\begin{align*}
  \alpha_t(s) :&= Pd(x_1,x_2,\dots,x_t~ \& \text{state $s$ at time $t$}\mid \theta ),\\
  \beta_t(s) :&= Pd(x_{t+1},\dots,x_T \mid ~\text{state $s$ at time $t$},\theta)\\
  \gamma_t(s) :&= \frac{\alpha_t(s)\beta_t(s)}{\int_{\R^n}\alpha_t(s)\beta_t(s)ds}.
\end{align*}

Thus, $\alpha_t(s)$ is the joint density of state $s$ at time $t$ and the observations up to (and including) time $t$,
$\beta_t(s)$ is the {\em conditional} density of the future observations given state $s$ at time $t$, and $\gamma_t(s)$
is the posterior probability density of state $s$ at time $t$.

It is clear from the definitions that the following recursions are satisfied:
\begin{align}
\alpha_t(s) &= X(x_t \mid s)\int_{\R^n}\alpha_{t-1}(u)Pd(s \mid u)du,\quad\text{and}\label{alpha:0}\\
\beta_t(s) &= \int_{\R^{n}}Pd(u \mid s)X(x_{t+1} \mid u)\beta_{t+1}(u)du.\label{beta:0}
\end{align}

We initialize these recursions with
\begin{align*}
\alpha_0(s) :&= \N(s;\mu_0,S_0) \\
\beta_{T}(s) :&= 1 \quad\text{for all $s$},
\end{align*}
where $S_0$ and $\mu_0$ are model parameters.

Because everything in sight is gaussian, the above integrals can be evaluated in closed form
as follows.
\section{Completing the Square} 
\begin{Lem}
 Define the quadratic form
  $$
 Q(x;\mu,S) := (x-\mu)^TS(x-\mu),
  $$
  where $x,\mu\in \R^n$ and  $S$ is a symmetric positive definite $n\times{n}$
  matrix. Let $M$ be an $m\times{n}$ matrix, $\mu_1\in\R^n$, $\mu_2\in\R^m$,
  and $S_1$,$S_2$ be symmetric positive definite.  Then 
\begin{equation}\label{comp_sq:1}
  Q(x;\mu_1,S_1)+Q(Mx;\mu_2,S_2) = Q(x;\mu,S) + R(\mu_1,\mu_2,\Sigma_1,\Sigma_2),
\end{equation}
where 
\begin{align}
S &= S_1 + M^TS_2M, \label{sigma}\\
\mu &= S^{-1}(S_1\mu_1 + M^TS_2\mu_2),\quad\text{and}\label{mu}\\
R &= \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2 - \mu^TS\mu. \label{R_def} 
\end{align}
Furthermore, if $M = I$, then $S_1S^{-1}S_2 = S_2S^{-1}S_1$,  and
\begin{equation}\label{comp_sq:R}
R = (\mu_2-\mu_1)^TS_1S^{-1}S_2(\mu_2-\mu_1).
\end{equation}
\end{Lem}
\begin{proof}
  Note that $S_1$ is postive definite and $M^TS_2M$ is positive semi-definite,
so $S$ is invertible.  Expanding the left-hand side of \eqref{comp_sq:1} and combining
like terms, we get
$$
x^T(S_1+ M^TS_2M)x -2x^T(S_1\mu_1+ M^TS_2\mu_2) 
+ \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2.
$$
Completing the square, we get
$$
(x-\mu)^TS(x-\mu) -\mu^TS\mu + \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2,
$$
as asserted.

Now assume $M = I$. We first note that
\begin{align}
S_1^{-1}SS_2^{-1} &= S_1^{-1}(S_1+S_2)S_2^{-1} = S_1^{-1}+S_2^{-1} = S_2^{-1}SS_1^{-1},\quad\text{hence}\notag\\
S_2S^{-1}S_1 &= (S_1^{-1}+S_2^{-1})^{-1} = S_1S^{-1}S_2.\label{symmetry}
\end{align}
Moreover, we can re-write \eqref{mu} as
\begin{equation}\label{mu:1}
\mu = S^{-1}(S_1\mu_1 + S_2\mu_1 + S_2(\mu_2-\mu_1) = \mu_1 + S^{-1}S_2(\mu_2-\mu_1),
\end{equation}
and by symmetry
\begin{equation}\label{mu:2}
\mu = \mu_2 + S^{-1}S_1(\mu_1-\mu_2).
\end{equation}
Then from \eqref{mu} we have
$$
\mu^TS\mu = \mu^T(S_1\mu_1 + S_2\mu_2),
$$
and substituting \eqref{mu:1} in the first term and \eqref{mu:2} in the second, we get
$$
\mu^TS\mu = \mu_1^TS_1\mu_1 + (\mu_2-\mu_1)^TS_2S^{-1}S_1\mu_1 + \mu_2^TS_2\mu_2 + (\mu_1-\mu_2)^TS_1S^{-1}S_2\mu_2.
$$
Finally, \eqref{symmetry} yields
$$
\mu^TS\mu = \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2 - (\mu_2-\mu_1)^TS_1S^{-1}S_2(\mu_2-\mu_1)
$$
and combining this with \eqref{R_def} proves \eqref{comp_sq:R} and completes the proof.
\end{proof}

By virtue of \eqref{symmetry}, we define for $S_1$ positive definite:
$$
S_1*S_2 := S_1(S_1+S_2)^{-1}S_2.
$$

\begin{Cor}\label{comp_sq:2}
\begin{align*}
  \N(x;\mu_1,S_1)\N(Mx;\mu_2,S_2) &= \N_0\N(x;\mu,S)\quad\text{where}\\
  S :&= S_1+M^TS_2M,\\
  \mu :&= S^{-1}(S_1\mu_1 + M^TS_2\mu_2)\quad\text{and} \\
  \N_0 :&= \left(\frac{|S_1|\cdot|S_2|}{2\pi|S|}\right)^{\frac{1}{2}}\exp\{-\frac{1}{2}(\mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2-\mu^TS\mu)\}.
\end{align*}

In particular, $\N_0$ does not depend on $x$, and
\begin{equation}\label{comp_sq:int}
  \int_{\R^{n}}  \N(x;\mu_1,S_1)\N(Mx;\mu_2,S_2)dx = \N_0.
\end{equation}

Furthermore, if $M = I$ then 
\begin{equation}\label{exact_prod} 
  \N_0 = \N(\mu_1;\mu_2,S_1*S_2).
\end{equation}
\end{Cor}

\section{The Forward Pass}
  Now we can inductively evaluate \eqref{alpha:0}. To do so, we split the computation
  into two steps.  In the first step, which we call the {\em time update}, we multiply
  $\alpha_{t-1}(u)$ by the state transition function $Pd(s\mid u)$ and integrate with respect
  to $u$. We will denote the result of the time update by $\hat{\alpha}_t(s)$.  It is the joint probability
  density of observations $x_1,\dots,x_{t-1}$ and state $s$ at time $t$.
  Then in the second step, which we call the {\em measurement update}, we multiply
  $\hat{\alpha}_t(s)$ by $X(x_t \mid s)$, the probability density of observing $x_t$ at time $t$
  in state $s$, to get $\alpha_t(s)$.

  First, for $1\le t \le T$ we inductively define
  \begin{align*}
    \hat{S}_{a,t} &= S_{Tr}*S_{a,t-1},\\
    S_{a,t} &:= M^TS_{Ob}M + \hat{S}_{a,t},\\
    \mu_{a,t} &:= S_{a,t}^{-1}(M^TS_{Ob}x_t + \hat{S}_{a,t}\mu_{a,t-1})\\
    R_{a,t} &:= x_t^TS_{Ob}x_t + \mu_{a,t-1}^T(\hat{S}_{a,t})\mu_{a,t-1} - \mu_{a,t}^TS_{a,t}\mu_{a,t},\\
    N_{a,t} &:= \left(\frac{|S_{Ob}|\cdot|\hat{S}_{a,t}|}{2\pi|S_{a,t}|}\right)^{\frac{1}{2}}e^{-\frac{1}{2}R_{a,t}},\\
    P_{a,t} &:= P_{a,t-1}N_{a,t}, \quad\text{and}\\
    P_{a,0} &:= 1.
  \end{align*}

\begin{Thm}\label{alpha:1}
  For each state $s$ and time $t \ge 1$,
$$
  \alpha_t(s) = P_{a,t}N(s;\mu_{a,t},S_{a,t})
$$
\end{Thm}

\begin{proof}

Proceeding by induction on $t$, we note that the case $t = 0$ holds by definition.
For the time update, we have, using \eqref{comp_sq:2}
\begin{align}
  \hat{\alpha}_t(s) &= \int_{\R^n}Pd(s|u)\alpha_{t-1}(u)du \\\notag
  &= P_{a,t-1}\int_{\R^n}\N(u;s,S_{Tr})\N(u;\mu_{a,t-1},S_{a,t-1})du \notag\\
  &= P_{a,t-1}\N(s;\mu_{a,t-1},\hat{S}_{a,t}).
\end{align}

Then multiplying by $X(x_t\mid s)$ and using \eqref{comp_sq:2} again yields
\begin{align*}
  \alpha_t(s) &= X(x_t\mid s)\hat{\alpha}_t(s) \\
  &= P_{a,t-1}\N(Ms;x_t,S_{Ob})\N(s;\mu_{a,t-1},\hat{S}_{a,t}) \\
  &= P_{a,t_1}N_{a,t}\N(s;\mu_{a,t},S_{a,t}).
\end{align*}
\end{proof}

\section{The Backward Pass}
This calculation is very similar to the forward pass; the main difference being that we do
the measurement update first by multiplying by $X(x_{t+1}\mid u)$ to obtain $\hat{\beta}_{t+1}(u)$,
and then we integrate with respect to $Pd(u\mid s)du$ for the time update.  

\begin{Thm}\label{beta:1}
  For $t < T$, inductively define
\begin{align*}
  \hat{S}_{b,t} :&= M^TS_{Ob}M + S_{b,t+1};\\
  S_{b,t} :&= \hat{S}_{b,t}*S_{Tr},\\
  \mu_{b,t} :&= \hat{S}_{b,t}^{-1}(M^TS_{Ob}x_{t+1} + S_{b,t+1}\mu_{b,t+1}),\\
  R_{b,t} :&= x_{t+1}^TS_{Ob}x_{t+1} + \mu_{b,t+1}^TS_{b,t+1}\mu_{b,t+1} - \mu_{b,t}^T\hat{S}_{b,t}\mu_{b,t},\\
  N_{b,t} :&= \left(\frac{|S_{Ob}|\cdot|S_{b,t+1}|}{|2\pi\hat{S}_{b,t}|}\right)^{\frac{1}{2}}e^{-\frac{1}{2}R_{b,t}},\quad\text{and}\\
  P_{b,t}:&=P_{b,t+1}N_{b,t},
\end{align*}
with initial values at $t = T$ given below. Then for each state $s$ and time $t < T$,
$$
  \beta_t(s) = P_{b,t}\N(s;\mu_{b,t},S_{b,t}).
$$
\end{Thm}

\begin{proof}
We proceed by reverse induction on $t$. However, to get started we first must  deal with
the special case of $\beta_{T}(u)$, the conditional probability of the future observations given
state $u$ at time $T$, but of course there are no such observations.  In the standard discrete
state HMM, we set $\beta(s) = 1$ for all states $s$, and this will work in our case as well by
setting $S_{b,T} = 0$ if $M$ is 1-1, but we really need a proper gaussian 
density in order to get the reverse induction started in all cases.  So we somewhat arbitrarily
assume the following values
\begin{align*}
  S_{b,T} :&= I,\\
  \mu_{b,T} :&= 0,\\
  P_{b,T} :&= 1;
\end{align*}
  For $t <  T$, we have the measurement update 
  \begin{align*}
    \hat{\beta}_{t+1}(u) &= X(x_{t+1}\mid u)\beta_{t+1}(u)\\
    &= P_{b,t+1}\N(Mu;x_{t+1},S_{Ob})\N(u;\mu_{b,t+1},S_{b,t+1})\\
    &= P_{b,t+1}N_{b,t}\N(u;\mu_{b,t},\hat{S}_{b,t+1})
  \end{align*}

  Then a second application of \eqref{comp_sq:2} yields the time update
  \begin{align*}
    \beta_t(s) &= \int_{\R^n}\hat{\beta}_{t+1}(u)Pd(u\mid s)du \\
    &= P_{b,t+1}N_{b,t}\int_{\R^n}\N(u;\mu_{b,t},\hat{S}_{b,t+1})\N(u;s,S_{Tr})du\\
    &=P_{b,t+1}N_{b,t}\N(s;\mu_{b,t},S_{b,t})\\
    &=P_{b,t}\N(s;\mu_{b,t},S_{b,t}).
  \end{align*}
\end{proof}

\section{The Posterior Likelihoods}
Recall that $\gamma_t(s)$ is the posterior probability density of state $s$
at time $t$. 
\begin{Thm}
  Let notation be as in \eqref{alpha:1} and \eqref{beta:1}.  Define
\begin{align*}
  S_{c,t} :&= S_{a,t} + S_{b,t}, \\
  \mu_{c,t} :&= S_{c,t}^{-1}(S_{a,t}\mu_{a,t} + S_{b,t}\mu_{b,t}), \quad\text{and}\\
  P_{c,t} :&= P_{a,t}P_{b,t}\N(\mu_{a,t};\mu_{b,t},S_{a,t}*S_{b,t}), \quad\text{then} \\
 \gamma_t(s) &= \N(s;\mu_{c,t},S_{c,t}).
  \end{align*}
\end{Thm}
\begin{proof}
  From \eqref{alpha:1} and \eqref{beta:1} we have (using \eqref{comp_sq:2}
  as usual)
  \begin{align*}
  \gamma_t{s} \propto \alpha_t(s)\beta_t(s) &= P_{a,t}P_{b,t}\N(s;\mu_{a,t},S_{a,t})\N(s;\mu_{b,t},S_{b,t})\\
  &= P_{c,t}\N(s;\mu_{c,t},S_{c,t}).
  \end{align*}
\end{proof}

We also need the joint posterior probability density of state $u$ at time $t-1$ and state $s$ at time $t$,
which we denote by $\hat{\gamma}_t(s,u)$. The joint probability density of all the data and the two given states
  $u,s$  at times $t-1,t$ is
  \begin{align}
    \hat{\gamma}_t(s,u) &\propto \alpha_{t-1}(u)Pd(s\mid u)X(x_t\mid s)\beta_t(s) \notag\\
    &\propto \N(u;\mu_{a,t-1},S_{a,t-1})\N(u;s,S_{Tr})  \N(Ms;x_t,S_{Ob})\N(s;\mu_{b,t},S_{b,t})\notag\\
    &\propto \N(u;\mu_{u,t},S_{u,t})\N(s;\mu_{a,t-1},\hat{S}_{a,t})\N(s;\mu_{b,t-1},\hat{S}_{b,t})\notag\\
    &\propto \N(u;\mu_{u,t},S_{u,t})\N(s;\mu_{s,t},S_{s,t}),\label{gamma_hat}
  \end{align}
  where we have discarded factors which do not depend on either $u$ or $s$, and 
  \begin{equation}\label{gamma_su}
    \begin{split}
      S_{u,t} &:= S_{a,t-1}+S_{Tr},\\
      \mu_{u,t} &:= S_{u,t}^{-1}(S_{Tr}s + S_{a,t-1}\mu_{a,t-1}),\\
      S_{s,t} &:= \hat{S}_{a,t} + \hat{S}_{b,t},\quad\text{and}\\
      \mu_{s,t} &:= S_{s,t}^{-1}(\hat{S}_{a,t}\mu_{a,t-1} + \hat{S}_{b,t}\mu_{b,t-1}). 
    \end{split}
  \end{equation}
  

  Note that
  $$
  \int_{\R^{2n}}\N(u;\mu_{u,t},S_{u,t})\N(s;\mu_{s,t}S_{s,t})duds = \int_{\R^n}\N(s;\mu_{s,t}S_{s,t})ds = 1 
  $$
  
  because the second factor does not depend on $u$, so \eqref{gamma_hat} is a bona-fide density and no
  normalization factor is required.
  
\section{Re-estimation}
Given a particular state sequence $S := \{s_0,s_1,s_2,\dots,s_T\}$ and the data sequence $X := \{x_1,x_2,\dots,x_T\}$,
the joint probability density of $S$ and $X$ given parameters $\theta := \{\mu_0,S_0,S_{Tr},S_{Ob},M\}$ is
$$
P(X,S\mid\theta) = \N(s_0;\mu_0,S_0)\prod_{t=1}^T\N(x_t;Ms_t,S_{Ob})\N(s_t;s_{t-1},S_{Tr}).
$$

Note that $P(X,S\mid\theta)$ can also be viewed as  $L(\theta\mid X,S)$, the posterior likelihood of $\theta$.
To re-estimate $\theta$, we try to maximize $L(\theta)$ using the EM algorithm. Namely, we define 
$$
E(\theta,\bar{\theta}) := \int_{\R^{Tn}}P(X,S\mid\theta)\log{L(\bar{\theta}\mid X,S)}dS,
$$
where $\theta$ is the current set of parameters, and $\bar{\theta}$ is the unknown set of new parameters
we wish to determine. So instead of maximizing the log-likelihood directly, we can maximize its expected
value with respect to the current posterior distribution, because it is a standard result (and easy to prove) that
$$
L(\bar{\theta}) - L(\theta) \ge E(\theta,\bar{\theta}) - E(\theta,\theta),
$$
so choosing $\bar{\theta}$ to maximize $E(\theta,\bar{\theta})$ will increase the value of $L(\theta)$.


  
Recall the notation $Q(x;\mu,S)$ from \eqref{comp_sq:1}.  Then we have
\begin{align}
  E(\theta,\bar{\theta}) &= -(2T+1)\frac{n\log(2\pi)}{2} + \frac{1}{2}(E_0 + E_1 + E_2),\quad\text{where}\notag\\
  E_0 :&=\int_{\R^{Tn}}P(X,S\mid\theta)\{\log|\overline{S}_0| - Q(s_0;\bar{\mu}_0,\overline{S}_0)\}dS,\label{E0}\\
  E_1 :&= \int_{\R^{Tn}}P(X,S\mid\theta)\left\{\sum_{t=1}^T\log|\overline{S}_{Ob}| - Q(x_t;Ms_t,\overline{S}_{Ob})\right\}dS,
\label{E1}\\
  E_2 :&= \int_{\R^{Tn}}P(X,S\mid\theta)\left\{\sum_{t=1}^T\log|\overline{S}_{Tr}| - Q(s_t;s_{t-1},\overline{S}_{Tr})\right\}dS,
\label{E2}
\end{align}
Since the expected value of a constant is just that constant, $-n\log(2\pi)/2$ can be moved outside each
integral sign and ignored in the optimization.  And since none of the parameters of $\theta$ appear in more
than one of the $E_i$, we can optimize $E$ by optimizing each of $E_1,E_2,E_3$ separately.

We begin with $E_0$. Since the only component of $S$ in the integrand of \eqref{E0} is $s_0$, the expected value
collapses to the marginal 
expected value of the integrand with respect to the marginalization of $P(X,S\mid\theta)$ at $s_0$, which is just 
$\gamma_0(s_0)$.  Furthermore, after pulling the constant and the summation 
outside the integral in \eqref{E1} we have the same collapse to the marginalization at $s_t$ of the posterior, which
is just $\gamma_t(s_t)$.  It follows that
\begin{align*}
E_0 &= \log|\overline{S}_0| - \int_{\R^n}\gamma_0(s)Q(s;\bar{\mu}_0,\overline{S}_0)ds,\quad\text{and}\\
E_1 &= T\log|\overline{S}_{Ob}| - \sum_{t=1}^T\int_{\R^n}\gamma_t(s)Q(x_t;Ms,\overline{S}_{Ob})ds.
\end{align*}

The situation for \eqref{E2} is slightly different, because after again moving the constant and the summation outside
the integral, the integrand depends on both $s_t$ and $s_{t-1}$.  So in this case, the integral collapses to the 
joint marginal expectation over $s_t$ and $s_{t-1}$, and we get
$$
E_2 = T\log|\overline{S}_{Tr}| - \sum_{t=1}^T\int_{\R^{2n}}\hat{\gamma}_t(s,u)Q(s;u,\overline{S}_{Tr})duds,
$$
where $\hat{\gamma}_t(s,u)$ is the joint posterior probability density of state $u$ at time $t-1$ and state $s$ 
at time $t$ given by \eqref{gamma_hat}.

If $f$ is any scalar-valued function of an $m\times{n}$ matrix $A = a_{ij}$, we denote by $\partial{f}/\partial{A}$ the 
$m\times{n}$ matrix whose $(i,j)$-entry is $\partial{f}/\partial{a_{ij}}$.  This also applies to column vectors (when $n=1$).
In particular, if $m=n$, $A$ is symmetric, and $f(A) = |A|$,  then the usual expansion 
in minors along the $i^{th}$ row shows that 
$$
\frac{\partial{|A|}}{\partial{A}} = (2-\delta_{ij})A^* = (2-\delta_{ij})|A|A^{-1},\quad\text{and therefore}
\quad\frac{\partial{\log|A|}}{\partial{A}} = (2-\delta_{ij})A^{-1}. 
$$
Here $A^*$ is the adjoint matrix and the notation $(2-\delta_{ij})A^*$ means to multiply all off-diagonal elements of $A^*$ by 2
and leave the diagonal unchanged.  This is of course necessary due the the symmetry $A = A^T$.

In addition, since $Q(x;\mu,S)$ is linear in the coefficients of $S$, we see that
$$
\frac{\partial{Q(x;\mu,S)}}{\partial{S}} = (2-\delta_{ij})(x-\mu)(x-\mu)^T,
$$
and it is also easy to verify that
$$
\frac{\partial{Q(x;\mu,S)}}{\partial{\mu}} = 2S(\mu-x).
$$

Finally, to reestimate the $M$-matrix, we need

\begin{Lem}\label{dM}
  $$
  \frac{\partial{Q(x;M\mu,S)}}{\partial{M}} = -2Sx\mu^T + 2SM\mu\mu^T.
  $$
\end{Lem}
\begin{proof}
Expanding, we have
$$
Q := (x-M\mu)^TS(x-M\mu) = x^TSx - 2x^TSM\mu + \mu^TM^TSM\mu.
$$
Differentiating the linear term is just a special case of
$$
\frac{\partial}{\partial{M}}y^TMx = yx^T.
$$
For the quadratic term, we have
$$
Q_0 := \mu^TM^TSM\mu = \sum_{i,j,k,l}\mu_iM_{ji}S_{jk}M_{kl}\mu_l.
$$
Collecting terms which involve $M_{qr}$, if $j=q$ and $i=r$, we get
$$
M_{qr}\mu_r\sum_{k,l}S_{qk}M_{kl}\mu_l.
$$
If $k=q$ and $l=r$, we get
$$
M_{qr}\mu_r\sum_{i,j}\mu_iM_{ji}S_{jq},
$$
which is the same as the previous case after replacing $i$ by $l$ and $j$ by $k$, except
that the term $\mu_r^2M_{qr}^2S_{qq}$
(which is the case $j = k = q,i = l = r$) has been counted twice.
We'll call this term the exceptional term. Then
\begin{align*}
  \frac{\partial{Q_0}}{\partial{M_{qr}}} &= 2\mu_r\sum_{(k,l)\neq(q,r)}S_{qk}M_{kl}\mu_l + 2\mu_r^2M_{qr}S_{qq}\\
    &= 2\mu_r\sum_{k,l}S_{qk}M_{kl}\mu_l,
\end{align*}
So the exceptional term isn't really exceptional after all.  We conclude that
$$
\frac{\partial{Q_0}}{\partial{M}} = 2SM\mu\mu^T
$$
as asserted.
\end{proof}
      
Armed with these formulas, we first optimize $E_0$ with respect to $\bar{\mu}_0$: 
 
\begin{align}
0 = \frac{\partial{E_0}}{\partial{\bar{\mu}_0}} &= \int_{\R^n}\gamma_0(s)\frac{\partial{Q(s;\bar{\mu}_0,\overline{S}_0)}}{\partial{\bar{\mu}_0}}ds \notag\\
&= \int_{\R^n}\gamma_0(s)(2\overline{S}_0(\bar{\mu}_0-s))ds\notag\\
&= 2\overline{S}_0\int_{\R^n}\gamma_0(s)(\bar{\mu}_0-s)ds \notag\\
&= 2\overline{S}_0(\bar{\mu}_0-\mu_{c,0}), \quad\text{hence}\notag\\
\bar{\mu}_0 &= \mu_{c,0}\label{mu_0}
\end{align}

Next, we optimize $E_0$ with respect to $\overline{S}_0$, and we can set $\bar{\mu}_0 = \mu_{c,0}$ by \eqref{mu_0}:
\begin{align*}
  0 = \frac{\partial{E_0}}{\partial{\overline{S}_0}} &= \frac{\partial\log|\overline{S}_0|}{\partial\overline{S}_0}
  -\int_{\R^n}\gamma_0(s)\frac{\partial{Q(s;\mu_{c,0},\overline{S}_0)}}{\partial\overline{S}_0}ds \\
&= (2-\delta_{ij})\overline{S}_0^{-1} - (2-\delta_{ij})\int_{\R^n}\gamma_0(s)(s-\mu_{c,0})(s-\mu_{c,0})^Tds.
\end{align*}
Not surprisingly, it follows that
\begin{equation}\label{Sigma_0}
  \begin{split}
    \overline{S}_0^{-1} &= \int_{\R^n}\gamma_0(s)(s-\mu_{c,0})(s-\mu_{c,0})^Tds = S_{c,0}^{-1}, \quad\text{i.e.}\\
    \overline{S}_0 &= S_{c,0}.
  \end{split}
\end{equation}

To optimize $E_1$, we first differentiate with respect to $\overline{S}_{ob}$:
\begin{align*}
  0 = \frac{\partial{E_1}}{\partial{\overline{S}_{Ob}}} &=
  T\frac{\partial{\log|\overline{S}_{Ob}|}}{\partial\overline{S}_{Ob}}
  - \sum_{t=1}^T\int_{\R^n}\gamma_t(s)\frac{\partial{Q(x_t;\overline{M}s,\overline{S}_{Ob})}}{\partial{\overline{S}_{Ob}}}ds \\
  &= (2-\delta{ij})T\overline{S}_{Ob}^{-1} - (2-\delta_{ij})\sum_{t=1}^T
  \int_{\R^n}\gamma_t(s)(x_t-\overline{M}s)(x_t-\overline{M}s)^Tds.
\end{align*}
Thus,  we see that
\begin{equation}\label{S_Ob}
  \begin{split}
    \overline{S}_{Ob}^{-1} &= \frac{1}{T}\sum_{t=1}^T\int_{\R^n}\gamma_t(s)(x_tx_t^T - x_ts^T\overline{M}^T
    - \overline{M}sx_t^T + \overline{M}ss^T\overline{M}^T)ds \\
    &= \frac{1}{T}\sum_{t=1}^T(x_tx_t^T - x_t\mu_{c,t}^T\overline{M}^T - \overline{M}\mu_{c,t}x_t^T +
    \overline{M}(S_{c,t}^{-1}+\mu_{c,t}\mu_{c,t}^T)\overline{M}^T)\\
    &= \frac{1}{T}\sum_{t=1}^T(x_t-\overline{M}\mu_{c,t})(x_t-\overline{M}\mu_{c,t})^T +
    \overline{M}S_{c,t}^{-1}\overline{M}^T.
  \end{split}
\end{equation}

We then differentiate with respect to $\overline{M}$ using \eqref{dM}:
\begin{align*}
  0 = \frac{\partial{E_1}}{\partial{\overline{M}}} &=
  - \sum_{t=1}^T\int_{\R^n}\gamma_t(s)\frac{\partial{Q(x_t;\overline{M}s,\overline{S}_{Ob})}}{\partial{\overline{M}}}ds \\
  &= 2\sum_{t=1}^T\int_{\R^n}\gamma_t(s)(\overline{S}_{Ob}x_ts^T - \overline{S}_{ob}\overline{M}ss^T)ds \\
  &= 2\sum_{t=1}^T(\overline{S}_{Ob}x_t\mu_{c,t}^T - \overline{S}_{Ob}\overline{M}(S_{c,t}^{-1} + \mu_{c,t}\mu_{c,t}^T).
\end{align*}
Multiplying through by $\frac{1}{2}S_{Ob}^{-1}$, we find that
\begin{equation}\label{M_bar}
  \begin{split}
    \overline{M} &= \Gamma_1\Gamma_2^{-1}, \quad\text{where}\\
    \Gamma_1 :&= \sum_{t=1}^Tx_t\mu_{c,t}^T, \quad\text{and}\\
    \Gamma_2 :&= \sum_{i=1}^TS_{c,t}^{-1} + \mu_{c,t}\mu_{c,t}^T.
  \end{split}
\end{equation}

It is important to note here that \eqref{M_bar} is independent of $S_{Ob}$, so to optimize $E_1$, we can first
solve for $\overline{M}$ and then use the result to solve for $\overline{S}_{Ob}$.  

Finally, we optimize $E_2$ by differentiating with respect to $\overline{S}_{Tr}$ and using \eqref{gamma_su}:
\begin{equation}\label{Sigma_Tr:0}
  \begin{split}
  0 &= \frac{\partial{E_2}}{\partial{\overline{S}_{Tr}}} = \frac{\partial{\log|\overline{S}_{Tr}|}}{\partial\overline{S}_{Tr}}
  - \sum_{t=1}^T\int_{\R^{2n}}\hat{\gamma}_t(s,u)\frac{\partial{Q(s;u,\overline{S}_{Tr})}}{\partial\overline{S}_{Tr}}duds \\
      &= (2-\delta_{ij})T\overline{S}_{Tr}^{-1} - (2-\delta{ij}) \sum_{t=1}^T\int_{\R^{2n}}\N(u;\mu_{u,t},S_{u,t})\N(s;\mu_{s,t}S_{s,t})(u-s)(u-s)^Tduds.
  \end{split}
\end{equation}

As noted earlier, $\N(s,\mu_{s,t},S_{s,t})$ does not depend on $u$, so we can integrate first with respect to $u$, with a result
analagous to \eqref{S_Ob}:
$$
  \int_{-\infty}^{\infty}\N(u;\mu_{u,t},S_{u,t})(u-s)(u-s)^Tdu = S_{u,t}^{-1} + (s-\mu_{u,t})(s-\mu_{u,t})^T.
$$
Substituting this result into \eqref{Sigma_Tr:0}, we get
\begin{equation}\label{Sigma_Tr:1}
  \overline{S}_{Tr}^{-1} =  \frac{1}{T}\sum_{t=1}^T\left(S_{u,t}^{-1} +\int_{-\infty}^{\infty}\N(s;\mu_{s,t},S_{s,t})
  (s-\mu_{u,t})(s-\mu_{u,t})^Tds\right).
\end{equation}

However, there is a problem evaluating this integral because after checking \eqref{gamma_su}, we see that $\mu_{u,t}$
depends on $s$, namely  
\begin{equation}\label{s-mu_1}
  \begin{split}
    s-\mu_{u,t} &= (I - S_{u,t}^{-1}S_{Tr})s - S_{u,t}^{-1}S_{a,t-1}\mu_{a,t-1}, \\
    &= (S_{u,t}^{-1}S_{u,t} - S_{u,t}^{-1}S_{Tr})s - S_{u,t}^{-1}S_{a,t-1}\mu_{a,t-1}, \\
    &= S_{u,t}^{-1}(S_{u,t} - S_{Tr})s - S_{u,t}^{-1}S_{a,t-1}\mu_{a,t-1}, \\
    &= S_{u,t}^{-1}S_{a,t-1}(s - \mu_{a,t-1}),\\
    &= S_{Tr}^{-1}\hat{S}_{a,t}(s-\mu_{a,t-1}).
  \end{split}
\end{equation}


Fortunately, neither $\mu_{s,t},S_{s,t},$ nor $S_{u,t}$ depends on $s$,
so using \eqref{gamma_su}, \eqref{Sigma_Tr:1} becomes
\begin{align}
  \overline{S}_{Tr}^{-1} &=\frac{1}{T}\sum_{t=1}^TS_{u,t}^{-1} +\frac{1}{T}S_{Tr}^{-1}\left(\sum_{t=1}^T\hat{S}_{a,t}
  \left[\int_{-\infty}^{\infty}\N(s;\mu_{s,t},S_{s,t})(s-\mu_{a,t-1})(s-\mu_{a,t-1})^Tds\right]\hat{S}_{a,t}\right)S_{Tr}^{-1}
  \notag\\
   &=\frac{1}{T}\sum_{t=1}^TS_{u,t}^{-1}+\frac{1}{T}S_{Tr}^{-1}\left(\sum_{t=1}^T\hat{S}_{a,t}
   [S_{s,t}^{-1}+(\mu_{s,t}-\mu_{a,t-1})(\mu_{s,t}-\mu_{a,t-1})^T]\hat{S}_{a,t}\right)S_{Tr}^{-1}.\label{S_Tr:2}
  \end{align}

Finally, we can make the following simplification by using \eqref{gamma_su}.
\begin{align*}
  \mu_{s,t} - \mu_{a,t-1} &= (S_{s,t}^{-1}\hat{S}_{a,t} - I)\mu_{a,t-1} + S_{s,t}^{-1}\hat{S}_{b,t}\mu_{b,t-1}\\
  &= (S_{s,t}^{-1}\hat{S}_{a,t} - S_{s,t}^{-1}S_{s,t})\mu_{a,t-1} + S_{s,t}^{-1}\hat{S}_{b,t}\mu_{b,t-1}\\
  &= -S_{s,t}^{-1}\hat{S}_{b,t}\mu_{a,t-1} + S_{s,t}^{-1}\hat{S}_{b,t}\mu_{b,t-1}\\
  &= S_{s,t}^{-1}\hat{S}_{b,t}(\mu_{b,t-1}-\mu_{a,t-1}).
\end{align*}

Define
$$
\hat{S}_{a,b,t} := \hat{S}_{a,t}S_{s,t}^{-1}\hat{S}_{b,t} = \hat{S}_{a,t}*\hat{S}_{b,t}.
$$
Then we have 
\begin{equation}\label{S_Tr:3}
  \begin{split}
\overline{S}_{Tr}^{-1} = \frac{1}{T}\sum_{t=1}^TS_{u,t}^{-1} + S_{Tr}^{-1}\left(\frac{1}{T}\sum_{t=1}^T\hat{S}_{a,t}
S_{s,t}^{-1}\hat{S}_{a,t}+ \hat{S}_{a,b,t}(\mu_{b,t-1}-\mu_{a,t-1})(\mu_{b,t-1}-\mu_{a,t-1})^T\hat{S}_{a,b,t}
\right)S_{Tr}^{-1}.
\end{split}
\end{equation}  
\end{document}


