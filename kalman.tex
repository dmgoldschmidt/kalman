\documentclass[12pt,leqno]{article}
\include{article_defs}
\title{A Continuous State Hidden Markov Model}
\author{David M. Goldschmidt}
%\oddsidemargin 10 pt \evensidemargin 10 pt \marginparwidth 0.75 in \textwidth
%6.0 true in \topmargin -40 pt \textheight 8.8 true in 
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lfoot{}
%\rfoot{\thepage}
\begin{document}
%\renewcommand{\footrulewidth}{0.4pt}
\newcommand{\p}{\ensuremath{u}}
\newcommand{\VV}{V}
\maketitle


\section{Introduction}
Let $\mu$ and $x$ be vectors in $\R^n$ and let $\Sigma$ be an $n\times{n}$ positive
definite real symmetric matrix.  Recall that the gaussian probability density
on $\R^n$ is given by 
$$
\N(x;\mu,\Sigma) := \det(2\pi\Sigma)^{-\frac{1}{2}}\exp\left\{-\frac{1}{2}
(x - \mu)^T\Sigma^{-1}(x-\mu)\right\}.
$$
We will make use of the formal identity $\N(x,\mu,\Sigma) = \N(\mu,x,\Sigma)$ below.

We now define a Hidden Markov Model with state space $\R^n$ as follows.  Given the state $s\in\R^n$ and an observation $x_t\in\R^m$ at time $t$, the output density is
$$
X(x_t\mid s) := \N(x_t;M_ts + b_t,\Sigma_{Ob}),
$$
where $M_t$ is a linear map from the state space to the
measurement space, $b_t$ is a bias, and $\Sigma_{Ob}$ is an $m\times{m}$ positive definite real symmetric matrix.  Typically, $\Sigma_{Ob}$ is a time-independent model parameter.
In the simplest case, $b_t = 0$  and $M_t = I$, the identity map, for all $t$ so that $m = n$ and $s$ is itself the mean of the output distribution.  We will call this case the {\em basic} case, and concentrate on it initially. Later, we will consider some generalizations, particularly the case that $M_t = M$ and $b_t = b$ are time-independent model parameters.

The state process is a discrete time continuous state Markov process with transition probability density
from state $s_0$ at time $t$ to state $s_1$ at time $t+1$ given by
$$
Pd(s_1\mid s_0) :=  \N(s_1;s_0+c_t,\Sigma_{Tr}),
$$
where $\Sigma_{Tr}$ is a time-independent parameter of the model
and $c_t$ is a known ``control input'' which we also assume is zero for the time being.

As usual, we are given observations $\{x_t\mid 1\le t\le T\}$ and model parameters $\theta$.  For each observation time $t$ and state $s$, we define
\begin{align*}
  \alpha_t(s) :&= Pd(x_1,x_2,\dots,x_t~ \& \text{state $s$ at time $t$}\mid \theta ),\\
  \beta_t(s) :&= Pd(x_{t+1},\dots,x_T \mid ~\text{state $s$ at time $t$},\theta)\\
  \gamma_t(s) :&= \frac{\alpha_t(s)\beta_t(s)}{\int_{\R^n}\alpha_t(s)\beta_t(s)ds}.
\end{align*}

Thus, $\alpha_t(s)$ is the joint density of state $s$ at time $t$ and the observations up to (and including) time $t$,
$\beta_t(s)$ is the {\em conditional} density of the future observations given state $s$ at time $t$, and $\gamma_t(s)$
is the posterior probability density of state $s$ at time $t$.

It is clear from the definitions that the following recursions are satisfied:
\begin{align}
\alpha_t(s) &= X(x_t \mid s)\int_{\R^n}\alpha_{t-1}(u)Pd(s \mid u)du,\quad\text{and}\label{alpha:0}\\
\beta_t(s) &= \int_{\R^{n}}Pd(u \mid s)X(x_{t+1} \mid u)\beta_{t+1}(u)du.\label{beta:0}
\end{align}

We initialize these recursions with
\begin{align*}
\alpha_0(s) :&= \N(s;\mu_0,\Sigma_0) \\
\beta_{T+1}(s) :&= 1 \quad\text{for all $s$},
\end{align*}
where $\Sigma_0$ and $\mu_0$ are model parameters.

Because everything in sight is gaussian, the above integrals can be evaluated in closed form, as we now proceed to show.

\section{Completing the Square}

\begin{Lem}\label{comp_sq:1}
 Define the quadratic form
  $$
 Q(x;\mu,\Sigma) := (x-\mu)^T\Sigma^{-1}(x-\mu),
  $$
  where $x$ and $\mu$ are $n$-dimensional vectors and $\Sigma$ is a symmetric positive definite $n\times{n}$
  matrix.  Then
  $$
  Q(x;\mu_1,\Sigma_1)+Q(x;\mu_2,\Sigma_2) = Q(x;\mu,\Sigma) + Q(\mu_2;\mu_1,\Sigma_1+\Sigma_2),
  %Q_1(x) + Q_2(x) = (x-\mu)^T\Sigma^{-1}(x-\mu) + (\mu_2-\mu_1)^T(\Sigma_1+\Sigma_2)^{-1}(\mu_2-\mu_1),
  $$
  where
  \begin{align}
    \mu :&= \Sigma_2(\Sigma_1+\Sigma_2)^{-1}\mu_1+\Sigma_1(\Sigma_1+\Sigma_2)^{-1}\mu_2,\quad\text{and}\label{mu}\\
    \Sigma :&= \Sigma_1(\Sigma_1 + \Sigma_2)^{-1}\Sigma_2\label{sigma}.
  \end{align}
\end{Lem}
\begin{proof}

  By expanding the left-hand side, combining like terms, and completing the
  square, it is straightforward to see that
$$
Q(x;\mu_1,\Sigma_1) + Q(x;\mu_2,\Sigma_2) = (x-\mu)^T\Sigma^{-1}(x-\mu) + C,
$$
where
\begin{align}
 \Sigma^{-1} &= \Sigma_1^{-1} + \Sigma_2^{-1},\label{Sigma}\\
  \Sigma^{-1}\mu &=\Sigma_1^{-1}\mu_1+\Sigma_2^{-1}\mu_2, \quad\text{and}\label{mu:1}\\
  C &= \mu_1^T\Sigma_1^{-1}\mu_1 +\mu_2^T\Sigma_2^{-1}\mu_2 - \mu^T\Sigma^{-1}\mu.
  \label{C}
\end{align}
The problem is to verify \eqref{mu} and \eqref{sigma}, and to put $C$ into the form given in the lemma.
 To begin with, we have

\begin{align}
  \Sigma_1\Sigma^{-1}\Sigma_2 &= \Sigma_1(\Sigma_1^{-1}+\Sigma_2^{-1})\Sigma_2 = \Sigma_1+\Sigma_2
  = \Sigma_2\Sigma^{-1}\Sigma_1,\quad\text{whence}\notag\\
  \Sigma_1^{-1}\Sigma\Sigma_2^{-1} &= (\Sigma_1+\Sigma_2)^{-1} = \Sigma_2^{-1}\Sigma\Sigma_1^{-1},\quad\text{and thus}\notag\\
  \Sigma &= \Sigma_1(\Sigma_1 + \Sigma_2)^{-1}\Sigma_2 = \Sigma_2(\Sigma_1+\Sigma_2)^{-1}\Sigma_1,\label{Sigma:1}
\end{align}
proving \eqref{sigma}.  Then \eqref{mu:1} becomes
\begin{equation}\label{mu:2}
  \begin{split}
    \mu &=  \Sigma(\Sigma_1^{-1}\mu_1+\Sigma_2^{-1}\mu_2)\\
    &=\Sigma_2(\Sigma_1+\Sigma_2)^{-1}\mu_1+\Sigma_1(\Sigma_1+\Sigma_2)^{-1}\mu_2,
  \end{split}
\end{equation}
proving \eqref{mu}.

Now using \eqref{Sigma:1}, we can re-write \eqref{mu:1} as  
\begin{equation}\label{mu:3}
  \begin{split}
  \mu &= \Sigma(\Sigma_1^{-1}\mu_1 + \Sigma_2^{-1}\mu_1 +\Sigma_2^{-1}(\mu_2-\mu_1)) \\
  &= \Sigma(\Sigma^{-1}\mu_1 + \Sigma_2^{-1}(\mu_2-\mu_1) \\
  &= \mu_1 + \Sigma_1(\Sigma_1+\Sigma_2)^{-1}(\mu_2-\mu_1),
  \end{split}
  \end{equation}
and by symmetry, we also have 
\begin{equation}\label{mu:4}
  \mu = \mu_2 + \Sigma_2(\Sigma_1+\Sigma_2)^{-1}(\mu_1-\mu_2).
\end{equation}
Then we premultiply \eqref{mu} by $\mu^T$, substituting \eqref{mu:3} into the first term and
  \eqref{mu:4} into the second, as well as using \eqref{Sigma:1}, to get 
\begin{align*}
    \mu^T\Sigma^{-1}\mu &= \mu^T\Sigma_1^{-1}\mu_1 + \mu^T\Sigma_2^{-1}\mu_2 \\
    &= \mu_1^T\Sigma_1^{-1}\mu_1 + (\mu_2-\mu_1)^T(\Sigma_1+\Sigma_2)^{-1}\mu_1
    +\mu_2^T\Sigma_1\mu_2 + (\mu_1-\mu_2)^T(\Sigma_1+\Sigma_2)^{-1}\mu_2\\
    &= \mu_1^T\Sigma_1^{-1}\mu_1 + \mu_2^T\Sigma_2^{-1}\mu_2 - (\mu_2-\mu_1)^T(\Sigma_1+\Sigma_2)^{-1}(\mu_2-\mu_1).
\end{align*}

Finally, substituting this result into \eqref{C} yields
\begin{equation}\label{C:1}
  C = (\mu_2-\mu_1)^T(\Sigma_1+\Sigma_2)^{-1}(\mu_2-\mu_1)
\end{equation}
as required.
\end{proof}

\begin{Cor}
\begin{align}
  \N(x;\mu_1,\Sigma_1)\N(x;\mu_2,\Sigma_2) &= \N(\mu_1;\mu_2,\Sigma_1+\Sigma_2)
  \N(x;\mu,\Sigma) \quad\text{where}\label{comp_sq:2}\\
 \mu :&= \Sigma_2(\Sigma_1+\Sigma_2)^{-1}\mu_1+\Sigma_1(\Sigma_1+\Sigma_2)^{-1}\mu_2, \quad\text{and}\notag\\
 \Sigma :&= \Sigma_1(\Sigma_1 + \Sigma_2)^{-1}\Sigma_2.\notag
\end{align}
In particular,
\begin{equation}\label{int_prod}
  \int_{\R^n}N(x;\mu_1,\Sigma_1)\N(x;\mu_2,\Sigma_2)dx = \N(\mu_1;\mu_2,\Sigma_1+\Sigma_2).
  \end{equation}
\begin{proof}
  The right-hand and left-hand exponents above are equal by \eqref{comp_sq:1}, and it is
  straightforward to verify directly that the normalization factors outside the exponentials are also equal. 
\end{proof}
\end{Cor}


\section{The Forward Pass}
  Now we can inductively evaluate \eqref{alpha:0}. To do so, we split the computation
  into two steps.  In the first step, which we call the {\em time update}, we multiply
  $\alpha_{t-1}(u)$ by the state transition function $Pd(s\mid u)$ and integrate with respect
  to $u$. We will denote the result of the time update by $\hat{\alpha}_t(s)$.  It is the joint probability
  density of observations $x_1,\dots,x_{t-1}$ and state $s$ at time $t$.
  Then in the second step, which we call the {\em measurement update}, we multiply
  $\hat{\alpha}_t(s)$ by $X(x_t \mid s)$, the probability density of observing $x_t$ at time $t$
  in state $s$, to get $\alpha_t(s)$.

  First, we define
\begin{align*}
  P_{a,t} &:= Pd(x_1,x_2,\dots,x_t \mid \theta),\quad\text{for $1\le t\le T$},\quad\text{and}\\
  P_{a,0} &:= 1.
\end{align*}

\begin{Thm}\label{alpha:1}
  For each state $s$ and time $t \ge 1$,
$$
  \alpha_t(s) = P_{a,t}N(s;\mu_{a,t},\Sigma_{a,t}),
$$
where
\begin{align*}
  \mu_{a,t} &= \Sigma_{Ob}(\Sigma_{Ob}+\widehat{\Sigma}_{a,t})^{-1}\mu_{a,t-1}
  + \widehat{\Sigma}_{a,t}(\Sigma_{Ob}+\widehat{\Sigma}_{a,t})^{-1}x_t, \\
  \Sigma_{a,t} &= \Sigma_{Ob}(\Sigma_{Ob}+\widehat{\Sigma}_{a,t})^{-1}\widehat{\Sigma}_{a,t},\\
  \widehat{\Sigma}_{a,t} &= \Sigma_{Tr}+\Sigma_{a,t-1},
 \quad\text{and}\\
  P_{a,t} &=P_{a,t-1}N(x_t;\mu_{a,t-1},\Sigma_{Ob} + \widehat{\Sigma}_{a,t}).
\end{align*}
\end{Thm}

\begin{proof}

Proceeding by induction on $t$, we note that the case $t = 0$ holds by definition.
For the time update, we have, using \eqref{int_prod}
\begin{align}
  \hat{\alpha}_t(s) &= \int_{\R^n}Pd(s|u)\alpha_{t-1}(u)du \\\notag
 &= P_{a,t-1}\int_{\R^n}\N(u;s,\Sigma_{Tr})\N(u;\mu_{a,t-1},\Sigma_{a,t-1})du \notag\\
 &= P_{a,t-1}\N(s;\mu_{a,t-1},\widehat{\Sigma}_{a,t}).
\end{align}

Then multiplying by $X(x_t\mid s)$ yields
\begin{align*}
  \alpha_t(s) &= X(x_t\mid s)\hat{\alpha}_t(s) \\
  &= P_{a,t-1}\N(s;x_t,\Sigma_{Ob})\N(s;\mu_{a,t-1},\widehat{\Sigma}_{a,t}) \\
  &= P_{a,t-1}\N(x_t;\mu_{a,t-1},\Sigma_{Ob} + \widehat{\Sigma}_{a,t})\N(s;\mu_{a,t},\Sigma_{a,t}), \\
  &= P_{a,t}\N(s;\mu_{a,t},\Sigma_{a,t}).
\end{align*}
\end{proof}

\section{The Backward Pass}
This calculation is very similar to the forward pass; the main difference being that we do
the measurement update first by multiplying by $X(x_{t+1}\mid u)$ to obtain $\hat{\beta}_{t+1}(u)$,
and then we integrate with respect to $Pd(u\mid s)du$ for the time update.  

\begin{Thm}\label{beta:1}
  Let 
\begin{align*}
  \widehat{\Sigma}_{b,t} :&= \Sigma_{Ob}+\Sigma_{b,t},\quad\text{and}\\
    \widetilde{\Sigma}_{b,t} :&= \Sigma_{b,t}\widehat{\Sigma}_{b,t}^{-1}\Sigma_{Ob}.
\end{align*}

  Then for each state $s$ and time $t < T$,
$$
  \beta_t(s) = P_{b,t}N(s;\mu_{b,t},\Sigma_{b,t}),
$$
where
\begin{align*}
  \mu_{b,t} :&= \Sigma_{b,t+1}\widehat{\Sigma}_{b,t+1}^{-1}x_{t+1} + \Sigma_{Ob}\widehat{\Sigma}_{b,t+1}^{-1}\mu_{b,t+1},\\
  \Sigma_{b,t} :&= \widetilde{\Sigma}_{b,t+1} + \Sigma_{Tr},\quad\text{and}\\
  P_{b,t}:&=P_{b,t+1}\N(x_{t+1};\mu_{b,t+1},\widehat{\Sigma}_{b,t+1}),\\
  P_{b,T} :&= 1.
\end{align*}
\end{Thm}

\begin{proof}
  We proceed by reverse induction on $t$. However, to get started we first must  deal with
  the special case $\beta_{T}(u) = 1$.  Thus,
  \begin{align*}
    \hat{\beta}_T(u) &= \N(u;x_T,\Sigma_{Ob}),\quad\text{and}\\
    \beta_{T-1}(s) &= \int_{\R^n}\N(u;x_T,\Sigma_{Ob})\N(u;s,\Sigma_{Tr})du \\
    &= \N(s;x_T,\Sigma_{Ob}+\Sigma_{Tr})
  \end{align*}

  Thus, our initial values at $t = T-1$ are
  \begin{align*}
  P_{b,t} &= 1,\\
  \mu_{b,t} &= x_T, \quad\text{and}\\
  \Sigma_{b,t} &= \Sigma_{Ob}+\Sigma_{Tr}.
  \end{align*}

  For $t <  T-1$, we have 
  \begin{align*}
    \hat{\beta}_{t+1}(u) &= X(x_{t+1}\mid u)\beta_{t+1}(u)\\
    &= P_{b,t+1}\N(u;x_{t+1},\Sigma_{Ob})\N(u;\mu_{b,t+1},\Sigma_{b,t+1})\\
    &= P_{b,t+1}\N(x_{t+1};\mu_{b,t+1},\widehat{\Sigma}_{b,t+1})\N(u;\mu_{b,t},\widetilde{\Sigma}_{b,t+1}).
  \end{align*}

  Then a second application of \eqref{comp_sq:2} yields
  \begin{align*}
    \beta_t(s) &= \int_{\R^n}\hat{\beta}_{t+1}(u)Pd(u\mid s)du \\
    &= P_{b,t+1}\N(x_{t+1};\mu_{b,t+1},\widehat{\Sigma}_{b,t+1})\int_{\R^n}\N(u;\mu_{b,t},\widetilde{\Sigma}_{b,t+1})\N(u;s,\Sigma_{Tr})du\\
    &=P_{b,t+1}\N(x_{t+1};\mu_{b,t+1},\widehat{\Sigma}_{b,t+1})\N(s;\mu_{b,t},\Sigma_{b,t}),\quad\text{whence}\\
    P_{b,t} &= P_{b,t+1}\N(x_{t+1};\mu_{b,t+1},\widehat{\Sigma}_{b,t+1}).
  \end{align*}
\end{proof}

\section{The Posterior Likelihoods}
Recall that $\gamma_t(s)$ is the posterior probability density of state $s$
at time $t$. 
\begin{Thm}
  Let notation be as in \eqref{alpha:1} and \eqref{beta:1}.  Define
\begin{align*}
  \widehat{\Sigma}_{c,t} :&= \Sigma_{a,t}+\Sigma_{b,t}, \quad\text{and}\\
  P_{c,t} :&= \int_{\R^n}\alpha_t(s)\beta_t(s)ds,\quad\text{then} \\
      \gamma_t(s) &= \N(s;\mu_{c,t},\Sigma_{c,t}),\quad\text{where}\\
      \Sigma_{c,t} &:= \Sigma_{a,t}\widehat{\Sigma}_{c,t}^{-1}\Sigma_{b,t}
      \quad\text{and}\\
      \mu_{c,t} &:= \Sigma_{a,t}\widehat{\Sigma}_{c_t}^{-1}\mu_{b,t} +
      \Sigma_{b,t}\widehat{\Sigma}_{c_t}^{-1}\mu_{a,t}.
  \end{align*}
\end{Thm}
\begin{proof}
  From \eqref{alpha:1} and \eqref{beta:1} we have (using \eqref{comp_sq:2}
  as usual)
  \begin{align*}
  \alpha_t(s)\beta_t(s) &= P_{a,t}P_{b,t}\N(s;\mu_{a,t},\Sigma_{a,t})
  \N(s;\mu_{b,t},\Sigma_{b,t})ds\\
  &= P_{a,t}P_{b,t}\N(\mu_{a,t};\mu_{b,t},\Sigma_{a,t}+\Sigma_{b,t})\N(s;\mu_{c,t},
  \Sigma_{c,t}), \quad\text{whence}\\
  P_{c,t} &= P_{a,t}P_{b,t}\N(\mu_{a,t};\mu_{b,t},\Sigma_{a,t}+\Sigma_{b,t})
  \end{align*}
by \eqref{int_prod}, and the theorem follows.
\end{proof}

We also need the joint posterior probability density of state $u$ at time $t-1$ and state $s$ at time $t$,
which we denote by $\hat{\gamma}_t(s,u)$. The joint probability density of all the data and the two given states
  $u,s$  at times $t-1,t$ is
  \begin{align}
    \hat{\gamma}_t(s,u) &\propto \alpha_{t-1}(u)Pd(s\mid u)X(x_t\mid s)\beta_t(s) \notag\\
    &= \N(u;\mu_{a,t-1},\Sigma_{a,t-1})\N(u;s,\Sigma_{Tr})  \N(s;x_t,\Sigma_{Ob})\N(s;\mu_{b,t},\Sigma_{b,t})\notag\\
    &\propto \N(u;\mu^{(1)}_t,\Sigma^{(1)}_t)\N(s;\mu_{a,t-1},\widehat{\Sigma}_{a,t})\N(s;\mu_{b,t-1},
    \widetilde{\Sigma}_{b,t})\notag\\
    &\propto \N(u;\mu^{(1)}_t,\Sigma^{(1)}_t)\N(s;\mu^{(2)}_t,\Sigma^{(2)}_t),\label{gamma_hat}
  \end{align}
  where we have discarded factors which do not depend on either $u$ or $s$, and 
  \begin{align}
    \Sigma^{(1)}_t &:= \Sigma_{a,t-1}\widehat{\Sigma}_{a,t}^{-1}\Sigma_{Tr},\notag\\
    \mu^{(1)}_t &:= \Sigma_{a,t-1}\widehat{\Sigma}_{a,t}^{-1}s + \Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}\mu_{a,t-1},\label{mu(1)}\\
    \Sigma^{(2)}_t &:= \widehat{\Sigma}_{a,t}(\widehat{\Sigma}_{a,t}+\widetilde{\Sigma}_{b,t})^{-1}\widetilde{\Sigma}_{b,t}
    = \widehat{\Sigma}_{a,t}\widehat{\Sigma}_{c,t}^{-1}\widetilde{\Sigma}_{b,t},\notag\quad\text{and}\\
    \mu^{(2)}_t &:= \widehat{\Sigma}_{a,t}\widehat{\Sigma}_{c,t}^{-1}\mu_{b,t-1} +
    \widetilde{\Sigma}_{b,t}\widehat{\Sigma}_{c,t}^{-1}\mu_{a,t-1}\label{mu(2)}.
  \end{align}

  Note that the second factor in \eqref{gamma_hat} does not depend on $u$, and the first factor depends on $s$
  only in the mean, which means that the integral of the first factor with respect to $u$ is always unity
  and does not depend on $s$, and thus the double integral of \eqref{gamma_hat} is still unity and no
  normalization factor is required.
  
\section{Re-estimation}
Given a state sequence $S := \{s_0,s_1,s_2,\dots,s_T\}$ and the data sequence $X := \{x_1,x_2,\dots,x_T\}$, the
joint probability density of $S$ and $X$ given parameters $\theta := \{\mu_0,\Sigma_0,\Sigma_{Tr},\Sigma_{Ob}\}$ is
$$
P(X,S\mid\theta) = \N(s_0;\mu_0,\Sigma_0)\prod_{t=1}^T\N(x_t;s_t,\Sigma_{Ob})\N(s_t;s_{t-1},\Sigma_{Tr}).
$$

Note that $P(X,S\mid\theta)$ can also be viewed as  $L(\theta\mid X,S)$, the posterior likelihood of $\theta$.
To re-estimate $\theta$, we try to maximize $L(\theta)$ using the EM algorithm. Namely, we define 
$$
E(\theta,\bar{\theta}) := \int_{\R^{Tn}}P(X,S\mid\theta)\log{L(\bar{\theta}\mid X,S)}dS,
$$
where $\theta$ is the current set of parameters, and $\bar{\theta}$ is the unknown set of new parameters
we wish to determine. So instead of maximizing the log-likelihood directly, we can maximize its expected
value with respect to the current posterior distribution, because it is a standard result (and easy to prove) that
$$
L(\bar{\theta}) - L(\theta) \ge E(\theta,\bar{\theta}) - E(\theta,\theta),
$$
so choosing $\bar{\theta}$ to maximize $E(\theta,\bar{\theta})$ will increase the value of $L(\theta)$.


  
Recall the notation $Q(x;\mu,\Sigma)$ from \eqref{comp_sq:1}.  Then we have
\begin{align}
  E(\theta,\bar{\theta}) &= -(2T+1)\frac{n\log(2\pi)}{2} + \frac{1}{2}(E_0 + E_1 + E_2),\quad\text{where}\notag\\
  E_0 :&=\int_{\R^{Tn}}P(X,S\mid\theta)\{\log\det\overline{\Sigma}^{-1}_0 - Q(s_0;\bar{\mu}_0,\overline{\Sigma}_0^{-1})\}dS,\label{E0}\\
  E_1 :&= \int_{\R^{Tn}}P(X,S\mid\theta)\left\{\sum_{t=1}^T\log\det\overline{\Sigma}^{-1}_{Ob} - Q(x_t;s_t,\overline{\Sigma}_{Ob}^{-1})\right\}dS,
\label{E1}\\
  E_2 :&= \int_{\R^{Tn}}P(X,S\mid\theta)\left\{\sum_{t=1}^T\log\det\overline{\Sigma}^{-1}_{Tr} - Q(s_t;s_{t-1},\overline{\Sigma}_{Tr}^{-1})\right\}dS,
\label{E2}
\end{align}
Since the expected value of a constant is just that constant, $-n\log(2\pi)/2$ can be moved outside each
integral sign and ignored in the optimization.  And since none of the parameters of $\theta$ appear in more
than one of the $E_i$, we can optimize $E$ by optimizing each of $E_1,E_2,E_3$ separately.

We begin with $E_0$. Since the only component of $S$ in the integrand of \eqref{E0} is $s_0$, the expected value
collapses to the marginal 
expected value of the integrand with respect to the marginalization of $P(X,S\mid\theta)$ at $s_0$, which is just 
$\gamma_0(s_0)$.  Furthermore, after pulling the constant and the summation 
outside the integral in \eqref{E1} we have the same collapse to the marginalization at $s_t$ of the posterior, which
is just $\gamma_t(s_t)$.  It follows that
\begin{align*}
E_0 &= \log\det(\overline{\Sigma}^{-1}_0) - \int_{\R^n}\gamma_0(s)Q(s;\bar{\mu}_0,\overline{\Sigma}_0)ds,\quad\text{and}\\
E_1 &= T\log\det(\overline{\Sigma}^{-1}_{Ob}) - \sum_{t=1}^T\int_{\R^n}\gamma_t(s)Q(x_t;s,\overline{\Sigma}_{Ob})ds.
\end{align*}

The situation for \eqref{E2} is slightly different, because after again moving the constant and the summation outside
the integral, the integrand depends on both $s_t$ and $s_{t-1}$.  So in this case, the integral collapses to the 
joint marginal expectation over $s_t$ and $s_{t-1}$, and we get
$$
E_2 = T\log\det(\overline{\Sigma}^{-1}_{Tr}) - \sum_{t=1}^T\int_{\R^{2n}}\hat{\gamma}_t(s,u)Q(s;u,\overline{\Sigma}_{Tr})duds,
$$
where $\hat{\gamma}_t(s,u)$ is the joint posterior probability density of state $u$ at time $t-1$ and state $s$ 
at time $t$ given by \eqref{gamma_hat}.

If $f$ is any scalar-valued function of an $m\times{n}$ matrix $A = a_{ij}$, we denote by $\partial{f}/\partial{A}$ the 
$m\times{n}$ matrix whose $(i,j)$-entry is $\partial{f}/\partial{a_{ij}}$.  This also applies to column vectors (when $n=1$).
In particular, if $m=n$, $A$ is symmetric, and $f(A) = \det(A)$,  then the usual expansion 
in minors along the $i^{th}$ row shows that 
$$
\frac{\partial{\det(A)}}{\partial{A}} = (2-\delta_{ij})A^* = (2-\delta_{ij})\det(A)A^{-1},\quad\text{and}
\quad\frac{\partial{\log\det(A)}}{\partial{A}} = (2-\delta_{ij})A^{-1}. 
$$
Here $A^*$ is the adjoint matrix and the notation $(2-\delta_{ij})A^*$ means to multiply all off-diagonal elements of $A^*$ by 2
and leave the diagonal unchanged.  This is of course necessary due the the symmetry $A = A^T$.

In addition, since $Q(x;\mu,\Sigma)$ is linear in the coefficients of $\Sigma$, we see that
$$
\frac{\partial{Q(x;\mu,\Sigma)}}{\partial{\Sigma}} = (2-\delta_{ij})(x-\mu)(x-\mu)^T,
$$
and it is also easy to verify that
$$
\frac{\partial{Q(x;\mu,\Sigma)}}{\partial{\mu}} = 2\Sigma(\mu-x).
$$

Armed with these formulas, we first minimize $E_0$ with respect to $\bar{\mu}_0$: 
 
\begin{align}
0 = \frac{\partial{E_0}}{\partial{\bar{\mu}_0}} &= \int_{\R^n}\gamma_0(s)\frac{\partial{Q(s;\bar{\mu}_0,\overline{\Sigma}_0)}}{\partial{\bar{\mu}_0}}ds \notag\\
&= \int_{\R^n}\gamma_0(s)(2\overline{\Sigma}_0^{-1}(\bar{\mu}_0-s))ds\notag\\
&= 2\overline{\Sigma}_0^{-1}\int_{\R^n}\gamma_0(s)(\bar{\mu}_0-s)ds \notag\\
&= 2\overline{\Sigma}_0^{-1}(\bar{\mu}_0-\mu_{c,0}), \quad\text{hence}\notag\\
\bar{\mu}_0 &= \mu_{c,0}\label{mu_0}
\end{align}

Next, we maximize $E_0$ with respect to $\overline{\Sigma}_0^{-1}$, and we can set $\bar{\mu}_0 = \mu_{c,0}$ by \eqref{mu_0}:
\begin{align*}
0 = \frac{\partial{E_0}}{\partial{\overline{\Sigma}_0^{-1}}} &= \frac{\partial{\log\det(\overline{\Sigma}_0^{-1})}}{\partial\overline{\Sigma}_0^{-1}} - \int_{\R^n}\gamma_0(s)\frac{\partial{Q(s;\mu_{c,0},\overline{\Sigma}_0^{-1})}}{\partial{\overline{\Sigma}_0^{-1}}}ds \\
&= (2-\delta_{ij})\overline{\Sigma}_0 - (2-\delta_{ij})\int_{\R^n}\gamma_0(s)(s-\mu_{c,0})(s-\mu_{c,0})^Tds.
\end{align*}
Not surprisingly, it follows that
\begin{equation}\label{Sigma_0}
  \overline{\Sigma}_0 = \int_{\R^n}\gamma_0(s)(s-\mu_{c,0})(s-\mu_{c,0})^Tds = \Sigma_{c,o}.
  \end{equation}

To minimize $E_1$, we differentiate with respect to $\overline{\Sigma}_{ob}^{-1}$:
\begin{align*}
  0 = \frac{\partial{E_1}}{\partial{\overline{\Sigma}_{Ob}^{-1}}} &= T\frac{\partial{\log\det(\overline{\Sigma}_{Ob}^{-1})}}{\partial\overline{\Sigma}_{Ob}^{-1}}
  - \sum_{t=1}^T\int_{\R^n}\gamma_t(s)\frac{\partial{Q(s;x_t,\overline{\Sigma}_{Ob}^{-1})}}{\partial{\overline{\Sigma}_{Ob}^{-1}}}ds \\
  &= (2-\delta{ij})T\overline{\Sigma}_{Ob} - (2-\delta_{ij})\sum_{t=1}^T\int_{\R^n}\gamma_t(s)(s-x_t)(s-x_t)^Tds.
\end{align*}
Thus,  we see that
\begin{equation}\label{Sigma_Ob}
  \overline{\Sigma}_{Ob} = \frac{1}{T}\sum_{t=1}^T [\Sigma_{c,t} + (\mu_{c,t}-x_t)(\mu_{c,t}-x_t)^T].
\end{equation}

Finally, we minimize $E_2$ by differentiating with respect to $\overline{\Sigma}_{Tr}^{-1}$:
\begin{equation}\label{Sigma_Tr:0}
  \begin{split}
  0 &= \frac{\partial{E_2}}{\partial{\overline{\Sigma}_{Tr}^{-1}}} = \frac{\partial{\log\det(\overline{\Sigma}_{Tr}^{-1})}}{\partial\overline{\Sigma}_{Tr}^{-1}}
  - \sum_{t=1}^T\int_{\R^{2n}}\hat{\gamma}_t(s,u)\frac{\partial{Q(s;u,\overline{\Sigma}_{Tr}^{-1})}}{\partial\overline{\Sigma}_{Tr}^{-1}}duds \\
      &= (2-\delta_{ij})T\overline{\Sigma}_{Tr} - (2-\delta{ij}) \sum_{t=1}^T\int_{\R^{2n}}\N(u;\mu^{(1)}_t,\Sigma^{(1)}_t)\N(s;\mu^{(2)}_t\Sigma^{(2)}_t)(u-s)(u-s)^Tduds.
  \end{split}
\end{equation}

As noted earlier, $\N(s,\mu^{(2)}_t,\Sigma^{(2)}_t)$ does not depend on $u$, so we can integrate first with respect to $u$, with a result
analagous to \eqref{Sigma_Ob}:
$$
  \int_{-\infty}^{\infty}\N(u;\mu^{(1)}_t,\Sigma^{(1)}_t)(u-s)(u-s)^Tdu = \Sigma^{(1)}_t + (s-\mu^{(1)}_t)(s-\mu^{(1)}_t)^T.
$$
Substituting this result into \eqref{Sigma_Tr:0}, we get
\begin{equation}\label{Sigma_Tr:1}
  \overline{\Sigma}_{Tr} =  \frac{1}{T}\left(\sum_{t=1}^T\Sigma^{(1)}_t +\int_{-\infty}^{\infty}\N(s;\mu^{(2)}_t,\Sigma^{(2)}_t)
  (s-\mu^{(1)}_t)(s-\mu^{(1)}_t)^Tds\right).
\end{equation}

However, there is a problem evaluating this integral because after checking \eqref{mu(1)}, we see that $\mu^{(1)}_t$
depends on $s$, namely  
\begin{equation}\label{s-mu_1}
    \begin{split}
s - \mu^{(1)}_t &= (I - \Sigma_{a,t-1}\widehat{\Sigma}_{a,t}^{-1})s - \Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}\mu_{a,t-1}, \\
  &= (\widehat{\Sigma}_{a,t}\widehat{\Sigma}_{a,t}^{-1} - \Sigma_{a,t-1}\widehat{\Sigma}_{a,t}^{-1})s  - 
  \Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}\mu_{a,t-1},\\
  &= (\widehat{\Sigma}_{a,t} - \Sigma_{a,t-1})\widehat{\Sigma}_{a,t}^{-1}s - \Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}\mu_{a,t-1},\\
  &= \Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}s - \Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}\mu_{a,t-1},\\
  &= \Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}(s - \mu_{a,t-1}).
    \end{split}
\end{equation}


Fortunately, neither $\mu^{(2)}_t,\Sigma^{(2)}_t,$ nor $\Sigma^{(1)}_t$ depends on $s$,
so \eqref{Sigma_Tr:1} becomes
\begin{align}
  \overline{\Sigma}_{Tr} &= \frac{1}{T}\sum_{t=1}^T\Sigma^{(1)}_t + \frac{1}{T}\sum_{t=1}^T
    \int_{-\infty}^{\infty}\N(s;\mu^{(2)}_t,\Sigma^{(2)}_t)\Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}(s-\mu_{a,t-1})
    s-\mu_{a,t-1})^T\widehat{\Sigma}_{a,t}^{-1}\Sigma_{Tr}ds\notag\\
    &=\frac{1}{T}\sum_{t=1}^T\Sigma^{(1)}_t +\frac{1}{T}\sum_{t=1}^T\Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}
    \left[\int_{-\infty}^{\infty}\N(s;\mu^{(2)}_t,\Sigma^{(2)}_t)(s-\mu_{a,t-1})(s-\mu_{a,t-1})^Tds\right]\widehat{\Sigma}_{a,t}^{-1}\Sigma_{Tr}\notag\\
   &=\frac{1}{T}\sum_{t=1}^T\Sigma^{(1)}_t+\Sigma_{Tr}\left(\frac{1}{T}\sum_{t=1}^T\widehat{\Sigma}_{a,t}^{-1}
   [\Sigma^{(2)}_t+(\mu^{(2)}_t-\mu_{a,t-1})(\mu^{(2)}_t-\mu_{a,t-1})^T]\widehat{\Sigma}_{a,t}^{-1}\right)\Sigma_{Tr}.\label{Sigma_Tr:2}
  \end{align}

Finally, we can make the following simplification by using \eqref{mu(2)}.
\begin{align*}
  \mu^{(2)} - \mu_{a,t-1} &= \widehat{\Sigma}_{a,t}\widehat{\Sigma}_{c,t}^{-1}\mu_{b,t-1} +
  (\widetilde{\Sigma}_{b,t-1}\widehat{\Sigma}_{c,t}^{-1}-I)\mu_{a,t-1}\\
  &= \widehat{\Sigma}_{a,t}\widehat{\Sigma}_{c,t}^{-1}\mu_{b,t-1} +
  (\widetilde{\Sigma}_{b,t-1}\widehat{\Sigma}_{c,t}^{-1}-\widehat{\Sigma}_{c,t}\widehat{\Sigma}_{c,t}^{-1})\mu_{a,t-1}\\
  &=  \widehat{\Sigma}_{a,t}\widehat{\Sigma}_{c,t}^{-1}\mu_{b,t-1} +
  (\widetilde{\Sigma}_{b,t-1}-\widehat{\Sigma}_{c,t})\widehat{\Sigma}_{c,t}^{-1}\mu_{a,t-1}\\
  &= \widehat{\Sigma}_{a,t}\widehat{\Sigma}_{c,t}^{-1}(\mu_{b,t-1} -\mu_{a,t-1}),
\end{align*}
and substituting this into \eqref{Sigma_Tr:2} we have
\begin{equation}\label{Sigma_Tr:3}
\overline{\Sigma}_{Tr}   =\frac{1}{T}\sum_{t=1}^T\Sigma^{(1)}_t+\Sigma_{Tr}\left(\frac{1}{T}\sum_{t=1}^T\widehat{\Sigma}_{c,t}^{-1}
   [\Sigma^{(2)}_t+(\mu_{b,t-1}-\mu_{a,t-1})(\mu_{b,t-1}-\mu_{a,t-1})^T]\widehat{\Sigma}_{c,t}^{-1}\right)\Sigma_{Tr}.
\end{equation}  
\end{document}





so we can make the affine change of variable 
$$
v := \Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}(s - \mu_{a,t-1}),\quad dv = \det(\Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1})ds
$$
in \eqref{Sigma_Tr:1}, which yields
\begin{equation} \label{Sigma_tr:2}
  \begin{split}
  \overline{\Sigma}_{Tr} &= \frac{1}{T}\sum_{t=1}^T \left(\Sigma^{(1)}_t +\det(\widehat{\Sigma}_{a,t}\Sigma_{Tr}^{-1})
  \int_{-\infty}^{\infty}\N(v;\mu^{(3)}_t,\Sigma_t^{(3)})vv^T dv\right)\\
  &=  \frac{1}{T}\sum_{t=1}^T \left(\Sigma^{(1)}_t +\det(\widehat{\Sigma}_{a,t}\Sigma_{Tr}^{-1}) (\Sigma^{(3)}_t
  + \mu^{(3)}_t(\mu^{(3)}_t)^T)\right).
  \end{split}
\end{equation}
where
\begin{align*}
\Sigma_t^{(3)} :&= \Sigma_{Tr}\widehat{\Sigma}^{-1}_{a,t}\Sigma^{(2)}_t\widehat{\Sigma}_{a,t}^{-1}\Sigma_{Tr},\quad\text{and}\\
\mu_t^{(3)} :&= \Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}(\mu_t^{(2)} - \mu_{a,t-1}).
\end{align*}

However, by a calculation entirely analagous to \eqref{s-mu_1}, we find that
$$
\Sigma_{tr}\widehat{\Sigma}_{a,t}^{-1}(\mu_t^{(2)}-\mu_{a,t-1})=
(\widehat{\Sigma}_{a,t}+\widetilde{\Sigma}_{b,t})^{-1}(\mu_{b,t-1}-\mu_{a,t-1}),
$$
which yields
$$
\mu_t^{(3)} = \Sigma_{Tr}(\widehat{\Sigma}_{a,t}+\widetilde{\Sigma}_{b,t})^{-1}(\mu_{b,t-1}-\mu_{a,t-1}).
$$


\eqref{Sigma_Tr:1} becomes
\begin{align}
  \overline{\Sigma}_{Tr} &= \frac{1}{T}\sum_{t=1}^T\Sigma^{(1)}_t + \frac{1}{T}\sum_{t=1}^T
    \int_{-\infty}^{\infty}\N(s;\mu^{(2)}_t,\Sigma^{(2)}_t)\Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}(s-\mu_{a,t-1})
    s-\mu_{a,t-1})^T\widehat{\Sigma}_{a,t}^{-1}\Sigma_{Tr}ds\notag\\
    &=\frac{1}{T}\sum_{t=1}^T\Sigma^{(1)}_t +\frac{1}{T}\sum_{t=1}^T\Sigma_{Tr}\widehat{\Sigma}_{a,t}^{-1}
    \left[\int_{-\infty}^{\infty}\N(s;\mu^{(2)}_t,\Sigma^{(2)}_t)(s-\mu_{a,t-1})(s-\mu_{a,t-1})^Tds\right]\widehat{\Sigma}_{a,t}^{-1}\Sigma_{Tr}\notag\\
   &=\frac{1}{T}\sum_{t=1}^T\Sigma^{(1)}_t+\Sigma_{Tr}\left(\frac{1}{T}\sum_{t=1}^T\widehat{\Sigma}_{a,t}^{-1}
   [\Sigma^{(2)}_t+(\mu^{(2)}_t-\mu_{a,t-1})(\mu^{(2)}_t-\mu_{a,t-1})^T]\widehat{\Sigma}_{a,t}^{-1}\right)\Sigma_{Tr}.\label{Sigma_Tr:2}
  \end{align}

Finally, we can make the following simplification by using \eqref{mu(2)} and defining
$$
\widehat{\Sigma}_{c,t} = \widehat{\Sigma}_{a,t} + \widetilde{\Sigma}_{b,t}.
$$
Then
\begin{align*}
  \mu^{(2)} - \mu_{a,t-1} &= \widehat{\Sigma}_{a,t}\widehat{\Sigma}_{c,t}^{-1}\mu_{b,t-1} +
  (\widetilde{\Sigma}_{b,t-1}\widehat{\Sigma}_{c,t}^{-1}-I)\mu_{a,t-1}\\
  &= \widehat{\Sigma}_{a,t}\widehat{\Sigma}_{c,t}^{-1}\mu_{b,t-1} +
  (\widetilde{\Sigma}_{b,t-1}\widehat{\Sigma}_{c,t}^{-1}-\widehat{\Sigma}_{c,t}\widehat{\Sigma}_{c,t}^{-1})\mu_{a,t-1}\\
  &=  \widehat{\Sigma}_{a,t}\widehat{\Sigma}_{c,t}^{-1}\mu_{b,t-1} +
  (\widetilde{\Sigma}_{b,t-1}-\widehat{\Sigma}_{c,t})\widehat{\Sigma}_{c,t}^{-1}\mu_{a,t-1}\\
  &= \widehat{\Sigma}_{a,t}\widehat{\Sigma}_{c,t}^{-1}(\mu_{b,t-1} -\mu_{a,t-1}),
\end{align*}
and substituting this into \eqref{Sigma_Tr:2} we have
\begin{equation}\label{Sigma_Tr:3}
\overline{\Sigma}_{Tr}   =\frac{1}{T}\sum_{t=1}^T\Sigma^{(1)}_t+\Sigma_{Tr}\left(\frac{1}{T}\sum_{t=1}^T\widehat{\Sigma}_{c,t}^{-1}
   [\Sigma^{(2)}_t+(\mu_{b,t-1}-\mu_{a,t-1})(\mu_{b,t-1}-\mu_{a,t-1})^T]\widehat{\Sigma}_{c,t}^{-1}\right)\Sigma_{Tr}.
\end{equation}  



By definition, $P_{c,t}$ is the posterior probability density of all the data and is therefore independent of $t$.
However, as a useful check on our calculations, we can derive this directly:
\begin{align*}
  \frac{P_{c,t}}{P_{c,t+1}} &= \frac{P_{a,t}P_{b,t}\N(\mu_{a,t};\mu_{b,t},\Sigma_{a,t}+\Sigma_{b,t}}
  {P_{a,t+1}P_{b,t+1}\N(\mu_{a,t+1};\mu_{b,t+1},\Sigma_{a,t+1}+\Sigma_{b,t+1})}\\
  &=\frac{P_{a,t}P_{b,t+1}\N(x_{t+1};\mu_{t+1},\widehat{\Sigma}_{b,t+1})\N(\mu_{a,t};\mu_{b,t},\Sigma_{a,t}+\Sigma_{b,t})}
  {P_{a,t}P_{b,t+1}\N(x_{t-1};\mu_{a,t}\Sigma_{Ob}+\widehat{\Sigma}_{a,t})\N(\mu_{a,t+1};\mu_{b,t+1},\Sigma_{a,t+1}+\Sigma_{b,t+1})}\\
\end{align*}

