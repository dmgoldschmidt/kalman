\documentclass[12pt,leqno]{article}
\include{article_defs}
\title{A Continuous State Hidden Markov Model}
\author{David M. Goldschmidt}
%\oddsidemargin 10 pt \evensidemargin 10 pt \marginparwidth 0.75 in \textwidth
%6.0 true in \topmargin -40 pt \textheight 8.8 true in 
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lfoot{}
%\rfoot{\thepage}
\begin{document}
%\renewcommand{\footrulewidth}{0.4pt}
\newcommand{\p}{\ensuremath{u}}
\newcommand{\VV}{V}
\maketitle


\section{Introduction}
Let $\mu$ and $x$ be vectors in $\R^n$ and let $\S$ be an $n\times{n}$ positive
definite real symmetric matrix.  Recall that the gaussian probability density
on $\R^n$ is given by 
$$
\N(x;\mu,S) := (2\pi)^{-\frac{n}{2}}|S|^{\frac{1}{2}}\exp\left\{-\frac{1}{2}
(x - \mu)^TS(x-\mu)\right\}.
$$
So here and below, capital $S$ denotes an inverse covariance matrix.

We now define a Hidden Markov Model with state space $\R^n$ as follows.  Given the state $s\in\R^n$ and an observation $x_t\in\R^m$ at time $t$, the output density is
$$
X(x_t\mid s) := \N(x_t;M_ts + b_t,S_{Ob}),
$$
where $M_t$ is a linear map from the state space to the
measurement space, $b_t$ is a bias, and $S_{Ob}$ is an $m\times{m}$ positive definite real symmetric matrix.  In this paper, $S_{Ob}$ and 
$M$ are time-independent model parameters which we usually seek to re-estimate from the data.  The bias $b_t$ can usually be 
subtracted from the data so we assume here that $b_t = 0$ for all $t$. 
In the simplest case, $M_t = I$ for all $t$ so that $m = n$ and $s$ is itself the mean of the output distribution.  
The state process is a discrete time continuous state Markov process with transition probability density
from state $s_0$ at time $t$ to state $s_1$ at time $t+1$ given by
$$
Pd(s_1\mid s_0) :=  \N(s_1;s_0+c_t,S_{Tr}),
$$
where $S_{Tr}$ is a time-independent model parameter 
and $c_t$ is a known ``control input'' which we also assume is zero for the time being.

As usual, we are given observations $\{x_t\mid 1\le t\le T\}$ and model parameters $\theta$.  For each observation time $t$ and state $s$, we define
\begin{align*}
  \alpha_t(s) :&= Pd(x_1,x_2,\dots,x_t~ \& \text{state $s$ at time $t$}\mid \theta ),\\
  \beta_t(s) :&= Pd(x_{t+1},\dots,x_T \mid ~\text{state $s$ at time $t$},\theta)\\
  \gamma_t(s) :&= \frac{\alpha_t(s)\beta_t(s)}{\int_{\R^n}\alpha_t(s)\beta_t(s)ds}.
\end{align*}

Thus, $\alpha_t(s)$ is the joint density of state $s$ at time $t$ and the observations up to (and including) time $t$,
$\beta_t(s)$ is the {\em conditional} density of the future observations given state $s$ at time $t$, and $\gamma_t(s)$
is the posterior probability density of state $s$ at time $t$.

It is clear from the definitions that the following recursions are satisfied:
\begin{align}
\alpha_t(s) &= X(x_t \mid s)\int_{\R^n}\alpha_{t-1}(u)Pd(s \mid u)du,\quad\text{and}\label{alpha:0}\\
\beta_t(s) &= \int_{\R^{n}}Pd(u \mid s)X(x_{t+1} \mid u)\beta_{t+1}(u)du.\label{beta:0}
\end{align}

We initialize these recursions with
\begin{align*}
\alpha_0(s) :&= \N(s;\mu_0,S_0) \\
\beta_{T}(s) :&= 1 \quad\text{for all $s$},
\end{align*}
where $S_0$ and $\mu_0$ are model parameters.

Because everything in sight is gaussian, the above integrals can be evaluated in closed form
as follows.
\section{Completing the Square} 
\begin{Lem}
 Define the quadratic form
  $$
 Q(x;\mu,S) := (x-\mu)^TS(x-\mu),
  $$
  where $x,\mu\in \R^n$ and  $S$ is a symmetric positive definite $n\times{n}$
  matrix. Let $M$ be an $m\times{n}$ matrix, $\mu_1\in\R^n$, $\mu_2\in\R^m$,
  and $S_1$,$S_2$ be symmetric positive definite.  Then 
\begin{equation}\label{comp_sq:1}
  Q(x;\mu_1,S_1)+Q(Mx;\mu_2,S_2) = Q(x;\mu,S) + R(\mu_1,\mu_2,\Sigma_1,\Sigma_2),
\end{equation}
where 
\begin{align}
S &= S_1 + M^TS_2M, \label{sigma}\\
\mu &= S^{-1}(S_1\mu_1 + M^TS_2\mu_2),\quad\text{and}\label{mu}\\
R &= \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2 - \mu^TS\mu. \label{R_def} 
\end{align}
Furthermore, if $M = I$, then $S_1S^{-1}S_2 = S_2S^{-1}S_1$,  and
\begin{equation}\label{comp_sq:R}
R = (\mu_2-\mu_1)^TS_1S^{-1}S_2(\mu_2-\mu_1).
\end{equation}
\end{Lem}
\begin{proof}
  Note that $S_1$ is postive definite and $M^TS_2M$ is positive semi-definite,
so $S$ is invertible.  Expanding the left-hand side of \eqref{comp_sq:1} and combining
like terms, we get
$$
x^T(S_1+ M^TS_2M)x -2x^T(S_1\mu_1+ M^TS_2\mu_2) 
+ \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2.
$$
Completing the square, we get
$$
(x-\mu)^TS(x-\mu) -\mu^TS\mu + \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2,
$$
so 
\begin{equation}\label{R}
R = \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2-\mu^TS\mu 
\end{equation}
as asserted. Now assume $M = I$. We first note that
\begin{align}
S_1^{-1}SS_2^{-1} &= S_1^{-1}(S_1+S_2)S_2^{-1} = S_1^{-1}+S_2^{-1} = S_2^{-1}SS_1^{-1},\quad\text{hence}\notag\\
S_2S^{-1}S_1 &= (S_1^{-1}+S_2^{-1})^{-1} = S_1S^{-1}S_2.\label{symmetry}
\end{align}
Moreover, we can re-write \eqref{mu} as
\begin{equation}\label{mu:1}
\mu = S^{-1}(S_1\mu_1 + S_2\mu_1 + S_2(\mu_2-\mu_1) = \mu_1 + S^{-1}S_2(\mu_2-\mu_1),
\end{equation}
and by symmetry
\begin{equation}\label{mu:2}
\mu = \mu_2 + S^{-1}S_1(\mu_1-\mu_2).
\end{equation}
Then from \eqref{mu} we have
$$
\mu^TS\mu = \mu^T(S_1\mu_1 + S_2\mu_2),
$$
and substituting \eqref{mu:1} in the first term and \eqref{mu:2} in the second, we get
$$
\mu^TS\mu = \mu_1^TS_1\mu_1 + (\mu_2-\mu_1)^TS_2S^{-1}S_1\mu_1 + \mu_2^TS_2\mu_2 + (\mu_1-\mu_2)^TS_1S^{-1}S_2\mu_2.
$$
Finally, \eqref{symmetry} yields
$$
\mu^TS\mu = \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2 - (\mu_2-\mu_1)^TS_1S^{-1}S_2(\mu_2-\mu_1)
$$
and combining this with \eqref{R} proves \eqref{comp_sq:R} and completes the proof.
\end{proof}

By virtue of \eqref{symmetry}, we define for $S_1$ positive definite:
$$
S_1*S_2 := S_1(S_1+S_2)^{-1}S_2.
$$

\begin{Cor}\label{comp_sq:2}
\begin{align*}
  \N(x;\mu_1,S_1)\N(Mx;\mu_2,S_2) &\propto \N(x;\mu,S_1 + M^TS_2M)\quad\text{where}\\
 \mu :&= S^{-1}(S_1\mu_1 + M^TS_2\mu_2). 
\end{align*}

Furthermore, if $M = I$ then 
\begin{equation}\label{exact_prod} 
  \N(x;\mu_1,S_1)\N(x;\mu_2,S_2) = \N(\mu_1;\mu_2,S_1*S_2)\N(x;\mu,S_1+S_2).
\end{equation}
In particular,
\begin{equation}
  \int_{\R^n}N(x;\mu_1,S_1)\N(x;\mu_2,S_2)dx = \N(\mu_1;\mu_2,S_1*S_2).
  \end{equation}
\begin{proof}
  The right-hand and left-hand exponents in \eqref{exact_prod} are equal by \eqref{comp_sq:R}, and it is
  straightforward to verify directly that the normalization factors outside the exponentials are also equal. 
\end{proof}
\end{Cor}


\section{The Forward Pass}
  Now we can inductively evaluate \eqref{alpha:0}. To do so, we split the computation
  into two steps.  In the first step, which we call the {\em time update}, we multiply
  $\alpha_{t-1}(u)$ by the state transition function $Pd(s\mid u)$ and integrate with respect
  to $u$. We will denote the result of the time update by $\hat{\alpha}_t(s)$.  It is the joint probability
  density of observations $x_1,\dots,x_{t-1}$ and state $s$ at time $t$.
  Then in the second step, which we call the {\em measurement update}, we multiply
  $\hat{\alpha}_t(s)$ by $X(x_t \mid s)$, the probability density of observing $x_t$ at time $t$
  in state $s$, to get $\alpha_t(s)$.

  First, for $1\le t \le T$ we inductively define
\begin{align*}
%  \widehat{S}_{a,t} &= S_{Tr}(S_{Tr}+S_{a,t-1})^{-1}S_{a,t-1},\\
  S_{a,t} &= M^TS_{Ob}M + S_{Tr}*S_{a,t-1},\\
  \mu_{a,t} &= S_{a,t}^{-1}(M^TS_{Ob}x_t + S_{Tr}*S_{a,t-1}\mu_{a,t-1})\\
  R_{a,t} &= R(x_t,\mu_{a,t-1},M^TS_{Ob}M,S_{Tr}*S_{a,t-1}),\\
  P_{a,t} &:= R_{a,t}P_{a,t-1},\quad\text{and}\\
  P_{a,0} &:= 1.
\end{align*}

\begin{Thm}\label{alpha:1}
  For each state $s$ and time $t \ge 1$,
$$
  \alpha_t(s) = P_{a,t}N(s;\mu_{a,t},S_{a,t}).
$$
\end{Thm}

\begin{proof}

Proceeding by induction on $t$, we note that the case $t = 0$ holds by definition.
For the time update, we have, using \eqref{comp_sq:2}
\begin{align}
  \hat{\alpha}_t(s) &= \int_{\R^n}Pd(s|u)\alpha_{t-1}(u)du \\\notag
 &= P_{a,t-1}\int_{\R^n}\N(u;s,S_{Tr})\N(u;\mu_{a,t-1},S_{a,t-1})du \notag\\
 &= P_{a,t-1}\N(s;\mu_{a,t-1},S_{Tr}*S_{a,t-1}).
\end{align}

Then multiplying by $X(x_t\mid s)$ and using \eqref{comp_sq:1} yields
\begin{align*}
  \alpha_t(s) &= X(x_t\mid s)\hat{\alpha}_t(s) \\
  &= P_{a,t-1}\N(Ms;x_t,S_{Ob})\N(s;\mu_{a,t-1},S_{Tr}*S_{a,t-1}) \\
  &= P_{a,t-1}R_{a,t}\N(s;\mu_{a,t},S_{a,t}), \\
  &= P_{a,t}\N(s;\mu_{a,t},S_{a,t}).
\end{align*}
\end{proof}

\section{The Backward Pass}
This calculation is very similar to the forward pass; the main difference being that we do
the measurement update first by multiplying by $X(x_{t+1}\mid u)$ to obtain $\hat{\beta}_{t+1}(u)$,
and then we integrate with respect to $Pd(u\mid s)du$ for the time update.  

\begin{Thm}\label{beta:1}
  Recursively define
\begin{align*}
  S_{b,t} :&= (M^TS_{Ob}M + S_{b,t+1})*S_{Tr},\\
  \mu_{b,t} :&= (M^TS_{Ob}M + S_{b,t+1})^{-1}(M^TS_{Ob}x_{t+1} + S_{b,t+1}\mu_{b,t+1}),\\
  R_{b,t} :&= R(x_{t-1},\mu_{b,t},S_{Ob},S_{b,t}),\quad\text{and}\\
  P_{b,t}:&=P_{b,t+1}R_{b,t+1},
\end{align*}
with initial values at $t = T$ given below. Then for each state $s$ and time $t < T$,
$$
  \beta_t(s) = P_{b,t}N(s;\mu_{b,t},S_{b,t}),
$$
\end{Thm}

\begin{proof}
We proceed by reverse induction on $t$. However, to get started we first must  deal with
the special case $\beta_{T}(u)$, the conditional probability of the future observations given
state $u$ at time $T$, but of course there are no such observations.  In the standard discrete
state HMM, we set $\beta(s) = 1$ for all states $s$, and this will work in our case as well by
setting $S_{b,T} = 0$ if $M$ is 1-1, but we really need a proper gaussian 
density in order to get the reverse induction started in all cases.  So we somewhat arbitrarily
assume the following values
\begin{align*}
  S_{b,T} :&= I,\\
  \mu_{b,T} :&= 0,\\
  P_{b,T} := 1;
\end{align*}
  For $t <  T-1$, we have 
  \begin{align*}
    \hat{\beta}_{t+1}(u) &= X(x_{t+1}\mid u)\beta_{t+1}(u)\\
    &= P_{b,t+1}\N(Mu;x_{t+1},S_{Ob})\N(u;\mu_{b,t+1},S_{b,t+1})\\
    &= P_{b,t+1}R_{b,t+1}\N(u;\mu_{b,t},M^TS_{Ob}M+S_{b,t+1})
  \end{align*}

  Then a second application of \eqref{comp_sq:2} yields
  \begin{align*}
    \beta_t(s) &= \int_{\R^n}\hat{\beta}_{t+1}(u)Pd(u\mid s)du \\
    &= P_{b,t+1}R_{b,t+1}\int_{\R^n}\N(u;\mu_{b,t},M^TS_{Ob}M + S_{b,t+1})\N(u;s,S_{Tr})du\\
    &=P_{b,t+1}R_{b,t+1}\N(s;\mu_{b,t},S_{b,t}),\quad\text{so that}\\
    P_{b,t} &= P_{b,t+1}R_{b,t+1}.
  \end{align*}
\end{proof}

\section{The Posterior Likelihoods}
Recall that $\gamma_t(s)$ is the posterior probability density of state $s$
at time $t$. 
\begin{Thm}
  Let notation be as in \eqref{alpha:1} and \eqref{beta:1}.  Define
\begin{align*}
  \widehat{S}_{c,t} :&= S_{a,t}+S_{b,t}, \quad\text{and}\\
  P_{c,t} :&= \int_{\R^n}\alpha_t(s)\beta_t(s)ds,\quad\text{then} \\
      \gamma_t(s) &= \N(s;\mu_{c,t},S_{c,t}),\quad\text{where}\\
      S_{c,t} &:= S_{a,t}\widehat{S}_{c,t}^{-1}S_{b,t}
      \quad\text{and}\\
      \mu_{c,t} &:= S_{a,t}\widehat{S}_{c_t}^{-1}\mu_{b,t} +
      S_{b,t}\widehat{S}_{c_t}^{-1}\mu_{a,t}.
  \end{align*}
\end{Thm}
\begin{proof}
  From \eqref{alpha:1} and \eqref{beta:1} we have (using \eqref{comp_sq:2}
  as usual)
  \begin{align*}
  \alpha_t(s)\beta_t(s) &= P_{a,t}P_{b,t}\N(s;\mu_{a,t},S_{a,t})
  \N(s;\mu_{b,t},S_{b,t})ds\\
  &= P_{a,t}P_{b,t}\N(\mu_{a,t};\mu_{b,t},S_{a,t}+S_{b,t})\N(s;\mu_{c,t},
  S_{c,t}), \quad\text{whence}\\
  P_{c,t} &= P_{a,t}P_{b,t}\N(\mu_{a,t};\mu_{b,t},S_{a,t}+S_{b,t})
  \end{align*}
by \eqref{exact_prod}, and the theorem follows.
\end{proof}

We also need the joint posterior probability density of state $u$ at time $t-1$ and state $s$ at time $t$,
which we denote by $\hat{\gamma}_t(s,u)$. The joint probability density of all the data and the two given states
  $u,s$  at times $t-1,t$ is
  \begin{align}
    \hat{\gamma}_t(s,u) &\propto \alpha_{t-1}(u)Pd(s\mid u)X(x_t\mid s)\beta_t(s) \notag\\
    &= \N(u;\mu_{a,t-1},S_{a,t-1})\N(u;s,S_{Tr})  \N(s;x_t,S_{Ob})\N(s;\mu_{b,t},S_{b,t})\notag\\
    &\propto \N(u;\mu^{(1)}_t,S^{(1)}_t)\N(s;\mu_{a,t-1},\widehat{S}_{a,t})\N(s;\mu_{b,t-1},
    \widetilde{S}_{b,t})\notag\\
    &\propto \N(u;\mu^{(1)}_t,S^{(1)}_t)\N(s;\mu^{(2)}_t,S^{(2)}_t),\label{gamma_hat}
  \end{align}
  where we have discarded factors which do not depend on either $u$ or $s$, and 
  \begin{align}
    S^{(1)}_t &:= S_{a,t-1}\widehat{S}_{a,t}^{-1}S_{Tr},\notag\\
    \mu^{(1)}_t &:= S_{a,t-1}\widehat{S}_{a,t}^{-1}s + S_{Tr}\widehat{S}_{a,t}^{-1}\mu_{a,t-1},\label{mu(1)}\\
    S^{(2)}_t &:= \widehat{S}_{a,t}(\widehat{S}_{a,t}+\widetilde{S}_{b,t})^{-1}\widetilde{S}_{b,t}
    = \widehat{S}_{a,t}\widehat{S}_{c,t}^{-1}\widetilde{S}_{b,t},\notag\quad\text{and}\\
    \mu^{(2)}_t &:= \widehat{S}_{a,t}\widehat{S}_{c,t}^{-1}\mu_{b,t-1} +
    \widetilde{S}_{b,t}\widehat{S}_{c,t}^{-1}\mu_{a,t-1}\label{mu(2)}.
  \end{align}

  Note that the second factor in \eqref{gamma_hat} does not depend on $u$, and the first factor depends on $s$
  only in the mean, which means that the integral of the first factor with respect to $u$ is always unity
  and does not depend on $s$, and thus the double integral of \eqref{gamma_hat} is still unity and no
  normalization factor is required.
  
\section{Re-estimation}
Given a state sequence $S := \{s_0,s_1,s_2,\dots,s_T\}$ and the data sequence $X := \{x_1,x_2,\dots,x_T\}$, the
joint probability density of $S$ and $X$ given parameters $\theta := \{\mu_0,S_0,S_{Tr},S_{Ob}\}$ is
$$
P(X,S\mid\theta) = \N(s_0;\mu_0,S_0)\prod_{t=1}^T\N(x_t;s_t,S_{Ob})\N(s_t;s_{t-1},S_{Tr}).
$$

Note that $P(X,S\mid\theta)$ can also be viewed as  $L(\theta\mid X,S)$, the posterior likelihood of $\theta$.
To re-estimate $\theta$, we try to maximize $L(\theta)$ using the EM algorithm. Namely, we define 
$$
E(\theta,\bar{\theta}) := \int_{\R^{Tn}}P(X,S\mid\theta)\log{L(\bar{\theta}\mid X,S)}dS,
$$
where $\theta$ is the current set of parameters, and $\bar{\theta}$ is the unknown set of new parameters
we wish to determine. So instead of maximizing the log-likelihood directly, we can maximize its expected
value with respect to the current posterior distribution, because it is a standard result (and easy to prove) that
$$
L(\bar{\theta}) - L(\theta) \ge E(\theta,\bar{\theta}) - E(\theta,\theta),
$$
so choosing $\bar{\theta}$ to maximize $E(\theta,\bar{\theta})$ will increase the value of $L(\theta)$.


  
Recall the notation $Q(x;\mu,S)$ from \eqref{comp_sq:1}.  Then we have
\begin{align}
  E(\theta,\bar{\theta}) &= -(2T+1)\frac{n\log(2\pi)}{2} + \frac{1}{2}(E_0 + E_1 + E_2),\quad\text{where}\notag\\
  E_0 :&=\int_{\R^{Tn}}P(X,S\mid\theta)\{\log\det\overline{S}^{-1}_0 - Q(s_0;\bar{\mu}_0,\overline{S}_0^{-1})\}dS,\label{E0}\\
  E_1 :&= \int_{\R^{Tn}}P(X,S\mid\theta)\left\{\sum_{t=1}^T\log\det\overline{S}^{-1}_{Ob} - Q(x_t;s_t,\overline{S}_{Ob}^{-1})\right\}dS,
\label{E1}\\
  E_2 :&= \int_{\R^{Tn}}P(X,S\mid\theta)\left\{\sum_{t=1}^T\log\det\overline{S}^{-1}_{Tr} - Q(s_t;s_{t-1},\overline{S}_{Tr}^{-1})\right\}dS,
\label{E2}
\end{align}
Since the expected value of a constant is just that constant, $-n\log(2\pi)/2$ can be moved outside each
integral sign and ignored in the optimization.  And since none of the parameters of $\theta$ appear in more
than one of the $E_i$, we can optimize $E$ by optimizing each of $E_1,E_2,E_3$ separately.

We begin with $E_0$. Since the only component of $S$ in the integrand of \eqref{E0} is $s_0$, the expected value
collapses to the marginal 
expected value of the integrand with respect to the marginalization of $P(X,S\mid\theta)$ at $s_0$, which is just 
$\gamma_0(s_0)$.  Furthermore, after pulling the constant and the summation 
outside the integral in \eqref{E1} we have the same collapse to the marginalization at $s_t$ of the posterior, which
is just $\gamma_t(s_t)$.  It follows that
\begin{align*}
E_0 &= \log\det(\overline{S}^{-1}_0) - \int_{\R^n}\gamma_0(s)Q(s;\bar{\mu}_0,\overline{S}_0)ds,\quad\text{and}\\
E_1 &= T\log\det(\overline{S}^{-1}_{Ob}) - \sum_{t=1}^T\int_{\R^n}\gamma_t(s)Q(x_t;s,\overline{S}_{Ob})ds.
\end{align*}

The situation for \eqref{E2} is slightly different, because after again moving the constant and the summation outside
the integral, the integrand depends on both $s_t$ and $s_{t-1}$.  So in this case, the integral collapses to the 
joint marginal expectation over $s_t$ and $s_{t-1}$, and we get
$$
E_2 = T\log\det(\overline{S}^{-1}_{Tr}) - \sum_{t=1}^T\int_{\R^{2n}}\hat{\gamma}_t(s,u)Q(s;u,\overline{S}_{Tr})duds,
$$
where $\hat{\gamma}_t(s,u)$ is the joint posterior probability density of state $u$ at time $t-1$ and state $s$ 
at time $t$ given by \eqref{gamma_hat}.

If $f$ is any scalar-valued function of an $m\times{n}$ matrix $A = a_{ij}$, we denote by $\partial{f}/\partial{A}$ the 
$m\times{n}$ matrix whose $(i,j)$-entry is $\partial{f}/\partial{a_{ij}}$.  This also applies to column vectors (when $n=1$).
In particular, if $m=n$, $A$ is symmetric, and $f(A) = \det(A)$,  then the usual expansion 
in minors along the $i^{th}$ row shows that 
$$
\frac{\partial{\det(A)}}{\partial{A}} = (2-\delta_{ij})A^* = (2-\delta_{ij})\det(A)A^{-1},\quad\text{and}
\quad\frac{\partial{\log\det(A)}}{\partial{A}} = (2-\delta_{ij})A^{-1}. 
$$
Here $A^*$ is the adjoint matrix and the notation $(2-\delta_{ij})A^*$ means to multiply all off-diagonal elements of $A^*$ by 2
and leave the diagonal unchanged.  This is of course necessary due the the symmetry $A = A^T$.

In addition, since $Q(x;\mu,S)$ is linear in the coefficients of $S$, we see that
$$
\frac{\partial{Q(x;\mu,S)}}{\partial{S}} = (2-\delta_{ij})(x-\mu)(x-\mu)^T,
$$
and it is also easy to verify that
$$
\frac{\partial{Q(x;\mu,S)}}{\partial{\mu}} = 2S(\mu-x).
$$

Armed with these formulas, we first minimize $E_0$ with respect to $\bar{\mu}_0$: 
 
\begin{align}
0 = \frac{\partial{E_0}}{\partial{\bar{\mu}_0}} &= \int_{\R^n}\gamma_0(s)\frac{\partial{Q(s;\bar{\mu}_0,\overline{S}_0)}}{\partial{\bar{\mu}_0}}ds \notag\\
&= \int_{\R^n}\gamma_0(s)(2\overline{S}_0^{-1}(\bar{\mu}_0-s))ds\notag\\
&= 2\overline{S}_0^{-1}\int_{\R^n}\gamma_0(s)(\bar{\mu}_0-s)ds \notag\\
&= 2\overline{S}_0^{-1}(\bar{\mu}_0-\mu_{c,0}), \quad\text{hence}\notag\\
\bar{\mu}_0 &= \mu_{c,0}\label{mu_0}
\end{align}

Next, we maximize $E_0$ with respect to $\overline{S}_0^{-1}$, and we can set $\bar{\mu}_0 = \mu_{c,0}$ by \eqref{mu_0}:
\begin{align*}
0 = \frac{\partial{E_0}}{\partial{\overline{S}_0^{-1}}} &= \frac{\partial{\log\det(\overline{S}_0^{-1})}}{\partial\overline{S}_0^{-1}} - \int_{\R^n}\gamma_0(s)\frac{\partial{Q(s;\mu_{c,0},\overline{S}_0^{-1})}}{\partial{\overline{S}_0^{-1}}}ds \\
&= (2-\delta_{ij})\overline{S}_0 - (2-\delta_{ij})\int_{\R^n}\gamma_0(s)(s-\mu_{c,0})(s-\mu_{c,0})^Tds.
\end{align*}
Not surprisingly, it follows that
\begin{equation}\label{Sigma_0}
  \overline{S}_0 = \int_{\R^n}\gamma_0(s)(s-\mu_{c,0})(s-\mu_{c,0})^Tds = S_{c,o}.
  \end{equation}

To minimize $E_1$, we differentiate with respect to $\overline{S}_{ob}^{-1}$:
\begin{align*}
  0 = \frac{\partial{E_1}}{\partial{\overline{S}_{Ob}^{-1}}} &= T\frac{\partial{\log\det(\overline{S}_{Ob}^{-1})}}{\partial\overline{S}_{Ob}^{-1}}
  - \sum_{t=1}^T\int_{\R^n}\gamma_t(s)\frac{\partial{Q(s;x_t,\overline{S}_{Ob}^{-1})}}{\partial{\overline{S}_{Ob}^{-1}}}ds \\
  &= (2-\delta{ij})T\overline{S}_{Ob} - (2-\delta_{ij})\sum_{t=1}^T\int_{\R^n}\gamma_t(s)(s-x_t)(s-x_t)^Tds.
\end{align*}
Thus,  we see that
\begin{equation}\label{Sigma_Ob}
  \overline{S}_{Ob} = \frac{1}{T}\sum_{t=1}^T [S_{c,t} + (\mu_{c,t}-x_t)(\mu_{c,t}-x_t)^T].
\end{equation}

Finally, we minimize $E_2$ by differentiating with respect to $\overline{S}_{Tr}^{-1}$:
\begin{equation}\label{Sigma_Tr:0}
  \begin{split}
  0 &= \frac{\partial{E_2}}{\partial{\overline{S}_{Tr}^{-1}}} = \frac{\partial{\log\det(\overline{S}_{Tr}^{-1})}}{\partial\overline{S}_{Tr}^{-1}}
  - \sum_{t=1}^T\int_{\R^{2n}}\hat{\gamma}_t(s,u)\frac{\partial{Q(s;u,\overline{S}_{Tr}^{-1})}}{\partial\overline{S}_{Tr}^{-1}}duds \\
      &= (2-\delta_{ij})T\overline{S}_{Tr} - (2-\delta{ij}) \sum_{t=1}^T\int_{\R^{2n}}\N(u;\mu^{(1)}_t,S^{(1)}_t)\N(s;\mu^{(2)}_tS^{(2)}_t)(u-s)(u-s)^Tduds.
  \end{split}
\end{equation}

As noted earlier, $\N(s,\mu^{(2)}_t,S^{(2)}_t)$ does not depend on $u$, so we can integrate first with respect to $u$, with a result
analagous to \eqref{Sigma_Ob}:
$$
  \int_{-\infty}^{\infty}\N(u;\mu^{(1)}_t,S^{(1)}_t)(u-s)(u-s)^Tdu = S^{(1)}_t + (s-\mu^{(1)}_t)(s-\mu^{(1)}_t)^T.
$$
Substituting this result into \eqref{Sigma_Tr:0}, we get
\begin{equation}\label{Sigma_Tr:1}
  \overline{S}_{Tr} =  \frac{1}{T}\left(\sum_{t=1}^TS^{(1)}_t +\int_{-\infty}^{\infty}\N(s;\mu^{(2)}_t,S^{(2)}_t)
  (s-\mu^{(1)}_t)(s-\mu^{(1)}_t)^Tds\right).
\end{equation}

However, there is a problem evaluating this integral because after checking \eqref{mu(1)}, we see that $\mu^{(1)}_t$
depends on $s$, namely  
\begin{equation}\label{s-mu_1}
    \begin{split}
s - \mu^{(1)}_t &= (I - S_{a,t-1}\widehat{S}_{a,t}^{-1})s - S_{Tr}\widehat{S}_{a,t}^{-1}\mu_{a,t-1}, \\
  &= (\widehat{S}_{a,t}\widehat{S}_{a,t}^{-1} - S_{a,t-1}\widehat{S}_{a,t}^{-1})s  - 
  S_{Tr}\widehat{S}_{a,t}^{-1}\mu_{a,t-1},\\
  &= (\widehat{S}_{a,t} - S_{a,t-1})\widehat{S}_{a,t}^{-1}s - S_{Tr}\widehat{S}_{a,t}^{-1}\mu_{a,t-1},\\
  &= S_{Tr}\widehat{S}_{a,t}^{-1}s - S_{Tr}\widehat{S}_{a,t}^{-1}\mu_{a,t-1},\\
  &= S_{Tr}\widehat{S}_{a,t}^{-1}(s - \mu_{a,t-1}).
    \end{split}
\end{equation}


Fortunately, neither $\mu^{(2)}_t,S^{(2)}_t,$ nor $S^{(1)}_t$ depends on $s$,
so \eqref{Sigma_Tr:1} becomes
\begin{align}
  \overline{S}_{Tr} &= \frac{1}{T}\sum_{t=1}^TS^{(1)}_t + \frac{1}{T}\sum_{t=1}^T
    \int_{-\infty}^{\infty}\N(s;\mu^{(2)}_t,S^{(2)}_t)S_{Tr}\widehat{S}_{a,t}^{-1}(s-\mu_{a,t-1})
    s-\mu_{a,t-1})^T\widehat{S}_{a,t}^{-1}S_{Tr}ds\notag\\
    &=\frac{1}{T}\sum_{t=1}^TS^{(1)}_t +\frac{1}{T}\sum_{t=1}^TS_{Tr}\widehat{S}_{a,t}^{-1}
    \left[\int_{-\infty}^{\infty}\N(s;\mu^{(2)}_t,S^{(2)}_t)(s-\mu_{a,t-1})(s-\mu_{a,t-1})^Tds\right]\widehat{S}_{a,t}^{-1}S_{Tr}\notag\\
   &=\frac{1}{T}\sum_{t=1}^TS^{(1)}_t+S_{Tr}\left(\frac{1}{T}\sum_{t=1}^T\widehat{S}_{a,t}^{-1}
   [S^{(2)}_t+(\mu^{(2)}_t-\mu_{a,t-1})(\mu^{(2)}_t-\mu_{a,t-1})^T]\widehat{S}_{a,t}^{-1}\right)S_{Tr}.\label{Sigma_Tr:2}
  \end{align}

Finally, we can make the following simplification by using \eqref{mu(2)}.
\begin{align*}
  \mu^{(2)} - \mu_{a,t-1} &= \widehat{S}_{a,t}\widehat{S}_{c,t}^{-1}\mu_{b,t-1} +
  (\widetilde{S}_{b,t-1}\widehat{S}_{c,t}^{-1}-I)\mu_{a,t-1}\\
  &= \widehat{S}_{a,t}\widehat{S}_{c,t}^{-1}\mu_{b,t-1} +
  (\widetilde{S}_{b,t-1}\widehat{S}_{c,t}^{-1}-\widehat{S}_{c,t}\widehat{S}_{c,t}^{-1})\mu_{a,t-1}\\
  &=  \widehat{S}_{a,t}\widehat{S}_{c,t}^{-1}\mu_{b,t-1} +
  (\widetilde{S}_{b,t-1}-\widehat{S}_{c,t})\widehat{S}_{c,t}^{-1}\mu_{a,t-1}\\
  &= \widehat{S}_{a,t}\widehat{S}_{c,t}^{-1}(\mu_{b,t-1} -\mu_{a,t-1}),
\end{align*}
and substituting this into \eqref{Sigma_Tr:2} we have
\begin{equation}\label{Sigma_Tr:3}
\overline{S}_{Tr}   =\frac{1}{T}\sum_{t=1}^TS^{(1)}_t+S_{Tr}\left(\frac{1}{T}\sum_{t=1}^T\widehat{S}_{c,t}^{-1}
   [S^{(2)}_t+(\mu_{b,t-1}-\mu_{a,t-1})(\mu_{b,t-1}-\mu_{a,t-1})^T]\widehat{S}_{c,t}^{-1}\right)S_{Tr}.
\end{equation}  

\section{The M matrix}
We now consider the important generalization of the basic case in which the observation space and the state space are different.
In particular, they may have different dimensions.  Let the state space have dimension $n$ and the observation space have
dimension $m$.  We are given a linear map $M: \R^n\rightarrow R^m$ such that if $s_t\in \R^n$ is the state at time $t$, then
$$
X(x_t\mid s_t)= \N(x_t;Ms_t,S_{Ob}).
$$
Our first task is to generalize \eqref{comp_sq:1}.

\begin{Lem}\label{comp_sq:3}
With the notation of \eqref{comp_sq:1}:
\begin{align}
  Q(x;\mu_1,S_1) + Q(x;M\mu_2,S_2) &= \label{comp_sq:3a}\\
  Q(x;\mu_1,S_1) + Q(Mx;\mu_2,S_2) = \label{comp_sq:3b}
\end{align}
\end{Lem}
\begin{proof}
\eqref{comp_sq:3a} is immediate from \eqref{comp_sq:1} by making the formal substitution $M\mu_2$ for $\mu_2$.  But \eqref{comp_sq:3b} is a bit more involved.  

\end{proof}
\end{document}





so we can make the affine change of variable 
$$
v := S_{Tr}\widehat{S}_{a,t}^{-1}(s - \mu_{a,t-1}),\quad dv = \det(S_{Tr}\widehat{S}_{a,t}^{-1})ds
$$
in \eqref{Sigma_Tr:1}, which yields
\begin{equation} \label{Sigma_tr:2}
  \begin{split}
  \overline{S}_{Tr} &= \frac{1}{T}\sum_{t=1}^T \left(S^{(1)}_t +\det(\widehat{S}_{a,t}S_{Tr}^{-1})
  \int_{-\infty}^{\infty}\N(v;\mu^{(3)}_t,S_t^{(3)})vv^T dv\right)\\
  &=  \frac{1}{T}\sum_{t=1}^T \left(S^{(1)}_t +\det(\widehat{S}_{a,t}S_{Tr}^{-1}) (S^{(3)}_t
  + \mu^{(3)}_t(\mu^{(3)}_t)^T)\right).
  \end{split}
\end{equation}
where
\begin{align*}
S_t^{(3)} :&= S_{Tr}\widehat{S}^{-1}_{a,t}S^{(2)}_t\widehat{S}_{a,t}^{-1}S_{Tr},\quad\text{and}\\
\mu_t^{(3)} :&= S_{Tr}\widehat{S}_{a,t}^{-1}(\mu_t^{(2)} - \mu_{a,t-1}).
\end{align*}

However, by a calculation entirely analagous to \eqref{s-mu_1}, we find that
$$
S_{tr}\widehat{S}_{a,t}^{-1}(\mu_t^{(2)}-\mu_{a,t-1})=
(\widehat{S}_{a,t}+\widetilde{S}_{b,t})^{-1}(\mu_{b,t-1}-\mu_{a,t-1}),
$$
which yields
$$
\mu_t^{(3)} = S_{Tr}(\widehat{S}_{a,t}+\widetilde{S}_{b,t})^{-1}(\mu_{b,t-1}-\mu_{a,t-1}).
$$


\eqref{Sigma_Tr:1} becomes
\begin{align}
  \overline{S}_{Tr} &= \frac{1}{T}\sum_{t=1}^TS^{(1)}_t + \frac{1}{T}\sum_{t=1}^T
    \int_{-\infty}^{\infty}\N(s;\mu^{(2)}_t,S^{(2)}_t)S_{Tr}\widehat{S}_{a,t}^{-1}(s-\mu_{a,t-1})
    s-\mu_{a,t-1})^T\widehat{S}_{a,t}^{-1}S_{Tr}ds\notag\\
    &=\frac{1}{T}\sum_{t=1}^TS^{(1)}_t +\frac{1}{T}\sum_{t=1}^TS_{Tr}\widehat{S}_{a,t}^{-1}
    \left[\int_{-\infty}^{\infty}\N(s;\mu^{(2)}_t,S^{(2)}_t)(s-\mu_{a,t-1})(s-\mu_{a,t-1})^Tds\right]\widehat{S}_{a,t}^{-1}S_{Tr}\notag\\
   &=\frac{1}{T}\sum_{t=1}^TS^{(1)}_t+S_{Tr}\left(\frac{1}{T}\sum_{t=1}^T\widehat{S}_{a,t}^{-1}
   [S^{(2)}_t+(\mu^{(2)}_t-\mu_{a,t-1})(\mu^{(2)}_t-\mu_{a,t-1})^T]\widehat{S}_{a,t}^{-1}\right)S_{Tr}.\label{Sigma_Tr:2}
  \end{align}

Finally, we can make the following simplification by using \eqref{mu(2)} and defining
$$
\widehat{S}_{c,t} = \widehat{S}_{a,t} + \widetilde{S}_{b,t}.
$$
Then
\begin{align*}
  \mu^{(2)} - \mu_{a,t-1} &= \widehat{S}_{a,t}\widehat{S}_{c,t}^{-1}\mu_{b,t-1} +
  (\widetilde{S}_{b,t-1}\widehat{S}_{c,t}^{-1}-I)\mu_{a,t-1}\\
  &= \widehat{S}_{a,t}\widehat{S}_{c,t}^{-1}\mu_{b,t-1} +
  (\widetilde{S}_{b,t-1}\widehat{S}_{c,t}^{-1}-\widehat{S}_{c,t}\widehat{S}_{c,t}^{-1})\mu_{a,t-1}\\
  &=  \widehat{S}_{a,t}\widehat{S}_{c,t}^{-1}\mu_{b,t-1} +
  (\widetilde{S}_{b,t-1}-\widehat{S}_{c,t})\widehat{S}_{c,t}^{-1}\mu_{a,t-1}\\
  &= \widehat{S}_{a,t}\widehat{S}_{c,t}^{-1}(\mu_{b,t-1} -\mu_{a,t-1}),
\end{align*}
and substituting this into \eqref{Sigma_Tr:2} we have
\begin{equation}\label{Sigma_Tr:3}
\overline{S}_{Tr}   =\frac{1}{T}\sum_{t=1}^TS^{(1)}_t+S_{Tr}\left(\frac{1}{T}\sum_{t=1}^T\widehat{S}_{c,t}^{-1}
   [S^{(2)}_t+(\mu_{b,t-1}-\mu_{a,t-1})(\mu_{b,t-1}-\mu_{a,t-1})^T]\widehat{S}_{c,t}^{-1}\right)S_{Tr}.
\end{equation}  



By definition, $P_{c,t}$ is the posterior probability density of all the data and is therefore independent of $t$.
However, as a useful check on our calculations, we can derive this directly:
\begin{align*}
  \frac{P_{c,t}}{P_{c,t+1}} &= \frac{P_{a,t}P_{b,t}\N(\mu_{a,t};\mu_{b,t},S_{a,t}+S_{b,t}}
  {P_{a,t+1}P_{b,t+1}\N(\mu_{a,t+1};\mu_{b,t+1},S_{a,t+1}+S_{b,t+1})}\\
  &=\frac{P_{a,t}P_{b,t+1}\N(x_{t+1};\mu_{t+1},\widehat{S}_{b,t+1})\N(\mu_{a,t};\mu_{b,t},S_{a,t}+S_{b,t})}
  {P_{a,t}P_{b,t+1}\N(x_{t-1};\mu_{a,t}S_{Ob}+\widehat{S}_{a,t})\N(\mu_{a,t+1};\mu_{b,t+1},S_{a,t+1}+S_{b,t+1})}\\
\end{align*}

 %  \hat{\beta}_T(u) &= \N(Mu;x_T,S_{Ob})\N(u;0,I) = R_{b,T}\N(u,M^Tx_T,M^TS_{Ob}M+I),\quad\text{and}\\
%  \beta_{T-1}(s) &= \int_{\R^n}\N(Mu;x_T,S_{Ob})\N(u;s,S_{Tr})du \\
%  &= \N(s;x_T,S_{Ob}*S_{Tr})
%\end{align*}

  Thus, our initial values at $t = T-1$ are
  \begin{align*}
  P_{b,T-1} &= 1,\\
  \mu_{b,T-1} &= x_T, \quad\text{and}\\
  S_{b,T-1} &= S_{Ob}*S_{Tr}.
  \end{align*}

