\documentclass[12pt,leqno]{article}
\include{article_defs}
\title{A Continuous State Hidden Markov Model}
\author{David M. Goldschmidt}
%\oddsidemargin 10 pt \evensidemargin 10 pt \marginparwidth 0.75 in \textwidth
%6.0 true in \topmargin -40 pt \textheight 8.8 true in 
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lfoot{}
%\rfoot{\thepage}
\begin{document}
%\renewcommand{\footrulewidth}{0.4pt}
\newcommand{\p}{\ensuremath{u}}
\newcommand{\VV}{V}
\maketitle


\section{Introduction}
In this paper, we define a discrete time Hidden Markov Model (HMM) with a continuous state.  The Markov state transitions
are governed by a deterministic linear map with added gaussian noise, and the outputs are similarly obtained from the
state via another deterministic linear map with added gaussian noise.  We then derive in closed form the Baum-Welch forward
and backward iterations \cite{Rabiner} to compute the joint posterior likelihood of all the data and the state at each
time $t$.  So far, this looks very much like Kalman smoothing \cite{}.  However we then apply the E-M algorithm \cite{}
to obtain re-estimation formulas for all model parameters.   We conclude with some numerical results on simulated data.

Some familiarity with the discrete state HMM methodology is assumed.  A good reference is \cite{bilmes}. 

\section{Preliminary Definitions and Notation}
Let $\mu$ and $x$ be vectors in $\R^n$ and let $\S$ be an $n\times{n}$ positive
definite real symmetric matrix.  Recall that the gaussian probability density
on $\R^n$ is given by 
$$
\N(x;\mu,S) := (2\pi)^{-\frac{n}{2}}|S|^{\frac{1}{2}}\exp\left\{-\frac{1}{2}
(x - \mu)^TS(x-\mu)\right\}.
$$
So here and below, capital $S$ denotes an inverse covariance matrix.

We now define a Hidden Markov Model with state space $\R^n$ as follows.  Given the state $s\in\R^n$ and an observation $x_t\in\R^m$ at time $t$, the output density is
$$
X(x_t\mid s) := \N(x_t;M_ts + b_t,S_M),
$$
where $M_t$ is a linear map from the state space to the
measurement space, $b_t$ is a bias, and $S_M$ is an $m\times{m}$ positive definite real symmetric matrix.  In this paper, $S_M$ and $M$ are time-independent model parameters which we usually seek to re-estimate from the data.  The bias $b_t$ can
usually be 
subtracted from the data so we assume here that $b_t = 0$ for all $t$. 
In the simplest case, $M_t = I$ for all $t$ so that $m = n$ and $s$ is itself the mean of the output distribution.  
The state process is a discrete time continuous state Markov process with transition probability density
from state $s_0$ at time $t$ to state $s_1$ at time $t+1$ given by
$$
Pd(s_1\mid s_0) :=  \N(s_1;T_ts_0+c_t,S_T),
$$
where $S_T$ is a time-independent model parameter, $T_t$, the ``transition matrix'' is an invertible linear map  
defining the deterministic component of the time update, and $c_t$ is a bias.  In this paper, we assume that
$T_t = T$ is a time-independent model parameter which we may
also re-estimate from data, and that $c_t = 0$.  Note that $T$ is not a transition matrix in the
discrete-state HMM sense. To avoid scaling issues, we constrain $T$ to be unimodular.

As usual, we are given observations $\{x_t\mid 1\le t\le T\}$ and model parameters $\theta := \{M,S_M,T,S_T,S_0,\mu_0\}$
where $S_0$ and $\mu_0$ are defined below.  For each observation time $t$ and state $s$, we define
\begin{align*}
  \alpha_t(s) :&= Pd(x_1,x_2,\dots,x_t~ \& \text{state $s$ at time $t$}\mid \theta ),\\
  \beta_t(s) :&= Pd(x_{t+1},\dots,x_T \mid ~\text{state $s$ at time $t$},\theta)\\
  \gamma_t(s) :&= \frac{\alpha_t(s)\beta_t(s)}{\int_{\R^n}\alpha_t(s)\beta_t(s)ds}.
\end{align*}

Thus, $\alpha_t(s)$ is the joint density of state $s$ at time $t$ and the observations up to (and including) time $t$,
$\beta_t(s)$ is the {\em conditional} density of the future observations given state $s$ at time $t$, and $\gamma_t(s)$
is the posterior probability density of state $s$ at time $t$.

It is clear from the definitions that the following recursions are satisfied:
\begin{align}
\alpha_t(s) &= X(x_t \mid s)\int_{\R^n}\alpha_{t-1}(u)Pd(s \mid u)du,\quad\text{and}\label{alpha:0}\\
\beta_t(s) &= \int_{\R^{n}}Pd(u \mid s)X(x_{t+1} \mid u)\beta_{t+1}(u)du.\label{beta:0}
\end{align}

We initialize these recursions with
\begin{align*}
\alpha_0(s) :&= \N(s;\mu_0,S_0) \\
\beta_{T}(s) :&= \N(s;0,I), \quad\text{the standard unit normal},
\end{align*}
where $S_0$ and $\mu_0$ are additional model parameters.

\section{Completing the Square} 
Because everything in sight is gaussian, the above integrals can be evaluated in closed form
using the following lemma, which is used repeatedly below.
\begin{Lem}
 Define the quadratic form
  $$
 Q(x;\mu,S) := (x-\mu)^TS(x-\mu),
  $$
  where $x,\mu\in \R^n$ and  $S$ is a symmetric positive definite $n\times{n}$
  matrix. Let $A_i$ be an $m_i\times{n}$ matrix, $\mu_i\in\R^{n_i} ~ (i = 1,2)$,
  and let $S_1$,$S_2$ be symmetric positive semi-definite, with at least one of them
  positive definite.  Then 
\begin{equation}\label{comp_sq:1}
  Q(A_1x;\mu_1,S_1)+Q(A_2x;\mu_2,S_2) = Q(x;\mu,S) + R(\mu_1,\mu_2,S_1,S_2,A_1,A_2),
\end{equation}
where 
\begin{align}
S &= A_1^TS_1A_1 + A_2^TS_2A_2, \label{sigma}\\
\mu &= S^{-1}(A_1^TS_1\mu_1 + A_2^TS_2\mu_2),\quad\text{and}\label{mu}\\
R &= \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2 - \mu^TS\mu. \label{R_def} 
\end{align}
Furthermore, put $S_i' := A_i^TS_iA_i ~ (i = 1,2)$, then
\begin{equation}\label{comp_sq:3}
  S_1'S^{-1}S_2' = S_2'S^{-1}S_1'.
\end{equation}
Moreover, if $A_1$ and $A_2$ are invertible, then with $\mu_i' := A_i^{-1}\mu_i ~ (i = 1,2)$ we have
\begin{equation}\label{comp_sq:R}
R = (\mu_2'-\mu_1')^TS_1'S^{-1}S_2'(\mu_2'-\mu_1').
\end{equation}
\end{Lem}
\begin{proof}
  Note that at least one of $S_1$, $S_2$ is positive definite and the other is positive semi-definite,
so $S$ is invertible.  Expanding the left-hand side of \eqref{comp_sq:1} and combining
like terms, we get
$$
x^T(S_1'+ S_2')x -2x^T(A_1^TS_1\mu_1+ A_2^TS_2\mu_2) 
+ \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2.
$$
Completing the square, we get
$$
Q(A_1x;\mu_1,S_1) + Q(A_2x;\mu_2,S_2) = (x-\mu)^TS(x-\mu) -\mu^TS\mu + \mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2,
$$
proving the first three assertions.
To prove \eqref{comp_sq:3}, assume for the moment that both of the $S_i'$ are invertible.
Then
\begin{align*}
S_1'^{-1}SS_2'^{-1} &= S_1'^{-1}(S_1'+S_2')S_2'^{-1} = S_1'^{-1}+S_2'^{-1} = S_2'^{-1}SS_1'^{-1},\quad\text{hence}\\
S_2'S^{-1}S_1' &= (S_1'^{-1}+S_2'^{-1})^{-1} = S_1'S^{-1}S_2'.
\end{align*}
Now for $i = 1,2$, $S_i'+\epsilon{I}$ is positive definite and thus invertible for any $\epsilon > 0$
so \eqref{comp_sq:3} holds for $S_i'+\epsilon{I}$ and all $\epsilon > 0$.  Taking the limit
we conclude that it holds for $\epsilon = 0$ as well.

Finally, suppose that both of the $A_i$ are invertible. Then it's easy to see that

$$
Q(A_ix;\mu_i,S_i) = Q(x;\mu_i',S_i')\quad(i = 1,2),
$$
and \eqref{mu} becomes
\begin{equation} \label{mu'}
\mu = S^{-1}(S_1'\mu_1' + S_2'\mu_2')
\end{equation}.
We can re-write this as
\begin{equation}\label{mu:1}
\mu = S^{-1}(S_1'\mu_1' + S_2'\mu_1' + S_2'(\mu_2'-\mu_1) = \mu_1' + S^{-1}S_2(\mu_2'-\mu_1'),
\end{equation}
and by symmetry we have
\begin{equation}\label{mu:2}
  \mu = \mu_2' + S^{-1}S_1'(\mu_1'-\mu_2').
\end{equation}
Then from \eqref{mu'} we have
$$
\mu^TS\mu = \mu^T(S_1'\mu_1' + S_2'\mu_2'),
$$
and substituting \eqref{mu:1} in the first term and \eqref{mu:2} in the second, we get
$$
\mu^TS\mu = \mu_1'^TS_1'\mu_1' + (\mu_2'-\mu_1')^TS_2'S^{-1}S_1'\mu_1' + \mu_2'^TS_2'\mu_2'+(\mu_1'-\mu_2')^TS_1'S^{-1}S_2'\mu_2'.
$$
Finally, \eqref{comp_sq:3} yields
$$
\mu^TS\mu = \mu_1'^TS_1'\mu_1' + \mu_2'^TS_2'\mu_2' - (\mu_2'-\mu_1')^TS_1'S^{-1}S_2'(\mu_2'-\mu_1')
$$
and combining this with \eqref{R_def} proves \eqref{comp_sq:R} and completes the proof.
\end{proof}

By virtue of \eqref{comp_sq:3}, we define
$$
S_1*S_2 := S_1(S_1+S_2)^{-1}S_2.
$$

\begin{Cor}\label{comp_sq:2}
\begin{align*}
  \N(x;\mu_1,S_1)\N(Ax;\mu_2,S_2) &= \N_0\N(x;\mu,S)\quad\text{where}\\
  S :&= S_1+A^TS_2A,\\
  \mu :&= S^{-1}(S_1\mu_1 + A^TS_2\mu_2)\quad\text{and} \\
  \N_0 :&= \left(\frac{|S_1|\cdot|S_2|}{|2\pi{S}|}\right)^{\frac{1}{2}}\exp\{-\frac{1}{2}(\mu_1^TS_1\mu_1 + \mu_2^TS_2\mu_2-\mu^TS\mu)\}.
\end{align*}

In particular, $\N_0$ does not depend on $x$, and
\begin{equation}\label{comp_sq:int}
  \int_{\R^{n}}  \N(x;\mu_1,S_1)\N(Ax;\mu_2,S_2)dx = \N_0.
\end{equation}

Furthermore, if $A$ is unimodular then 
\begin{equation}\label{exact_prod} 
  \N_0 = \N(\mu_1;\mu_2',S_1*S_2').
\end{equation}
\qed
\end{Cor}

\section{The Forward Pass}
  Now we can inductively evaluate \eqref{alpha:0}. To do so, we split the computation
  into two steps.  In the first step, which we call the {\em time update}, we multiply
  $\alpha_{t-1}(u)$ by the state transition function $Pd(s\mid u)$ and integrate with respect
  to $u$. We will denote the result of the time update by $\hat{\alpha}_t(s)$.  It is the joint probability
  density of observations $x_1,\dots,x_{t-1}$ and state $s$ at time $t$.
  Then in the second step, which we call the {\em measurement update}, we multiply
  $\hat{\alpha}_t(s)$ by $X(x_t \mid s)$, the probability density of observing $x_t$ at time $t$
  in state $s$, to get $\alpha_t(s)$.  Henceforth, we put $S_T' := T^TS_TT$.

  First, for $1\le t \le T$ we inductively define
  \begin{align*}
    \hat{S}_{a,t-1} &= S_T'*S_{a,t-1},\\
    S_{a,t} &:= M^TS_MM  + \hat{S}_{a,t-1},\\
    \mu_{a,t} &:= S_{a,t}^{-1}(M^TS_Mx_t + \hat{S}_{a,t-1}\mu_{a,t-1})\\
    R_{a,t} &:= x_t^TS_Mx_t + \mu_{a,t-1}^T\hat{S}_{a,t-1}\mu_{a,t-1} - \mu_{a,t}^TS_{a,t}\mu_{a,t},\\
    N_{a,t} &:= \left(\frac{|S_M|\cdot|\hat{S}_{a,t-1}|}{2\pi|S_{a,t}|}\right)^{\frac{1}{2}}e^{-\frac{1}{2}R_{a,t}},\\
    P_{a,t} &:= P_{a,t-1}N_{a,t}, \quad\text{and}\\
    P_{a,0} &:= 1.
  \end{align*}

\begin{Thm}\label{alpha:1}
  For each state $s$ and time $t \ge 1$,
$$
  \alpha_t(s) = P_{a,t}N(s;\mu_{a,t},S_{a,t})
$$
\end{Thm}

\begin{proof}

Proceeding by induction on $t$, we note that the case $t = 0$ holds by definition.
For the time update, we have, using \eqref{comp_sq:2}
\begin{align}
  \hat{\alpha}_t(s) &= \int_{\R^n}Pd(s|u)\alpha_{t-1}(u)du \\\notag
  &= P_{a,t-1}\int_{\R^n}\N(Tu;s,S_T)\N(u;\mu_{a,t-1},S_{a,t-1})du \notag\\
  &= P_{a,t-1}\N(T^{-1}s;\mu_{a,t-1},\hat{S}_{a,t-1}).
\end{align}

Then multiplying by $X(x_t\mid s)$ and using \eqref{comp_sq:2} again yields
\begin{align*}
  \alpha_t(s) &= X(x_t\mid s)\hat{\alpha}_t(s) \\
  &= P_{a,t-1}\N(Ms;x_t,S_M)\N(T^{-1}s;\mu_{a,t-1},\hat{S}_{a,t-1}) \\
  &= P_{a,t-1}N_{a,t}\N(s;\mu_{a,t},S_{a,t}).
\end{align*}
\end{proof}

\section{The Backward Pass}
This calculation is very similar to the forward pass; the main difference being that we do
the measurement update first by multiplying by $X(x_{t+1}\mid u)$ to obtain $\hat{\beta}_{t+1}(u)$,
and then we integrate with respect to $Pd(u\mid s)du$ for the time update.  

\begin{Thm}\label{beta:1}
  For $t < T$, inductively define
\begin{align*}
  \hat{S}_{b,t} :&= M^TS_MM + S_{b,t+1};\\
  S_{b,t} :&= \hat{S}_{b,t}*S_T,\\
  \mu_{b,t} :&= \hat{S}_{b,t}^{-1}(M^TS_Mx_{t+1} + S_{b,t+1}\mu_{b,t+1}),\\
  R_{b,t} :&= x_{t+1}^TS_Mx_{t+1} + \mu_{b,t+1}^TS_{b,t+1}\mu_{b,t+1} - \mu_{b,t}^T\hat{S}_{b,t}\mu_{b,t},\\
  N_{b,t} :&= \left(\frac{|S_M|\cdot|S_{b,t+1}|}{|2\pi\hat{S}_{b,t}|}\right)^{\frac{1}{2}}e^{-\frac{1}{2}R_{b,t}},\quad\text{and}\\
  P_{b,t}:&=P_{b,t+1}N_{b,t},
\end{align*}
with initial values at $t = T$ given below. Then for each state $s$ and time $t < T$,
$$
  \beta_t(s) = P_{b,t}\N(s;\mu_{b,t},S_{b,t}).
$$
\end{Thm}

\begin{proof}
We proceed by reverse induction on $t$. However, to get started we first must  deal with
the special case of $\beta_{T}(u)$, the conditional probability of the future observations given
state $u$ at time $T$, but of course there are no such observations.  In the standard discrete
state HMM, we set $\beta(s) = 1$ for all states $s$, and this will work in our case as well by
setting $S_{b,T} = 0$ if $M$ is 1-1, but we really need a proper gaussian 
density in order to get the reverse induction started in all cases.  So we somewhat arbitrarily
assume the following values
\begin{align*}
  S_{b,T} :&= I,\\
  \mu_{b,T} :&= 0,\\
  P_{b,T} :&= 1;
\end{align*}
  For $t <  T$, we have the measurement update 
  \begin{align*}
    \hat{\beta}_{t+1}(u) &= X(x_{t+1}\mid u)\beta_{t+1}(u)\\
    &= P_{b,t+1}\N(Mu;x_{t+1},S_M)\N(u;\mu_{b,t+1},S_{b,t})\\
    &= P_{b,t+1}N_{b,t}\N(u;\mu_{b,t},\hat{S}_{b,t})
  \end{align*}

  Then a second application of \eqref{comp_sq:2} yields the time update
  \begin{align*}
    \beta_t(s) &= \int_{\R^n}\hat{\beta}_{t+1}(u)Pd(u\mid s)du \\
    &= P_{b,t+1}N_{b,t}\int_{\R^n}\N(u;\mu_{b,t},\hat{S}_{b,t+1})\N(u;s,S_T)du\\
    &=P_{b,t+1}N_{b,t}\N(s;\mu_{b,t},S_{b,t})\\
    &=P_{b,t}\N(s;\mu_{b,t},S_{b,t}).
  \end{align*}
\end{proof}

\section{The Posterior Likelihoods}
Recall that $\gamma_t(s)$ is the posterior probability density of state $s$
at time $t$. 
\begin{Thm}
  Let notation be as in \eqref{alpha:1} and \eqref{beta:1}.  Define
\begin{align*}
  S_{c,t} :&= S_{a,t} + S_{b,t}, \\
  \mu_{c,t} :&= S_{c,t}^{-1}(S_{a,t}\mu_{a,t} + S_{b,t}\mu_{b,t}), \quad\text{and}\\
  P_{c,t} :&= P_{a,t}P_{b,t}\N(\mu_{a,t};\mu_{b,t},S_{a,t}*S_{b,t}), \quad\text{then} \\
 \gamma_t(s) &= \N(s;\mu_{c,t},S_{c,t}).
  \end{align*}
\end{Thm}
\begin{proof}
  From \eqref{alpha:1} and \eqref{beta:1} we have (using \eqref{comp_sq:2}
  as usual)
  \begin{align*}
  \gamma_t{s} \propto \alpha_t(s)\beta_t(s) &= P_{a,t}P_{b,t}\N(s;\mu_{a,t},S_{a,t})\N(s;\mu_{b,t},S_{b,t})\\
  &= P_{c,t}\N(s;\mu_{c,t},S_{c,t}).
  \end{align*}
\end{proof}

We also need the joint posterior probability density of state $u$ at time $t-1$ and state $s$ at time $t$,
which we denote by $\hat{\gamma}_t(u,s)$.
\begin{Lem}\label{gamma_hat}
The joint probability density of all the data and states $u,s$  at times $t-1,t$ is
\begin{align*}
  \hat{\gamma}_t(u,s) &= \N(u;\mu_{u,t},S_{u,t})\gamma_t(s), \quad\text{where} \\
  S_{u,t} &:= S_{a,t-1}+S_T,\quad\text{and}\\
  \mu_{u,t} &:= S_{u,t}^{-1}(S_Ts + S_{a,t-1}\mu_{a,t-1}).
\end{align*}
\end{Lem}

\begin{proof}
  By definition, the joint density of all the data and states $u,s$ at times $t-1,t$ respectively is 
  \begin{align*}
    \hat{\gamma}_t(u,s) &= \alpha_{t-1}(u)Pd(s\mid u)X(x_t\mid s)\beta_t(s) \\
    &\propto \N(u;\mu_{a,t-1},S_{a,t-1})\N(u;s,S_T)\N(x_t;Ms,S_M)\N(s;\mu_{b,t},S_{b,t}).
  \end{align*}
  Combining the first two factors using \eqref{comp_sq:2} yields
  \begin{equation}\label{gamma_hat:1}
    \hat{\gamma}_t(u,s) = \N(u;\mu_{u,t},S_{u,t})\N(s;\mu_{a,t-1},\hat{S}_{a,t-1})\N(Ms,x_t,S_M)\N(s;\mu_{b,t},S_{b,t}).
  \end{equation}
  Then using \eqref{comp_sq:2} again to combine the second and third factors of \eqref{gamma_hat:1}, and discarding
  the term that does not involve either $u$ or $s$, we have
    \begin{align*}
      \hat{\gamma}_t(u,s) &= \N(u;\mu_{u,t},S_{u,t})\N(s;\mu_{a,t},S_{a,t})\N(s;\mu_{b,t},S_{b,t})\\
      &\propto \N(u;\mu_{u,t},S_{u,t})\N(s;\mu_{c,t},S_{c,t})
    \end{align*}
    as asserted.
    \end{proof}
  Note that
  $$
  \int_{\R^{2n}}\N(u;\mu_{u,t},S_{u,t})\N(s;\mu_{c,t}S_{c,t})duds = \int_{\R^n}\N(s;\mu_{c,t}S_{c,t})ds = 1 
  $$
  because the second factor does not depend on $u$, so \eqref{gamma_hat} is a bona-fide density and no
  normalization factor is required.  We remark that $\mu_{u,t}$ does depend on $s$, showing that $\gamma_{t-1}(u)$
  and $\gamma_t(s)$ are not independent.  This fact will play a role in the reestimation of $S_T$ below.
  
\section{Re-estimation}
Given a particular state sequence $S := \{s_0,s_1,s_2,\dots,s_T\}$ and the data sequence $X := \{x_1,x_2,\dots,x_T\}$,
the joint probability density of $S$ and $X$ given parameters $\theta := \{\mu_0,S_0,S_T,S_M,M\}$ is
$$
P(X,S\mid\theta) = \N(s_0;\mu_0,S_0)\prod_{t=1}^T\N(x_t;Ms_t,S_M)\N(s_t;s_{t-1},S_T).
$$

Note that $P(X,S\mid\theta)$ can also be viewed as  $L(\theta\mid X,S)$, the posterior likelihood of $\theta$.
To re-estimate $\theta$, we try to maximize $L(\theta)$ using the EM algorithm. Namely, we define 
$$
E(\theta,\bar{\theta}) := \int_{\R^{Tn}}P(X,S\mid\theta)\log{L(\bar{\theta}\mid X,S)}dS,
$$
where $\theta$ is the current set of parameters, and $\bar{\theta}$ is the unknown set of new parameters
we wish to determine. So instead of maximizing the log-likelihood directly, we can maximize its expected
value with respect to the current posterior distribution, because it is a standard result (and easy to prove) that
$$
L(\bar{\theta}) - L(\theta) \ge E(\theta,\bar{\theta}) - E(\theta,\theta),
$$
so choosing $\bar{\theta}$ to maximize $E(\theta,\bar{\theta})$ will increase the value of $L(\theta)$.


  
Recall the notation $Q(x;\mu,S)$ from \eqref{comp_sq:1}.  Then we have
\begin{align}
  E(\theta,\bar{\theta}) &= -(2T+1)\frac{n\log(2\pi)}{2} + \frac{1}{2}(E_0 + E_1 + E_2),\quad\text{where}\notag\\
  E_0 :&=\int_{\R^{Tn}}P(X,S\mid\theta)\{\log|\overline{S}_0| - Q(s_0;\bar{\mu}_0,\overline{S}_0)\}dS,\label{E0}\\
  E_1 :&= \int_{\R^{Tn}}P(X,S\mid\theta)\left\{\sum_{t=1}^T\log|\overline{S}_M| - Q(x_t;Ms_t,\overline{S}_M)\right\}dS,
\label{E1}\\
  E_2 :&= \int_{\R^{Tn}}P(X,S\mid\theta)\left\{\sum_{t=1}^T\log|\overline{S}_T| - Q(s_t;s_{t-1},\overline{S}_T)\right\}dS,
\label{E2}
\end{align}
Since the expected value of a constant is just that constant, $-n\log(2\pi)/2$ can be moved outside each
integral sign and ignored in the optimization.  And since none of the parameters of $\theta$ appear in more
than one of the $E_i$, we can optimize $E$ by optimizing each of $E_1,E_2,E_3$ separately.

We begin with $E_0$. Since the only component of $S$ in the integrand of \eqref{E0} is $s_0$, the expected value
collapses to the marginal 
expected value of the integrand with respect to the marginalization of $P(X,S\mid\theta)$ at $s_0$, which is just 
$\gamma_0(s_0)$.  Furthermore, after pulling the constant and the summation 
outside the integral in \eqref{E1} we have the same collapse to the marginalization at $s_t$ of the posterior, which
is just $\gamma_t(s_t)$.  It follows that
\begin{align*}
E_0 &= \log|\overline{S}_0| - \int_{\R^n}\gamma_0(s)Q(s;\bar{\mu}_0,\overline{S}_0)ds,\quad\text{and}\\
E_1 &= T\log|\overline{S}_M| - \sum_{t=1}^T\int_{\R^n}\gamma_t(s)Q(x_t;Ms,\overline{S}_M)ds.
\end{align*}

The situation for \eqref{E2} is slightly different, because after again moving the constant and the summation outside
the integral, the integrand depends on both $s_t$ and $s_{t-1}$.  So in this case, the integral collapses to the 
joint marginal expectation over $s_t$ and $s_{t-1}$, and we get
$$
E_2 = T\log|\overline{S}_T| - \sum_{t=1}^T\int_{\R^{2n}}\hat{\gamma}_t(u,s)Q(u;s,\overline{S}_T)duds,
$$
where $\hat{\gamma}_t(u,s)$ is the joint posterior probability density of state $u$ at time $t-1$ and state $s$ 
at time $t$ given by \eqref{gamma_hat}.

If $f$ is any scalar-valued function of an $m\times{n}$ matrix $A = a_{ij}$, we denote by $\partial{f}/\partial{A}$ the 
$m\times{n}$ matrix whose $(i,j)$-entry is $\partial{f}/\partial{a_{ij}}$.  This also applies to column vectors (when $n=1$).
In particular, if $m=n$, $A$ is symmetric, and $f(A) = |A|$,  then the usual expansion 
in minors along the $i^{th}$ row shows that 
$$
\frac{\partial{|A|}}{\partial{A}} = (2-\delta_{ij})A^* = (2-\delta_{ij})|A|A^{-1},\quad\text{and therefore}
\quad\frac{\partial{\log|A|}}{\partial{A}} = (2-\delta_{ij})A^{-1}. 
$$
Here $A^*$ is the adjoint matrix and the notation $(2-\delta_{ij})A^*$ means to multiply all off-diagonal elements of $A^*$ by 2
and leave the diagonal unchanged.  This is of course necessary due the the symmetry $A = A^T$.

In addition, since $Q(x;\mu,S)$ is linear in the coefficients of $S$, we see that
$$
\frac{\partial{Q(x;\mu,S)}}{\partial{S}} = (2-\delta_{ij})(x-\mu)(x-\mu)^T,
$$
and it is also easy to verify that
$$
\frac{\partial{Q(x;\mu,S)}}{\partial{\mu}} = 2S(\mu-x).
$$

Finally, to reestimate the $M$-matrix, we need

\begin{Lem}\label{dM}
  $$
  \frac{\partial{Q(x;M\mu,S)}}{\partial{M}} = -2Sx\mu^T + 2SM\mu\mu^T.
  $$
\end{Lem}
\begin{proof}
Expanding, we have
$$
Q := (x-M\mu)^TS(x-M\mu) = x^TSx - 2x^TSM\mu + \mu^TM^TSM\mu.
$$
Differentiating the linear term is just a special case of
$$
\frac{\partial}{\partial{M}}y^TMx = yx^T.
$$
For the quadratic term, we have
$$
Q_0 := \mu^TM^TSM\mu = \sum_{i,j,k,l}\mu_iM_{ji}S_{jk}M_{kl}\mu_l.
$$
Collecting terms which involve $M_{qr}$, if $j=q$ and $i=r$, we get
$$
M_{qr}\mu_r\sum_{k,l}S_{qk}M_{kl}\mu_l.
$$
If $k=q$ and $l=r$, we get
$$
M_{qr}\mu_r\sum_{i,j}\mu_iM_{ji}S_{jq},
$$
which is the same as the previous case after replacing $i$ by $l$ and $j$ by $k$, except
that the term $\mu_r^2M_{qr}^2S_{qq}$
(which is the case $j = k = q,i = l = r$) has been counted twice.
We'll call this term the exceptional term. Then
\begin{align*}
  \frac{\partial{Q_0}}{\partial{M_{qr}}} &= 2\mu_r\sum_{(k,l)\neq(q,r)}S_{qk}M_{kl}\mu_l + 2\mu_r^2M_{qr}S_{qq}\\
    &= 2\mu_r\sum_{k,l}S_{qk}M_{kl}\mu_l,
\end{align*}
So the exceptional term isn't really exceptional after all.  We conclude that
$$
\frac{\partial{Q_0}}{\partial{M}} = 2SM\mu\mu^T
$$
as asserted.
\end{proof}
      
Armed with these formulas, we first optimize $E_0$ with respect to $\bar{\mu}_0$: 
 
\begin{align}
0 = \frac{\partial{E_0}}{\partial{\bar{\mu}_0}} &= \int_{\R^n}\gamma_0(s)\frac{\partial{Q(s;\bar{\mu}_0,\overline{S}_0)}}{\partial{\bar{\mu}_0}}ds \notag\\
&= \int_{\R^n}\gamma_0(s)(2\overline{S}_0(\bar{\mu}_0-s))ds\notag\\
&= 2\overline{S}_0\int_{\R^n}\gamma_0(s)(\bar{\mu}_0-s)ds \notag\\
&= 2\overline{S}_0(\bar{\mu}_0-\mu_{c,0}), \quad\text{hence}\notag\\
\bar{\mu}_0 &= \mu_{c,0}\label{mu_0}
\end{align}

Next, we optimize $E_0$ with respect to $\overline{S}_0$, and we can set $\bar{\mu}_0 = \mu_{c,0}$ by \eqref{mu_0}:
\begin{align*}
  0 = \frac{\partial{E_0}}{\partial{\overline{S}_0}} &= \frac{\partial\log|\overline{S}_0|}{\partial\overline{S}_0}
  -\int_{\R^n}\gamma_0(s)\frac{\partial{Q(s;\mu_{c,0},\overline{S}_0)}}{\partial\overline{S}_0}ds \\
&= (2-\delta_{ij})\overline{S}_0^{-1} - (2-\delta_{ij})\int_{\R^n}\gamma_0(s)(s-\mu_{c,0})(s-\mu_{c,0})^Tds.
\end{align*}
Not surprisingly, it follows that
\begin{equation}\label{Sigma_0}
  \begin{split}
    \overline{S}_0^{-1} &= \int_{\R^n}\gamma_0(s)(s-\mu_{c,0})(s-\mu_{c,0})^Tds = S_{c,0}^{-1}, \quad\text{i.e.}\\
    \overline{S}_0 &= S_{c,0}.
  \end{split}
\end{equation}

To optimize $E_1$, we first differentiate with respect to $\overline{S}_{ob}$:
\begin{align*}
  0 = \frac{\partial{E_1}}{\partial{\overline{S}_M}} &=
  T\frac{\partial{\log|\overline{S}_M|}}{\partial\overline{S}_M}
  - \sum_{t=1}^T\int_{\R^n}\gamma_t(s)\frac{\partial{Q(x_t;\overline{M}s,\overline{S}_M)}}{\partial{\overline{S}_M}}ds \\
  &= (2-\delta{ij})T\overline{S}_M^{-1} - (2-\delta_{ij})\sum_{t=1}^T
  \int_{\R^n}\gamma_t(s)(x_t-\overline{M}s)(x_t-\overline{M}s)^Tds.
\end{align*}
Thus,  we see that
\begin{equation}\label{S_Ob}
  \begin{split}
    \overline{S}_M^{-1} &= \frac{1}{T}\sum_{t=1}^T\int_{\R^n}\gamma_t(s)(x_tx_t^T - x_ts^T\overline{M}^T
    - \overline{M}sx_t^T + \overline{M}ss^T\overline{M}^T)ds \\
    &= \frac{1}{T}\sum_{t=1}^T(x_tx_t^T - x_t\mu_{c,t}^T\overline{M}^T - \overline{M}\mu_{c,t}x_t^T +
    \overline{M}(S_{c,t}^{-1}+\mu_{c,t}\mu_{c,t}^T)\overline{M}^T)\\
    &= \frac{1}{T}\sum_{t=1}^T(x_t-\overline{M}\mu_{c,t})(x_t-\overline{M}\mu_{c,t})^T +
    \overline{M}S_{c,t}^{-1}\overline{M}^T.
  \end{split}
\end{equation}

We then differentiate with respect to $\overline{M}$ using \eqref{dM}:
\begin{align*}
  0 = \frac{\partial{E_1}}{\partial{\overline{M}}} &=
  - \sum_{t=1}^T\int_{\R^n}\gamma_t(s)\frac{\partial{Q(x_t;\overline{M}s,\overline{S}_M)}}{\partial{\overline{M}}}ds \\
  &= 2\sum_{t=1}^T\int_{\R^n}\gamma_t(s)(\overline{S}_Mx_ts^T - \overline{S}_{ob}\overline{M}ss^T)ds \\
  &= 2\sum_{t=1}^T(\overline{S}_Mx_t\mu_{c,t}^T - \overline{S}_M\overline{M}(S_{c,t}^{-1} + \mu_{c,t}\mu_{c,t}^T).
\end{align*}
Multiplying through by $\frac{1}{2}S_M^{-1}$, we find that
\begin{equation}\label{M_bar}
  \begin{split}
    \overline{M} &= \Gamma_1\Gamma_2^{-1}, \quad\text{where}\\
    \Gamma_1 :&= \sum_{t=1}^Tx_t\mu_{c,t}^T, \quad\text{and}\\
    \Gamma_2 :&= \sum_{i=1}^TS_{c,t}^{-1} + \mu_{c,t}\mu_{c,t}^T.
  \end{split}
\end{equation}

It is important to note here that \eqref{M_bar} is independent of $S_M$, so to optimize $E_1$, we can first
solve for $\overline{M}$ and then use the result to solve for $\overline{S}_M$.  

Finally, we optimize $E_2$ by differentiating with respect to $\overline{S}_T$ and using \eqref{gamma_hat}:
\begin{equation}\label{Sigma_Tr:0}
  \begin{split}
  0 &= \frac{\partial{E_2}}{\partial{\overline{S}_T}} = \frac{\partial{\log|\overline{S}_T|}}{\partial\overline{S}_T}
  - \sum_{t=1}^T\int_{\R^{2n}}\hat{\gamma}_t(u,s)\frac{\partial{Q(u;s,\overline{S}_T)}}{\partial\overline{S}_T}duds \\
      &= (2-\delta_{ij})T\overline{S}_T^{-1} - (2-\delta{ij}) \sum_{t=1}^T\int_{\R^{2n}}\N(u;\mu_{u,t},S_{u,t})\gamma_t(s)(u-s)(u-s)^Tduds.
  \end{split}
\end{equation}

As noted earlier, $\N(s,\mu_{c,t},S_{c,t})$ does not depend on $u$, so we can integrate first with respect to $u$, with a result
analagous to \eqref{S_Ob}:
$$
  \int_{-\infty}^{\infty}\N(u;\mu_{u,t},S_{u,t})(u-s)(u-s)^Tdu = S_{u,t}^{-1} + (s-\mu_{u,t})(s-\mu_{u,t})^T.
$$
Substituting this result into \eqref{Sigma_Tr:0}, we get
\begin{equation}\label{Sigma_Tr:1}
  \overline{S}_T^{-1} =  \frac{1}{T}\sum_{t=1}^T\left(S_{u,t}^{-1} +\int_{-\infty}^{\infty}\gamma_t(s)
  (s-\mu_{u,t})(s-\mu_{u,t})^Tds\right).
\end{equation}

However, there is a problem evaluating this integral because after checking \eqref{gamma_hat}, we see that $\mu_{u,t}$
depends on $s$, namely  
\begin{equation}\label{s-mu_1}
  \begin{split}
    s-\mu_{u,t} &= (I - S_{u,t}^{-1}S_T)s - S_{u,t}^{-1}S_{a,t-1}\mu_{a,t-1}, \\
    &= (S_{u,t}^{-1}S_{u,t} - S_{u,t}^{-1}S_T)s - S_{u,t}^{-1}S_{a,t-1}\mu_{a,t-1}, \\
    &= S_{u,t}^{-1}(S_{u,t} - S_T)s - S_{u,t}^{-1}S_{a,t-1}\mu_{a,t-1}, \\
    &= S_{u,t}^{-1}S_{a,t-1}(s - \mu_{a,t-1}),\\
  \end{split}
\end{equation}


Fortunately, neither $\mu_{a,t-1},S_{a,t-1},$ nor $S_{u,t}$ depends on $s$,
so using \eqref{gamma_hat}, \eqref{Sigma_Tr:1} becomes
\begin{equation}\label{S_Tr:2}
  \begin{split}
  \overline{S}_T^{-1} &=\frac{1}{T}\sum_{t=1}^T\left(S_{u,t}^{-1} +S_{u,t}^{-1}S_{a,t-1}
  \left[\int_{-\infty}^{\infty}\gamma_t(s)(s-\mu_{a,t-1})(s-\mu_{a,t-1})^Tds\right]S_{a,t-1}S_{u,t}^{-1}\right)\\
   &=\frac{1}{T}\sum_{t=1}^TS_{u,t}^{-1}+S_{u,t}^{-1}S_{a,t-1}
   [S_{c,t}^{-1}+(\mu_{c,t}-\mu_{a,t-1})(\mu_{c,t}-\mu_{a,t-1})^T]S_{a,t-1}S_{u,t}^{-1}.
  \end{split}
\end{equation}

\end{document}


